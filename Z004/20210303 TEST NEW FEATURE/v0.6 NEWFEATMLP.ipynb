{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('DataSciences': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7f9c69b77f8cb78a9d8b8acc2d09c3972908e6673afd8bfd04ee2f6acaaac495"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Importing Librairies...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import librairies.dagfeaturingfx \n",
    "from  librairies.dagfeaturingfx import *\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel,delayed\n",
    "import pyttsx3\n",
    "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler,quantile_transform,PolynomialFeatures\n",
    "from librairies.bt import *\n",
    "from sklearn.metrics import accuracy_score, make_scorer, precision_score, recall_score, precision_recall_curve, confusion_matrix, classification_report\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "import colorama as col\n",
    "from collections import Counter\n",
    "from librairies.strategy import *\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "engine = pyttsx3.init()\n",
    "engine.say(\"Librairies loaded\")\n",
    "engine.runAndWait() "
   ]
  },
  {
   "source": [
    "###### RELOAD LIBRAIRIE #####\n",
    "from importlib import reload\n",
    "sys.path.append('../')\n",
    "librairies.dagfeaturingfx= reload(librairies.dagfeaturingfx)\n",
    "from  librairies.dagfeaturingfx import *"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "%%time\n",
    "\n",
    "_t1 = dt.datetime.now()\n",
    "print('Début des opérations horodatée à',col.Fore.YELLOW,dt.datetime.now(),col.Style.RESET_ALL)\n",
    "\n",
    "TICKER_LIST = ['EUR/USD']\n",
    "x=TICKER_LIST[0]\n",
    "_ticker = x.replace('/','')\n",
    "_period = 'm5'\n",
    "df_all = pd.read_csv(x.replace('/','')+'_'+_period+'_BidAndAsk.csv')\n",
    "\n",
    "##### Ajout de la colonne Symbol pour identifier le ticker\n",
    "df_all['Symbol'] = _ticker\n",
    "\n",
    "##### On fixe la date en index sous forme de Timestamp\n",
    "df_all['Lindex'] = pd.to_datetime(df_all['Date'] + ' ' + df_all['Time'])\n",
    "df_all.set_index(pd.to_datetime(df_all.Lindex,format='%Y-%m-%d %H:%M:%S'),drop=True,inplace=True)\n",
    "\n",
    "###### On drop les colonnes inutiles\n",
    "df_all = df_all.drop(['Date','Lindex','Time','Total Ticks'],axis=1)\n",
    "\n",
    "##### On enlève les jours correspondant au samedi et au dimanche\n",
    "df_all['WE'] = np.where(((df_all.index.weekday == 5) | (df_all.index.weekday == 6)),None,df_all.index.weekday)\n",
    "df_all = df_all.dropna()\n",
    "df_all = df_all.drop(['WE'],axis=1)\n",
    "\n",
    "##### Calcul des averages pour les OHLC\n",
    "df_all['Open'] = (df_all['OpenBid'] + df_all['OpenAsk']) / 2\n",
    "df_all['High'] = (df_all['HighBid'] + df_all['HighAsk']) / 2\n",
    "df_all['Low'] = (df_all['LowBid'] + df_all['LowAsk']) / 2\n",
    "df_all['Close'] = (df_all['CloseBid'] + df_all['CloseAsk']) / 2\n",
    "\n",
    "hourly_all = pd.read_csv(x.replace('/','')+'_H1_BidAndAsk.csv')\n",
    "\n",
    "##### Ajout de la colonne Symbol pour identifier le ticker\n",
    "hourly_all['Symbol'] = _ticker\n",
    "\n",
    "##### On fixe la date en index sous forme de Timestamp\n",
    "hourly_all['Lindex'] = pd.to_datetime(hourly_all['Date'] + ' ' + hourly_all['Time'])\n",
    "hourly_all.set_index(pd.to_datetime(hourly_all.Lindex,format='%Y-%m-%d %H:%M:%S'),drop=True,inplace=True)\n",
    "\n",
    "###### On drop les colonnes inutiles\n",
    "hourly_all = hourly_all.drop(['Date','Lindex','Time','Total Ticks'],axis=1)\n",
    "\n",
    "##### On enlève les jours correspondant au samedi et au dimanche\n",
    "hourly_all['WE'] = np.where(((hourly_all.index.weekday == 5) | (hourly_all.index.weekday == 6)),None,hourly_all.index.weekday)\n",
    "hourly_all = hourly_all.dropna()\n",
    "hourly_all = hourly_all.drop(['WE'],axis=1)\n",
    "\n",
    "##### Calcul des averages pour les OHLC\n",
    "hourly_all['Open'] = (hourly_all['OpenBid'] + hourly_all['OpenAsk']) / 2\n",
    "hourly_all['High'] = (hourly_all['HighBid'] + hourly_all['HighAsk']) / 2\n",
    "hourly_all['Low'] = (hourly_all['LowBid'] + hourly_all['LowAsk']) / 2\n",
    "hourly_all['Close'] = (hourly_all['CloseBid'] + hourly_all['CloseAsk']) / 2\n",
    "\n",
    "engine.say(\"Processing featuring of dataframes, daily and intra-day\")\n",
    "engine.runAndWait()\n",
    "##### Récupération des data pour tous les tickers sur la période demandée en intraday => df_all\n",
    "\n",
    "engine.say(\"Raw data are loaded in memory\")\n",
    "engine.runAndWait()\n",
    "\n",
    "\n",
    "df_all = timerange1D(df_all)\n",
    "hourly_all = timerange1D(hourly_all)\n",
    "daily_all = get_daily(hourly_all,TICKER_LIST)\n",
    "\n",
    "engine.say(\"Daily up to date\")\n",
    "engine.runAndWait()\n",
    "\n",
    "daily_all = timerange1W(daily_all)\n",
    "weekly_all = get_weekly(daily_all,TICKER_LIST)\n",
    "engine.say(\"Weekly up to date\")\n",
    "engine.runAndWait()\n",
    "\n",
    "##### On calcule l'ADR sur le daily\n",
    "daily_all = adr(daily_all,_window=14)\n",
    "engine.say(\"ADR computed\")\n",
    "engine.runAndWait()\n",
    "\n",
    "##### On récupère l'ADR qui a été calculé en daily (daily_all) pour le mettre dans la base intraday df_all\n",
    "df_all = getadr(daily_all,df_all,TICKER_LIST)\n",
    "engine.say(\"ADR get in daily\")\n",
    "engine.runAndWait()\n",
    "\n",
    "df_all = adrhnl(daily_all,df_all,TICKER_LIST)\n",
    "\n",
    "##### Calcul d'une SMA 200 sur df_all\n",
    "df_all = sma(df_all=df_all,_window=200)\n",
    "\n",
    "##### Calcul des bollinger sur df_all\n",
    "df_all = bollinger(df_all,_slow=20)\n",
    "\n",
    "##### Calcul du stochastic slow. Par défaut les paramètres sont 5 et 3.\n",
    "df_all = slowstochastic(df_all,TICKER_LIST)\n",
    "\n",
    "df_all = ema(df_all,21,TICKER_LIST)\n",
    "\n",
    "df_all = ema(df_all,8,TICKER_LIST)\n",
    "\n",
    "weekly_all = pivot(weekly_all,TICKER_LIST)\n",
    "\n",
    "df_all = pivotimportdf(df_all,weekly_all,TICKER_LIST)\n",
    "\n",
    "df_all = atr(df_all,TICKER_LIST,14)\n",
    "\n",
    "df_all = rvi(df_all,TICKER_LIST,_window=14)\n",
    "\n",
    "df_all = sbgamma(df_all,TICKER_LIST)\n",
    "\n",
    "df_all = onhisma(df_all,TICKER_LIST,_window=5)\n",
    "df_all = onlosma(df_all,TICKER_LIST,_window=5)\n",
    "\n",
    "df_all = onhisma(df_all,TICKER_LIST,_window=21)\n",
    "df_all = onlosma(df_all,TICKER_LIST,_window=21)\n",
    "\n",
    "df_all = onhisma(df_all,TICKER_LIST,_window=34)\n",
    "df_all = onlosma(df_all,TICKER_LIST,_window=34)\n",
    "\n",
    "df_all = importohlc(df_all,weekly_all,TICKER_LIST,_suffix='_weekly')\n",
    "\n",
    "df_all = importohlc(df_all=df_all,other_all=daily_all,TICKER_LIST=TICKER_LIST,_suffix='_daily')\n",
    "\n",
    "engine.say(\"The job is done\")\n",
    "engine.runAndWait()\n",
    "\n",
    "\n",
    "engine.say(\"AAll the bases are saved\")\n",
    "engine.runAndWait()\n",
    "\n",
    "_t2 = dt.datetime.now()\n",
    "print('Début des opérations horodatée à',col.Fore.YELLOW,dt.datetime.now(),col.Style.RESET_ALL)\n",
    "print((_t2 - _t1))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Début des opérations horodatée à \u001b[33m 2021-03-03 22:34:21.233197 \u001b[0m\n",
      "\n",
      "Ajout Date\n",
      "100%|██████████| 1/1 [00:00<00:00, 2824.45it/s]\u001b[32m\n",
      "Calcul du Slow STOCHASTIC\u001b[0m\n",
      "\u001b[34mFenêtre de Glissement : \u001b[33m5 periodes.\u001b[0m\n",
      "\u001b[34mPériode de lissage : \u001b[33m3 periodes.\u001b[0m\n",
      "\n",
      "CPU times: user 1min 3s, sys: 2.39 s, total: 1min 5s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_t1 = dt.datetime.now()\n",
    "print('Début des opérations horodatée à',col.Fore.YELLOW,dt.datetime.now(),col.Style.RESET_ALL)\n",
    "\n",
    "TICKER_LIST = ['EUR/USD']\n",
    "x=TICKER_LIST[0]\n",
    "_ticker = x.replace('/','')\n",
    "_period = 'm5'\n",
    "df_all = pd.read_csv(x.replace('/','')+'_'+_period+'_BidAndAsk.csv')\n",
    "\n",
    "##### Ajout de la colonne Symbol pour identifier le ticker\n",
    "df_all['Symbol'] = _ticker\n",
    "\n",
    "##### On fixe la date en index sous forme de Timestamp\n",
    "df_all['Lindex'] = pd.to_datetime(df_all['Date'] + ' ' + df_all['Time'])\n",
    "df_all.set_index(pd.to_datetime(df_all.Lindex,format='%Y-%m-%d %H:%M:%S'),drop=True,inplace=True)\n",
    "\n",
    "###### On drop les colonnes inutiles\n",
    "df_all = df_all.drop(['Date','Lindex','Time','Total Ticks'],axis=1)\n",
    "##### Calcul des averages pour les OHLC\n",
    "df_all['Open'] = (df_all['OpenBid'] + df_all['OpenAsk']) / 2\n",
    "df_all['High'] = (df_all['HighBid'] + df_all['HighAsk']) / 2\n",
    "df_all['Low'] = (df_all['LowBid'] + df_all['LowAsk']) / 2\n",
    "df_all['Close'] = (df_all['CloseBid'] + df_all['CloseAsk']) / 2\n",
    "df_all = timerange1D(df_all)\n",
    "df_all = slowstochastic(df_all,TICKER_LIST)"
   ]
  },
  {
   "source": [
    "%%time\n",
    "joblib.dump(df_all,x.replace('/','')+'_DF_ALL')\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1.18 s, sys: 307 ms, total: 1.49 s\nWall time: 1.69 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['EURUSD_DF_ALL']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 287 ms, sys: 107 ms, total: 394 ms\nWall time: 393 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TICKER_LIST = ['EUR/USD']\n",
    "x=TICKER_LIST[0]\n",
    "df_all = joblib.load(x.replace('/','')+'_DF_ALL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(835382, 17)\n",
      "EUR/USD\n",
      "\n",
      "\u001b[31m ###############################################################################################\n",
      " ####################################### OOS WITHOUT AI ########################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-04 10:59:59.468897\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 84912/84912 [00:05<00:00, 15807.94it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[31m -50302.0 . En  1460 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 457 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 1003 \u001b[0m\n",
      "BT's execution time 0:00:11.680692\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2019-12-30  à 2021-02-15 \u001b[0m\n",
      "\u001b[36m Total Number of trades 1460 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m -50302.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m -25.15% \u001b[0m\n",
      "Average trade duration 44.59\n",
      "# Winners  457.0\n",
      "# Winners long  210.0\n",
      "# Winners short  247.0\n",
      "# Loosers  1003.0\n",
      "# Loosers  long 465.0\n",
      "# Loosers  short 538.0\n",
      "Cumulated gains 240288.0\n",
      "Cumulated losses -290590.0\n",
      "\u001b[34m PROFIT FACTOR :  0.83 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 31.3 % \u001b[0m\n",
      "Average Winners 525.79\n",
      "% Average Winners 0.26\n",
      "Average Loosers -289.72\n",
      "% Average Loosers -0.14\n",
      "Average pnl -34.45\n",
      "% Average pnl -0.06\n",
      "Number of opened trades 1460\n",
      "Number of closed trades 1460\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "\n",
      "\u001b[36m ###############################################################################################\n",
      " #################################### TRAIN/TEST WITHOUT AI ####################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-04 11:00:14.335741\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 750176/750176 [01:08<00:00, 11011.52it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[31m -601598.0 . En  13272 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 4170 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 9102 \u001b[0m\n",
      "BT's execution time 0:01:14.796941\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2010-01-01  à 2019-12-30 \u001b[0m\n",
      "\u001b[36m Total Number of trades 13272 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m -601598.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m -300.8% \u001b[0m\n",
      "Average trade duration 40.66\n",
      "# Winners  4170.0\n",
      "# Winners long  2093.0\n",
      "# Winners short  2077.0\n",
      "# Loosers  9102.0\n",
      "# Loosers  long 4751.0\n",
      "# Loosers  short 4351.0\n",
      "Cumulated gains 2399994.0\n",
      "Cumulated losses -3001592.0\n",
      "\u001b[34m PROFIT FACTOR :  0.8 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 31.42 % \u001b[0m\n",
      "Average Winners 575.54\n",
      "% Average Winners 0.29\n",
      "Average Loosers -329.77\n",
      "% Average Loosers -0.16\n",
      "Average pnl -45.33\n",
      "% Average pnl -0.26\n",
      "Number of opened trades 13272\n",
      "Number of closed trades 13272\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "\n",
      "\u001b[34m ###############################################################################################\n",
      " #################################### DENOISING & ENHANCING ####################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-04 11:01:34.156623\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 750176/750176 [00:39<00:00, 18892.28it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[32m 2399980.0 . En  4169 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 4169 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 0 \u001b[0m\n",
      "BT's execution time 0:00:46.592396\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2010-01-01  à 2019-12-30 \u001b[0m\n",
      "\u001b[36m Total Number of trades 4169 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L  in currency: \u001b[32m 2399980.0$ \u001b[0m\n",
      "P&L in %: \u001b[32m 1199.99% \u001b[0m\n",
      "Average trade duration 56.81\n",
      "# Winners  4169.0\n",
      "# Winners long  2092.0\n",
      "# Winners short  2077.0\n",
      "# Loosers  0.0\n",
      "# Loosers  long 0.0\n",
      "# Loosers  short 0.0\n",
      "Cumulated gains 2399980.0\n",
      "Cumulated losses 0.0\n",
      "\u001b[34m PROFIT FACTOR :  inf \u001b[0m\n",
      "\u001b[36m Winners Ratio : 100.0 % \u001b[0m\n",
      "Average Winners 575.67\n",
      "% Average Winners 0.29\n",
      "No looser\n",
      "Average pnl 575.67\n",
      "% Average pnl 2.18\n",
      "Number of opened trades 4169\n",
      "Number of closed trades 4169\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "Signaux - Accuracy : 81.49 %\n",
      "Signaux - Precision : 10.56 %\n",
      "Signaux - Recall : 19.48 %\n",
      "Achat - F-measure: : 13.7 %\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.93      0.87      0.90      4281\n",
      "         Pos       0.11      0.19      0.14       349\n",
      "\n",
      "    accuracy                           0.81      4630\n",
      "   macro avg       0.52      0.53      0.52      4630\n",
      "weighted avg       0.87      0.81      0.84      4630\n",
      "\n",
      "            Bonnes_Estimations Mauvaises_Estimations\n",
      "vrais-réels                 68                   281\n",
      "faux-réels                 576                  3705\n",
      "\n",
      "\u001b[34m Signaux pour \u001b[33m EUR/USD \u001b[0m\n",
      "Vrais signaux trouvés    :  68\n",
      "Vrais signaux non trouvé : 281\n",
      "Total des vrais signaux  : 349\n",
      "\u001b[31m\n",
      "Precision :  19.48 %\n",
      "\u001b[32m\n",
      "Recall 80.52 %\n",
      "\u001b[0m\n",
      "EUR/USD\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-04 11:02:31.285610\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 84624/84624 [00:08<00:00, 10064.76it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[31m -24402.0 . En  223 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 26 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 197 \u001b[0m\n",
      "BT's execution time 0:00:14.787680\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2019-12-30  à 2021-02-15 \u001b[0m\n",
      "\u001b[36m Total Number of trades 223 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m -24402.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m -12.2% \u001b[0m\n",
      "Average trade duration 197.31\n",
      "# Winners  26.0\n",
      "# Winners long  8.0\n",
      "# Winners short  18.0\n",
      "# Loosers  197.0\n",
      "# Loosers  long 17.0\n",
      "# Loosers  short 180.0\n",
      "Cumulated gains 33944.0\n",
      "Cumulated losses -58346.0\n",
      "\u001b[34m PROFIT FACTOR :  0.58 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 11.66 % \u001b[0m\n",
      "Average Winners 1305.54\n",
      "% Average Winners 0.65\n",
      "Average Loosers -296.17\n",
      "% Average Loosers -0.15\n",
      "Average pnl -109.43\n",
      "% Average pnl -0.09\n",
      "Number of opened trades 223\n",
      "Number of closed trades 223\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "CPU times: user 2min 9s, sys: 2.69 s, total: 2min 11s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "####### CHOIX DE LA TIME FRAME\n",
    "_period = 'm5'\n",
    "##### Récupération de la liste des tickers à partir du répertoire où sont rangées les bases\n",
    "TICKER_LIST = ['EUR/USD']\n",
    "TIK = ['JPY','CHF','CAD','GBP','AUD','NZD','SEK','NOK','MXN','ZAR','TRY','ILS','CNH','USD','HKD','0']\n",
    "RATE = [105.4,0.8989,1.276,0.877,0.7676,0.7201,8.3684,8.5238,20.09,14.81,7.03,3.28,6.458,1,7.7527,1.21]\n",
    "df_ratefx = pd.DataFrame(index=TIK)\n",
    "df_ratefx['rate'] = RATE\n",
    "#############################\n",
    "##### F E A T U R I N G #####\n",
    "#############################\n",
    "\n",
    "print(df_all.shape)\n",
    "# Make our choice for the split, compliant to our backtest \n",
    "_start = '2010-01-01' # start the train there '2010-01-01'\n",
    "_mid = '2018-08-29' # stop the train and begin the test there '2016-08-31'\n",
    "_stop = '2019-12-30' # stop the test there. After that, it is kept for oos\n",
    "_last = '2021-02-15' # '2020-12-31'\n",
    "#### S T R A T E G Y\n",
    "\n",
    "df_all = stochastic(df_all)\n",
    "##### On backtest selon le ticker selectionné sur la période déterminée\n",
    "\n",
    "x=TICKER_LIST[0]\n",
    "print(x)\n",
    "_year_bottom = _stop\n",
    "_year_top = _last\n",
    "_nb_bougie_exit = 2000000\n",
    "_trigger_reengage = 0\n",
    "_trigger_target = 1\n",
    "_trigger_invers = 0\n",
    "_trigger_sl = 1\n",
    "_verbose = 0\n",
    "_cash_ini = 200000\n",
    "_rate = 1/df_ratefx.loc[x[4:],'rate']\n",
    "backtest = df_all[df_all.Symbol == x.replace('/','')]\n",
    "_target =  0.002\n",
    "_exposure = 10\n",
    "_size = 200000\n",
    "_sl =  0.001\n",
    "_open_hour = 8 # day only\n",
    "_close_hour = 23 # day only\n",
    "_window = 0 # day only\n",
    "print()\n",
    "print(col.Fore.RED,'###############################################################################################')\n",
    "print(' ####################################### OOS WITHOUT AI ########################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "##### Backtest Over Night\n",
    "TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "print()\n",
    "print(col.Fore.CYAN,'###############################################################################################')\n",
    "print(' #################################### TRAIN/TEST WITHOUT AI ####################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "_year_bottom = _start\n",
    "_year_top = _stop\n",
    "\n",
    "TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "print()\n",
    "print(col.Fore.BLUE,'###############################################################################################')\n",
    "print(' #################################### DENOISING & ENHANCING ####################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "while _nb_looser > 0 :\n",
    "    \n",
    "    df_all['TRACKER'] = np.where(df_all.index.isin(TRACKER),1,0)\n",
    "    df_all['Valid'] = np.where(((df_all.Signal!=0)&(df_all.TRACKER==1)),1,0)\n",
    "    df_all['Signal'] = np.where(((df_all.Valid==1)&(df_all.Signal==1)),1,np.where(((df_all.Valid==1)&(df_all.Signal==-1)),-1,0))\n",
    "    backtest = df_all[df_all.Symbol == x.replace('/','')]\n",
    "    ##### Purification of signal by denoising and enhancing\n",
    "    TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "            _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "df_all = joblib.load(x.replace('/','')+'_DF_ALL')\n",
    "df_all = stochastic(df_all)\n",
    "#features = featuring(df_all)\n",
    "features = df_all.copy()\n",
    "\n",
    "features['TRACKER'] = np.where(features.index.isin(TRACKER),1,0)\n",
    "\n",
    "# First, we must have an output. We'll call it 'Valid'. It wil be where Tracker & Signal are both to 1\n",
    "features['Valid'] = np.where(((features.Signal!=0)&(features.TRACKER==1)),1,0) # Don't miss the point that even a Signal -1 must be considered as a good one by TRACKER\n",
    "\n",
    "# Let's isolate the ticker on which we made the test\n",
    "#features = features[features.Symbol==x.replace('/','')]\n",
    "#features = df_all.copy()\n",
    "# And drop the nan\n",
    "features = features.dropna()\n",
    "##### Signal is from strategy. This is potential good one. But we have to create the TRACKER column where the Signal where efficient\n",
    "\n",
    "features['deltaHL'] = features['High'] - features['Low']\n",
    "features['deltaOC'] = features['Open'] - features['Close']\n",
    "\n",
    "features = features.dropna()\n",
    "features = features[features.Symbol==_ticker]\n",
    "features['TRACKER'] = np.where(features.index.isin(TRACKER),1,0)\n",
    "features_train = features[(features.Date>=_start)&(features.Date<=_mid)]\n",
    "features_test = features[(features.Date>_mid)&(features.Date<=_stop)]\n",
    "features_oos = features[(features.Date>_stop)&(features.Date <= _last)]\n",
    "features_train = features_train[features_train.Signal!=0]\n",
    "features_test = features_test[features_test.Signal!=0]\n",
    "\n",
    "\n",
    "features_train['OpenBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.OpenBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['HighBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.HighBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['LowBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.LowBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['CloseBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.CloseBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['HighAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.HighAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['LowAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.LowAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['CloseAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.CloseAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['Open'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.Open.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['High'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.High.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['Low'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.Low.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['Close'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.Close.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['slow_K5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.slow_K5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['slow_D5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.slow_D5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['OpenAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.OpenAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaHL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaHL.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "\n",
    "features_test['OpenBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.OpenBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['HighBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.HighBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['LowBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.LowBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['CloseBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.CloseBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['HighAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.HighAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['LowAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.LowAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['CloseAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.CloseAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['Open'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.Open.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['High'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.High.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['Low'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.Low.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['Close'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.Close.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['slow_K5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.slow_K5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['slow_D5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.slow_D5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['OpenAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.OpenAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaHL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaHL.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "\n",
    "\n",
    "features_oos['OpenBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.OpenBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['HighBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.HighBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['LowBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.LowBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['CloseBid'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.CloseBid.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['HighAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.HighAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['LowAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.LowAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['CloseAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.CloseAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['Open'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.Open.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['High'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.High.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['Low'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.Low.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['Close'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.Low.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['slow_K5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.slow_K5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['slow_D5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.slow_D5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['OpenAsk'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.OpenAsk.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "\n",
    "# Proceed an MaxAbsScaler on features\n",
    "#features_train,features_test,features_oos = scaling(features,x.replace('/',''),TRACKER,_start,_mid,_stop,_last,scaler=MaxAbsScaler())\n",
    "#features_train,features_test,features_oos = quantile(features_train,features_test,features_oos,quantile_transform)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "_name = 'DecisionTreeClassifier'\n",
    "#_model = MLPClassifier(hidden_layer_sizes=(300,15), activation='relu', solver='adam', alpha=0.000001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=400, shuffle=True, random_state=26, tol=0.0000001, verbose=False, warm_start=True, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.8, beta_2=0.999, epsilon=1e-08)\n",
    "_model = DecisionTreeClassifier(criterion='entropy',random_state=26,splitter='best',ccp_alpha=0.00007,max_features=7,class_weight={0: 1, 1: 1})\n",
    "\n",
    "_model.fit(features_train.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1),features_train.Valid)\n",
    "yhat = _model.predict(features_test.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1))\n",
    "accu = round(accuracy_score(features_test.Valid, yhat) * 100,2)\n",
    "prec = round(precision_score(features_test.Valid, yhat,pos_label=1) * 100,2)\n",
    "recall = round(recall_score(features_test.Valid, yhat) * 100,2)\n",
    "f1 = round(f1_score(features_test.Valid, yhat) * 100,2)\n",
    "\n",
    "\n",
    "print('Signaux - Accuracy :' ,accu,'%')\n",
    "print('Signaux - Precision :',prec,'%')\n",
    "print('Signaux - Recall :', recall,'%')\n",
    "print('Achat - F-measure: :' ,f1,'%')\n",
    "print('\\n')\n",
    "print(classification_report(features_test.Valid, yhat, target_names=['Neg', 'Pos']))\n",
    "conf_matrix = pd.DataFrame(columns=['Bonnes_Estimations','Mauvaises_Estimations'])\n",
    "tt = confusion_matrix(features_test.Valid, yhat, labels=[1,0])    #_model.classes_)\n",
    "\n",
    "conf_matrix.loc['vrais-réels'] = tt[0]\n",
    "conf_matrix.loc['faux-réels'] = tt[1]\n",
    "\n",
    "print(conf_matrix)\n",
    "print()\n",
    "print(col.Fore.BLUE,'Signaux pour',col.Fore.YELLOW,x,col.Style.RESET_ALL)\n",
    "_tp = tt[0][0]\n",
    "_fn = tt[0][1]\n",
    "_prec = round((tt[0][0]/(tt[0][0]+tt[0][1]))*100,2)\n",
    "_rec = round((tt[0][1]/(tt[0][0]+tt[0][1]))*100,2)\n",
    "\n",
    "print('Vrais signaux trouvés    : ',tt[0][0])\n",
    "print('Vrais signaux non trouvé :',tt[0][1])\n",
    "print('Total des vrais signaux  :',tt[0][0]+tt[0][1])\n",
    "if _prec > 69 and prec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif _prec < 51 or prec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "print('Precision : ',_prec,'%')\n",
    "if _rec > 69 and _rec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif _rec < 51 or _rec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "print('Recall',_rec,'%')\n",
    "print(col.Style.RESET_ALL)\n",
    "\n",
    "df_all_oos = df_all[(df_all.Date > _stop)&(df_all.Date <= _last)].dropna()\n",
    "\n",
    "df_all_oos = df_all_oos[df_all_oos.Symbol==x.replace('/','')]\n",
    "\n",
    "\n",
    "df_all_oos['Valid'] = _model.predict(features_oos.drop(['Date','Symbol','Signal','TRACKER','Valid'],axis=1))\n",
    "\n",
    "\n",
    "df_all_oos['Signal'] = np.where((df_all_oos.Signal==1)&(df_all_oos.Valid==1),1,np.where((df_all_oos.Signal==-1)&(df_all_oos.Valid==1),-1,0))\n",
    " \n",
    "##### On backtest selon le ticker selectionné sur la période déterminée\n",
    "\n",
    "joblib.dump(_model,'Save_'+x.replace('/','')+'_m5.sav')\n",
    "\n",
    "print(x)\n",
    "_year_bottom = _stop\n",
    "_year_top = _last\n",
    "\n",
    "_trigger_invers = 1\n",
    "_target = 0.017\n",
    "##### Backtest Over Night\n",
    "FINAL_TRACKER = bt(df_all_oos,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Signaux - Accuracy : 12.16 %\nSignaux - Precision : 7.64 %\nSignaux - Recall : 98.25 %\nAchat - F-measure: : 14.18 %\n\n\n              precision    recall  f1-score   support\n\n         Neg       0.97      0.05      0.10      4288\n         Pos       0.08      0.98      0.14       342\n\n    accuracy                           0.12      4630\n   macro avg       0.53      0.52      0.12      4630\nweighted avg       0.91      0.12      0.10      4630\n\n            Bonnes_Estimations Mauvaises_Estimations\nvrais-réels                336                     6\nfaux-réels                4061                   227\n\n\u001b[34m Signaux pour \u001b[33m EUR/USD \u001b[0m\nVrais signaux trouvés    :  336\nVrais signaux non trouvé : 6\nTotal des vrais signaux  : 342\n\u001b[31m\nPrecision :  98.25 %\n\u001b[31m\nRecall 1.75 %\n\u001b[0m\nCPU times: user 2.12 s, sys: 529 ms, total: 2.65 s\nWall time: 2.6 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import librairies.dagfeaturingfx \n",
    "from  librairies.dagfeaturingfx import *\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel,delayed\n",
    "import pyttsx3\n",
    "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler,quantile_transform,PolynomialFeatures\n",
    "from librairies.bt import *\n",
    "from sklearn.metrics import accuracy_score, make_scorer, precision_score, recall_score, precision_recall_curve, confusion_matrix, classification_report\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "import colorama as col\n",
    "from collections import Counter\n",
    "from librairies.strategy import *\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "TICKER_LIST = ['EUR/USD']\n",
    "x=TICKER_LIST[0]\n",
    "df_all = joblib.load(x.replace('/','')+'_DF_ALL')\n",
    "\n",
    "\n",
    "_start = '2010-01-01' # start the train there '2010-01-01'\n",
    "_mid = '2018-08-29' # stop the train and begin the test there '2016-08-31'\n",
    "_stop = '2019-12-30' # stop the test there. After that, it is kept for oos\n",
    "_last = '2021-02-15' \n",
    "\n",
    "TICKER_LIST = ['EUR/USD']\n",
    "x=TICKER_LIST[0]\n",
    "_ticker = x.replace('/','')\n",
    "TRACKER = joblib.load('TRACKER')\n",
    "df_all = joblib.load(x.replace('/','')+'_DF_ALL')\n",
    "\n",
    "df_all = stochastic(df_all)\n",
    "#features = featuring(df_all)\n",
    "features = df_all.copy()\n",
    "\n",
    "features['TRACKER'] = np.where(features.index.isin(TRACKER),1,0)\n",
    "\n",
    "# First, we must have an output. We'll call it 'Valid'. It wil be where Tracker & Signal are both to 1\n",
    "features['Valid'] = np.where(((features.Signal!=0)&(features.TRACKER==1)),1,0) # Don't miss the point that even a Signal -1 must be considered as a good one by TRACKER\n",
    "\n",
    "# Let's isolate the ticker on which we made the test\n",
    "#features = features[features.Symbol==x.replace('/','')]\n",
    "#features = df_all.copy()\n",
    "# And drop the nan\n",
    "features = features.dropna()\n",
    "##### Signal is from strategy. This is potential good one. But we have to create the TRACKER column where the Signal where efficient\n",
    "\n",
    "features['deltaHL'] = features['High'] - features['Low']\n",
    "features['deltaOC'] = features['Open'] - features['Close']\n",
    "features['deltaHC'] = features['High'] - features['Close']\n",
    "features['deltaLC'] = features['Low'] - features['Close']\n",
    "features['deltaOH'] = features['Open'] - features['High']\n",
    "features['deltaOL'] = features['Open'] - features['Low']\n",
    "\n",
    "features = features.drop(['OpenBid','OpenAsk','HighBid','LowBid','CloseBid','HighAsk','LowAsk','CloseAsk','Open','High','Low','Close'],axis=1)\n",
    "\n",
    "features = features.dropna()\n",
    "features = features[features.Symbol==_ticker]\n",
    "features['TRACKER'] = np.where(features.index.isin(TRACKER),1,0)\n",
    "features_train = features[(features.Date>=_start)&(features.Date<=_mid)]\n",
    "features_test = features[(features.Date>_mid)&(features.Date<=_stop)]\n",
    "features_oos = features[(features.Date>_stop)&(features.Date <= _last)]\n",
    "features_train = features_train[features_train.Signal!=0]\n",
    "features_test = features_test[features_test.Signal!=0]\n",
    "\n",
    "\n",
    "\n",
    "features_train['slow_K5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.slow_K5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['slow_D5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.slow_D5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaHL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaHL.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaHC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaHC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaLC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaLC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaOH'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaOH.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaOL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaOL.astype(np.float32)).reshape(-1, 1)))\n",
    "\n",
    "\n",
    "features_test['slow_K5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.slow_K5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['slow_D5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.slow_D5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaHL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaHL.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaHC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaHC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaLC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaLC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaOH'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaOH.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaOL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaOL.astype(np.float32)).reshape(-1, 1)))\n",
    "\n",
    "\n",
    "\n",
    "features_oos['slow_K5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.slow_K5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['slow_D5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.slow_D5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaHC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaHC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaLC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaLC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaOH'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaOH.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaOL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaOL.astype(np.float32)).reshape(-1, 1)))\n",
    "\n",
    "\n",
    "#_model = MLPClassifier(hidden_layer_sizes=(300,300), activation='logistic', solver='adam', alpha=0.000001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.15, max_iter=800, shuffle=True, random_state=26, tol=0.000001, verbose=1, warm_start=True, momentum=0.99, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier,RadiusNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "_model = DecisionTreeClassifier(criterion='entropy',random_state=26,splitter='best',ccp_alpha=0.00007,max_features=7,class_weight={0: 1, 1: 1000})\n",
    "_model.fit(features_train.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1),features_train.Valid)\n",
    "yhat = _model.predict(features_test.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1))\n",
    "accu = round(accuracy_score(features_test.Valid, yhat) * 100,2)\n",
    "prec = round(precision_score(features_test.Valid, yhat,pos_label=1) * 100,2)\n",
    "recall = round(recall_score(features_test.Valid, yhat) * 100,2)\n",
    "f1 = round(f1_score(features_test.Valid, yhat) * 100,2)\n",
    "\n",
    "\n",
    "print('Signaux - Accuracy :' ,accu,'%')\n",
    "print('Signaux - Precision :',prec,'%')\n",
    "print('Signaux - Recall :', recall,'%')\n",
    "print('Achat - F-measure: :' ,f1,'%')\n",
    "print('\\n')\n",
    "print(classification_report(features_test.Valid, yhat, target_names=['Neg', 'Pos']))\n",
    "conf_matrix = pd.DataFrame(columns=['Bonnes_Estimations','Mauvaises_Estimations'])\n",
    "tt = confusion_matrix(features_test.Valid, yhat, labels=[1,0])    #_model.classes_)\n",
    "\n",
    "conf_matrix.loc['vrais-réels'] = tt[0]\n",
    "conf_matrix.loc['faux-réels'] = tt[1]\n",
    "\n",
    "print(conf_matrix)\n",
    "print()\n",
    "print(col.Fore.BLUE,'Signaux pour',col.Fore.YELLOW,x,col.Style.RESET_ALL)\n",
    "_tp = tt[0][0]\n",
    "_fn = tt[0][1]\n",
    "_prec = round((tt[0][0]/(tt[0][0]+tt[0][1]))*100,2)\n",
    "_rec = round((tt[0][1]/(tt[0][0]+tt[0][1]))*100,2)\n",
    "\n",
    "print('Vrais signaux trouvés    : ',tt[0][0])\n",
    "print('Vrais signaux non trouvé :',tt[0][1])\n",
    "print('Total des vrais signaux  :',tt[0][0]+tt[0][1])\n",
    "if _prec > 69 and prec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif _prec < 51 or prec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "print('Precision : ',_prec,'%')\n",
    "if _rec > 69 and _rec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif _rec < 51 or _rec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "print('Recall',_rec,'%')\n",
    "print(col.Style.RESET_ALL)\n",
    "joblib.dump(_model,'1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(835382, 16)\n",
      "EUR/USD\n",
      "\n",
      "\u001b[31m ###############################################################################################\n",
      " ####################################### OOS WITHOUT AI ########################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-04 12:53:46.698980\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 84912/84912 [00:05<00:00, 15227.90it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[31m -50302.0 . En  1460 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 457 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 1003 \u001b[0m\n",
      "BT's execution time 0:00:11.916744\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2019-12-30  à 2021-02-15 \u001b[0m\n",
      "\u001b[36m Total Number of trades 1460 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m -50302.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m -25.15% \u001b[0m\n",
      "Average trade duration 44.59\n",
      "# Winners  457.0\n",
      "# Winners long  210.0\n",
      "# Winners short  247.0\n",
      "# Loosers  1003.0\n",
      "# Loosers  long 465.0\n",
      "# Loosers  short 538.0\n",
      "Cumulated gains 240288.0\n",
      "Cumulated losses -290590.0\n",
      "\u001b[34m PROFIT FACTOR :  0.83 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 31.3 % \u001b[0m\n",
      "Average Winners 525.79\n",
      "% Average Winners 0.26\n",
      "Average Loosers -289.72\n",
      "% Average Loosers -0.14\n",
      "Average pnl -34.45\n",
      "% Average pnl -0.06\n",
      "Number of opened trades 1460\n",
      "Number of closed trades 1460\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "\n",
      "\u001b[36m ###############################################################################################\n",
      " #################################### TRAIN/TEST WITHOUT AI ####################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-04 12:54:01.790295\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 750176/750176 [00:45<00:00, 16425.32it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[31m -601598.0 . En  13272 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 4170 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 9102 \u001b[0m\n",
      "BT's execution time 0:00:52.365763\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2010-01-01  à 2019-12-30 \u001b[0m\n",
      "\u001b[36m Total Number of trades 13272 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m -601598.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m -300.8% \u001b[0m\n",
      "Average trade duration 40.66\n",
      "# Winners  4170.0\n",
      "# Winners long  2093.0\n",
      "# Winners short  2077.0\n",
      "# Loosers  9102.0\n",
      "# Loosers  long 4751.0\n",
      "# Loosers  short 4351.0\n",
      "Cumulated gains 2399994.0\n",
      "Cumulated losses -3001592.0\n",
      "\u001b[34m PROFIT FACTOR :  0.8 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 31.42 % \u001b[0m\n",
      "Average Winners 575.54\n",
      "% Average Winners 0.29\n",
      "Average Loosers -329.77\n",
      "% Average Loosers -0.16\n",
      "Average pnl -45.33\n",
      "% Average pnl -0.26\n",
      "Number of opened trades 13272\n",
      "Number of closed trades 13272\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "\n",
      "\u001b[34m ###############################################################################################\n",
      " #################################### DENOISING & ENHANCING ####################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-04 12:54:58.615684\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 750176/750176 [00:31<00:00, 23639.82it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[32m 2399980.0 . En  4169 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 4169 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 0 \u001b[0m\n",
      "BT's execution time 0:00:38.432893\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2010-01-01  à 2019-12-30 \u001b[0m\n",
      "\u001b[36m Total Number of trades 4169 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L  in currency: \u001b[32m 2399980.0$ \u001b[0m\n",
      "P&L in %: \u001b[32m 1199.99% \u001b[0m\n",
      "Average trade duration 56.81\n",
      "# Winners  4169.0\n",
      "# Winners long  2092.0\n",
      "# Winners short  2077.0\n",
      "# Loosers  0.0\n",
      "# Loosers  long 0.0\n",
      "# Loosers  short 0.0\n",
      "Cumulated gains 2399980.0\n",
      "Cumulated losses 0.0\n",
      "\u001b[34m PROFIT FACTOR :  inf \u001b[0m\n",
      "\u001b[36m Winners Ratio : 100.0 % \u001b[0m\n",
      "Average Winners 575.67\n",
      "% Average Winners 0.29\n",
      "No looser\n",
      "Average pnl 575.67\n",
      "% Average pnl 2.18\n",
      "Number of opened trades 4169\n",
      "Number of closed trades 4169\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "EUR/USD\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-04 12:55:42.819154\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 84624/84624 [00:07<00:00, 11001.53it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[31m -49326.0 . En  1459 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 458 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 1001 \u001b[0m\n",
      "BT's execution time 0:00:14.021750\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2019-12-30  à 2021-02-15 \u001b[0m\n",
      "\u001b[36m Total Number of trades 1459 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m -49326.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m -24.66% \u001b[0m\n",
      "Average trade duration 44.42\n",
      "# Winners  458.0\n",
      "# Winners long  211.0\n",
      "# Winners short  247.0\n",
      "# Loosers  1001.0\n",
      "# Loosers  long 465.0\n",
      "# Loosers  short 536.0\n",
      "Cumulated gains 240756.0\n",
      "Cumulated losses -290082.0\n",
      "\u001b[34m PROFIT FACTOR :  0.83 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 31.39 % \u001b[0m\n",
      "Average Winners 525.67\n",
      "% Average Winners 0.26\n",
      "Average Loosers -289.79\n",
      "% Average Loosers -0.14\n",
      "Average pnl -33.81\n",
      "% Average pnl -0.06\n",
      "Number of opened trades 1459\n",
      "Number of closed trades 1459\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "CPU times: user 1min 35s, sys: 2.06 s, total: 1min 37s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TICKER_LIST = ['EUR/USD']\n",
    "x=TICKER_LIST[0]\n",
    "df_all = joblib.load(x.replace('/','')+'_DF_ALL')\n",
    "####### CHOIX DE LA TIME FRAME\n",
    "_period = 'm5'\n",
    "##### Récupération de la liste des tickers à partir du répertoire où sont rangées les bases\n",
    "TICKER_LIST = ['EUR/USD']\n",
    "TIK = ['JPY','CHF','CAD','GBP','AUD','NZD','SEK','NOK','MXN','ZAR','TRY','ILS','CNH','USD','HKD','0']\n",
    "RATE = [105.4,0.8989,1.276,0.877,0.7676,0.7201,8.3684,8.5238,20.09,14.81,7.03,3.28,6.458,1,7.7527,1.21]\n",
    "df_ratefx = pd.DataFrame(index=TIK)\n",
    "df_ratefx['rate'] = RATE\n",
    "#############################\n",
    "##### F E A T U R I N G #####\n",
    "#############################\n",
    "\n",
    "print(df_all.shape)\n",
    "# Make our choice for the split, compliant to our backtest \n",
    "_start = '2010-01-01' # start the train there '2010-01-01'\n",
    "_mid = '2018-08-29' # stop the train and begin the test there '2016-08-31'\n",
    "_stop = '2019-12-30' # stop the test there. After that, it is kept for oos\n",
    "_last = '2021-02-15' # '2020-12-31'\n",
    "#### S T R A T E G Y\n",
    "\n",
    "df_all = stochastic(df_all)\n",
    "##### On backtest selon le ticker selectionné sur la période déterminée\n",
    "\n",
    "x=TICKER_LIST[0]\n",
    "print(x)\n",
    "_year_bottom = _stop\n",
    "_year_top = _last\n",
    "_nb_bougie_exit = 2000000\n",
    "_trigger_reengage = 0\n",
    "_trigger_target = 1\n",
    "_trigger_invers = 0\n",
    "_trigger_sl = 1\n",
    "_verbose = 0\n",
    "_cash_ini = 200000\n",
    "_rate = 1/df_ratefx.loc[x[4:],'rate']\n",
    "backtest = df_all[df_all.Symbol == x.replace('/','')]\n",
    "_target =  0.002\n",
    "_exposure = 10\n",
    "_size = 200000\n",
    "_sl =  0.001\n",
    "_open_hour = 8 # day only\n",
    "_close_hour = 23 # day only\n",
    "_window = 0 # day only\n",
    "print()\n",
    "print(col.Fore.RED,'###############################################################################################')\n",
    "print(' ####################################### OOS WITHOUT AI ########################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "##### Backtest Over Night\n",
    "TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "print()\n",
    "print(col.Fore.CYAN,'###############################################################################################')\n",
    "print(' #################################### TRAIN/TEST WITHOUT AI ####################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "_year_bottom = _start\n",
    "_year_top = _stop\n",
    "\n",
    "TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "print()\n",
    "print(col.Fore.BLUE,'###############################################################################################')\n",
    "print(' #################################### DENOISING & ENHANCING ####################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "while _nb_looser > 0 :\n",
    "    \n",
    "    df_all['TRACKER'] = np.where(df_all.index.isin(TRACKER),1,0)\n",
    "    df_all['Valid'] = np.where(((df_all.Signal!=0)&(df_all.TRACKER==1)),1,0)\n",
    "    df_all['Signal'] = np.where(((df_all.Valid==1)&(df_all.Signal==1)),1,np.where(((df_all.Valid==1)&(df_all.Signal==-1)),-1,0))\n",
    "    backtest = df_all[df_all.Symbol == x.replace('/','')]\n",
    "    ##### Purification of signal by denoising and enhancing\n",
    "    TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "            _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "df_all = joblib.load(x.replace('/','')+'_DF_ALL')\n",
    "df_all = stochastic(df_all)\n",
    "#features = featuring(df_all)\n",
    "features = df_all.copy()\n",
    "\n",
    "features['TRACKER'] = np.where(features.index.isin(TRACKER),1,0)\n",
    "\n",
    "# First, we must have an output. We'll call it 'Valid'. It wil be where Tracker & Signal are both to 1\n",
    "features['Valid'] = np.where(((features.Signal!=0)&(features.TRACKER==1)),1,0) # Don't miss the point that even a Signal -1 must be considered as a good one by TRACKER\n",
    "\n",
    "# Let's isolate the ticker on which we made the test\n",
    "#features = features[features.Symbol==x.replace('/','')]\n",
    "#features = df_all.copy()\n",
    "# And drop the nan\n",
    "features = features.dropna()\n",
    "##### Signal is from strategy. This is potential good one. But we have to create the TRACKER column where the Signal where efficient\n",
    "\n",
    "features['deltaHL'] = features['High'] - features['Low']\n",
    "features['deltaOC'] = features['Open'] - features['Close']\n",
    "features['deltaHC'] = features['High'] - features['Close']\n",
    "features['deltaLC'] = features['Low'] - features['Close']\n",
    "features['deltaOH'] = features['Open'] - features['High']\n",
    "features['deltaOL'] = features['Open'] - features['Low']\n",
    "\n",
    "features = features.drop(['OpenBid','OpenAsk','HighBid','LowBid','CloseBid','HighAsk','LowAsk','CloseAsk','Open','High','Low','Close'],axis=1)\n",
    "\n",
    "features = features.dropna()\n",
    "features = features[features.Symbol==_ticker]\n",
    "features['TRACKER'] = np.where(features.index.isin(TRACKER),1,0)\n",
    "features_train = features[(features.Date>=_start)&(features.Date<=_mid)]\n",
    "features_test = features[(features.Date>_mid)&(features.Date<=_stop)]\n",
    "features_oos = features[(features.Date>_stop)&(features.Date <= _last)]\n",
    "features_train = features_train[features_train.Signal!=0]\n",
    "features_test = features_test[features_test.Signal!=0]\n",
    "\n",
    "\n",
    "\n",
    "features_train['slow_K5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.slow_K5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['slow_D5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.slow_D5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaHL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaHL.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaHC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaHC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaLC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaLC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaOH'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaOH.astype(np.float32)).reshape(-1, 1)))\n",
    "features_train['deltaOL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_train.deltaOL.astype(np.float32)).reshape(-1, 1)))\n",
    "\n",
    "\n",
    "features_test['slow_K5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.slow_K5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['slow_D5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.slow_D5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaHL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaHL.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaHC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaHC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaLC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaLC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaOH'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaOH.astype(np.float32)).reshape(-1, 1)))\n",
    "features_test['deltaOL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_test.deltaOL.astype(np.float32)).reshape(-1, 1)))\n",
    "\n",
    "\n",
    "\n",
    "features_oos['slow_K5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.slow_K5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['slow_D5'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.slow_D5.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaOC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaOC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaHC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaHC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaLC'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaLC.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaOH'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaOH.astype(np.float32)).reshape(-1, 1)))\n",
    "features_oos['deltaOL'] = quantile_transform(scaler.fit_transform(np.nan_to_num(features_oos.deltaOL.astype(np.float32)).reshape(-1, 1)))\n",
    "\n",
    "\n",
    "# Proceed an MaxAbsScaler on features\n",
    "#features_train,features_test,features_oos = scaling(features,x.replace('/',''),TRACKER,_start,_mid,_stop,_last,scaler=MaxAbsScaler())\n",
    "#features_train,features_test,features_oos = quantile(features_train,features_test,features_oos,quantile_transform)\n",
    "\n",
    "\n",
    "#_model = MLPClassifier(hidden_layer_sizes=(300,15), activation='relu', solver='adam', alpha=0.000001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=400, shuffle=True, random_state=26, tol=0.0000001, verbose=False, warm_start=True, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.8, beta_2=0.999, epsilon=1e-08)\n",
    "_model0 = joblib.load('0')\n",
    "_model1 = joblib.load('1')\n",
    "\n",
    "\n",
    "df_all_oos = df_all[(df_all.Date > _stop)&(df_all.Date <= _last)].dropna()\n",
    "\n",
    "df_all_oos = df_all_oos[df_all_oos.Symbol==x.replace('/','')]\n",
    "\n",
    "\n",
    "df_all_oos['Valid0'] = _model0.predict(features_oos.drop(['Date','Symbol','Signal','TRACKER','Valid'],axis=1))\n",
    "df_all_oos['Valid1'] = _model0.predict(features_oos.drop(['Date','Symbol','Signal','TRACKER','Valid'],axis=1))\n",
    "\n",
    "\n",
    "df_all_oos['Signal'] = np.where((df_all_oos.Signal==1)&((df_all_oos.Valid0==0)),1,np.where((df_all_oos.Signal==-1)&((df_all_oos.Valid0==0)),-1,0)) #&(df_all_oos.Valid1==1)\n",
    " \n",
    "##### On backtest selon le ticker selectionné sur la période déterminée\n",
    "\n",
    "joblib.dump(_model,'Save_'+x.replace('/','')+'_m5.sav')\n",
    "\n",
    "print(x)\n",
    "_year_bottom = _stop\n",
    "_year_top = _last\n",
    "\n",
    "#_trigger_invers = 1\n",
    "#_target = 0.017\n",
    "##### Backtest Over Night\n",
    "FINAL_TRACKER = bt(df_all_oos,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}