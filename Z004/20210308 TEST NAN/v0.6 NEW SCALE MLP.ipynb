{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7f9c69b77f8cb78a9d8b8acc2d09c3972908e6673afd8bfd04ee2f6acaaac495"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Importing Librairies...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import librairies.dagfeaturingfx \n",
    "from  librairies.dagfeaturingfx import *\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel,delayed\n",
    "import pyttsx3\n",
    "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler,quantile_transform,PolynomialFeatures\n",
    "from librairies.bt import *\n",
    "from sklearn.metrics import accuracy_score, make_scorer, precision_score, recall_score, precision_recall_curve, confusion_matrix, classification_report\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "import colorama as col\n",
    "from collections import Counter\n",
    "from librairies.strategy import *\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "engine = pyttsx3.init()\n",
    "engine.say(\"Librairies loaded\")\n",
    "engine.runAndWait() "
   ]
  },
  {
   "source": [
    "###### RELOAD LIBRAIRIE #####\n",
    "from importlib import reload\n",
    "sys.path.append('../')\n",
    "librairies.dagfeaturingfx= reload(librairies.dagfeaturingfx)\n",
    "from  librairies.dagfeaturingfx import *"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "%%time\n",
    "\n",
    "_t1 = dt.datetime.now()\n",
    "print('Début des opérations horodatée à',col.Fore.YELLOW,dt.datetime.now(),col.Style.RESET_ALL)\n",
    "\n",
    "TICKER_LIST = ['EUR/AUD']\n",
    "x=TICKER_LIST[0]\n",
    "_ticker = x.replace('/','')\n",
    "_period = 'm5'\n",
    "df_all = pd.read_csv(x.replace('/','')+'_'+_period+'_BidAndAsk.csv')\n",
    "\n",
    "##### Ajout de la colonne Symbol pour identifier le ticker\n",
    "df_all['Symbol'] = _ticker\n",
    "\n",
    "##### On fixe la date en index sous forme de Timestamp\n",
    "df_all['Lindex'] = pd.to_datetime(df_all['Date'] + ' ' + df_all['Time'])\n",
    "df_all.set_index(pd.to_datetime(df_all.Lindex,format='%Y-%m-%d %H:%M:%S'),drop=True,inplace=True)\n",
    "\n",
    "###### On drop les colonnes inutiles\n",
    "df_all = df_all.drop(['Date','Lindex','Time','Total Ticks'],axis=1)\n",
    "\n",
    "##### On enlève les jours correspondant au samedi et au dimanche\n",
    "df_all['WE'] = np.where(((df_all.index.weekday == 5) | (df_all.index.weekday == 6)),None,df_all.index.weekday)\n",
    "df_all = df_all.dropna()\n",
    "df_all = df_all.drop(['WE'],axis=1)\n",
    "\n",
    "##### Calcul des averages pour les OHLC\n",
    "df_all['Open'] = (df_all['OpenBid'] + df_all['OpenAsk']) / 2\n",
    "df_all['High'] = (df_all['HighBid'] + df_all['HighAsk']) / 2\n",
    "df_all['Low'] = (df_all['LowBid'] + df_all['LowAsk']) / 2\n",
    "df_all['Close'] = (df_all['CloseBid'] + df_all['CloseAsk']) / 2\n",
    "\n",
    "hourly_all = pd.read_csv(x.replace('/','')+'_H1_BidAndAsk.csv')\n",
    "\n",
    "##### Ajout de la colonne Symbol pour identifier le ticker\n",
    "hourly_all['Symbol'] = _ticker\n",
    "\n",
    "##### On fixe la date en index sous forme de Timestamp\n",
    "hourly_all['Lindex'] = pd.to_datetime(hourly_all['Date'] + ' ' + hourly_all['Time'])\n",
    "hourly_all.set_index(pd.to_datetime(hourly_all.Lindex,format='%Y-%m-%d %H:%M:%S'),drop=True,inplace=True)\n",
    "\n",
    "###### On drop les colonnes inutiles\n",
    "hourly_all = hourly_all.drop(['Date','Lindex','Time','Total Ticks'],axis=1)\n",
    "\n",
    "##### On enlève les jours correspondant au samedi et au dimanche\n",
    "hourly_all['WE'] = np.where(((hourly_all.index.weekday == 5) | (hourly_all.index.weekday == 6)),None,hourly_all.index.weekday)\n",
    "hourly_all = hourly_all.dropna()\n",
    "hourly_all = hourly_all.drop(['WE'],axis=1)\n",
    "\n",
    "##### Calcul des averages pour les OHLC\n",
    "hourly_all['Open'] = (hourly_all['OpenBid'] + hourly_all['OpenAsk']) / 2\n",
    "hourly_all['High'] = (hourly_all['HighBid'] + hourly_all['HighAsk']) / 2\n",
    "hourly_all['Low'] = (hourly_all['LowBid'] + hourly_all['LowAsk']) / 2\n",
    "hourly_all['Close'] = (hourly_all['CloseBid'] + hourly_all['CloseAsk']) / 2\n",
    "\n",
    "engine.say(\"Processing featuring of dataframes, daily and intra-day\")\n",
    "engine.runAndWait()\n",
    "##### Récupération des data pour tous les tickers sur la période demandée en intraday => df_all\n",
    "\n",
    "engine.say(\"Raw data are loaded in memory\")\n",
    "engine.runAndWait()\n",
    "\n",
    "\n",
    "df_all = timerange1D(df_all)\n",
    "hourly_all = timerange1D(hourly_all)\n",
    "daily_all = get_daily(hourly_all,TICKER_LIST)\n",
    "\n",
    "engine.say(\"Daily up to date\")\n",
    "engine.runAndWait()\n",
    "\n",
    "daily_all = timerange1W(daily_all)\n",
    "weekly_all = get_weekly(daily_all,TICKER_LIST)\n",
    "engine.say(\"Weekly up to date\")\n",
    "engine.runAndWait()\n",
    "\n",
    "##### On calcule l'ADR sur le daily\n",
    "daily_all = adr(daily_all,_window=14)\n",
    "engine.say(\"ADR computed\")\n",
    "engine.runAndWait()\n",
    "\n",
    "##### On récupère l'ADR qui a été calculé en daily (daily_all) pour le mettre dans la base intraday df_all\n",
    "df_all = getadr(daily_all,df_all,TICKER_LIST)\n",
    "engine.say(\"ADR get in daily\")\n",
    "engine.runAndWait()\n",
    "\n",
    "df_all = adrhnl(daily_all,df_all,TICKER_LIST)\n",
    "\n",
    "##### Calcul d'une SMA 200 sur df_all\n",
    "df_all = sma(df_all=df_all,_window=200)\n",
    "\n",
    "##### Calcul des bollinger sur df_all\n",
    "df_all = bollinger(df_all,_slow=20)\n",
    "\n",
    "##### Calcul du stochastic slow. Par défaut les paramètres sont 5 et 3.\n",
    "df_all = slowstochastic(df_all,TICKER_LIST)\n",
    "\n",
    "df_all = ema(df_all,21,TICKER_LIST)\n",
    "\n",
    "df_all = ema(df_all,8,TICKER_LIST)\n",
    "\n",
    "weekly_all = pivot(weekly_all,TICKER_LIST)\n",
    "\n",
    "df_all = pivotimportdf(df_all,weekly_all,TICKER_LIST)\n",
    "\n",
    "df_all = atr(df_all,TICKER_LIST,14)\n",
    "\n",
    "df_all = rvi(df_all,TICKER_LIST,_window=14)\n",
    "\n",
    "df_all = sbgamma(df_all,TICKER_LIST)\n",
    "\n",
    "df_all = onhisma(df_all,TICKER_LIST,_window=5)\n",
    "df_all = onlosma(df_all,TICKER_LIST,_window=5)\n",
    "\n",
    "df_all = onhisma(df_all,TICKER_LIST,_window=21)\n",
    "df_all = onlosma(df_all,TICKER_LIST,_window=21)\n",
    "\n",
    "df_all = onhisma(df_all,TICKER_LIST,_window=34)\n",
    "df_all = onlosma(df_all,TICKER_LIST,_window=34)\n",
    "\n",
    "df_all = importohlc(df_all,weekly_all,TICKER_LIST,_suffix='_weekly')\n",
    "\n",
    "df_all = importohlc(df_all=df_all,other_all=daily_all,TICKER_LIST=TICKER_LIST,_suffix='_daily')\n",
    "\n",
    "engine.say(\"The job is done\")\n",
    "engine.runAndWait()\n",
    "\n",
    "\n",
    "engine.say(\"AAll the bases are saved\")\n",
    "engine.runAndWait()\n",
    "\n",
    "_t2 = dt.datetime.now()\n",
    "print('Début des opérations horodatée à',col.Fore.YELLOW,dt.datetime.now(),col.Style.RESET_ALL)\n",
    "print((_t2 - _t1))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "%%time\n",
    "joblib.dump(df_all,x.replace('/','')+'_DF_ALL')\n",
    "joblib.dump(hourly_all,x.replace('/','')+'_HOURLY_ALL')\n",
    "joblib.dump(weekly_all,x.replace('/','')+'_WEEKLY_ALL')\n",
    "joblib.dump(daily_all,x.replace('/','')+'_DAILY_ALL')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 257 ms, sys: 298 ms, total: 554 ms\nWall time: 580 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x='EUR/USD'\n",
    "df_all = joblib.load(x.replace('/','')+'_DF_ALL')\n",
    "hourly_all = joblib.load(x.replace('/','')+'_HOURLY_ALL')\n",
    "weekly_all = joblib.load(x.replace('/','')+'_WEEKLY_ALL')\n",
    "daily_all = joblib.load(x.replace('/','')+'_DAILY_ALL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(808006, 57) (2884, 7) (581, 19)\n",
      "EUR/USD\n",
      "\n",
      "\u001b[31m ###############################################################################################\n",
      " ####################################### OOS WITHOUT AI ########################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-05 13:26:44.746158\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 82009/82009 [00:04<00:00, 16585.83it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[31m -52796.0 . En  1429 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 443 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 986 \u001b[0m\n",
      "BT's execution time 0:00:11.278968\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2019-12-30  à 2021-02-15 \u001b[0m\n",
      "\u001b[36m Total Number of trades 1429 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m -52796.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m -26.4% \u001b[0m\n",
      "Average trade duration 43.9\n",
      "# Winners  443.0\n",
      "# Winners long  209.0\n",
      "# Winners short  234.0\n",
      "# Loosers  986.0\n",
      "# Loosers  long 459.0\n",
      "# Loosers  short 527.0\n",
      "Cumulated gains 233124.0\n",
      "Cumulated losses -285920.0\n",
      "\u001b[34m PROFIT FACTOR :  0.82 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 31.0 % \u001b[0m\n",
      "Average Winners 526.24\n",
      "% Average Winners 0.26\n",
      "Average Loosers -289.98\n",
      "% Average Loosers -0.14\n",
      "Average pnl -36.95\n",
      "% Average pnl -0.06\n",
      "Number of opened trades 1429\n",
      "Number of closed trades 1429\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "\n",
      "\u001b[36m ###############################################################################################\n",
      " #################################### TRAIN/TEST WITHOUT AI ####################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-05 13:26:59.207073\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 721519/721519 [00:42<00:00, 16966.07it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[31m -598160.0 . En  12887 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 4052 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 8835 \u001b[0m\n",
      "BT's execution time 0:00:49.307361\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2010-01-01  à 2019-12-30 \u001b[0m\n",
      "\u001b[36m Total Number of trades 12887 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m -598160.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m -299.08% \u001b[0m\n",
      "Average trade duration 40.1\n",
      "# Winners  4052.0\n",
      "# Winners long  2045.0\n",
      "# Winners short  2007.0\n",
      "# Loosers  8835.0\n",
      "# Loosers  long 4620.0\n",
      "# Loosers  short 4215.0\n",
      "Cumulated gains 2322714.0\n",
      "Cumulated losses -2920874.0\n",
      "\u001b[34m PROFIT FACTOR :  0.8 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 31.44 % \u001b[0m\n",
      "Average Winners 573.23\n",
      "% Average Winners 0.29\n",
      "Average Loosers -330.6\n",
      "% Average Loosers -0.17\n",
      "Average pnl -46.42\n",
      "% Average pnl -0.26\n",
      "Number of opened trades 12887\n",
      "Number of closed trades 12887\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "\n",
      "\u001b[34m ###############################################################################################\n",
      " #################################### DENOISING & ENHANCING ####################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-05 13:27:52.964357\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 721519/721519 [00:23<00:00, 30544.92it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[32m 2322700.0 . En  4051 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 4051 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 0 \u001b[0m\n",
      "BT's execution time 0:00:30.421711\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2010-01-01  à 2019-12-30 \u001b[0m\n",
      "\u001b[36m Total Number of trades 4051 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L  in currency: \u001b[32m 2322700.0$ \u001b[0m\n",
      "P&L in %: \u001b[32m 1161.35% \u001b[0m\n",
      "Average trade duration 55.59\n",
      "# Winners  4051.0\n",
      "# Winners long  2044.0\n",
      "# Winners short  2007.0\n",
      "# Loosers  0.0\n",
      "# Loosers  long 0.0\n",
      "# Loosers  short 0.0\n",
      "Cumulated gains 2322700.0\n",
      "Cumulated losses 0.0\n",
      "\u001b[34m PROFIT FACTOR :  inf \u001b[0m\n",
      "\u001b[36m Winners Ratio : 100.0 % \u001b[0m\n",
      "Average Winners 573.36\n",
      "% Average Winners 0.29\n",
      "No looser\n",
      "Average pnl 573.36\n",
      "% Average pnl 2.15\n",
      "Number of opened trades 4051\n",
      "Number of closed trades 4051\n",
      "Max Exposure 1 x  200000 =  200000 $\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \"\"\"\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    362\u001b[0m                       (not self.warm_start and not incremental))\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental, reset)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc'],\n\u001b[0m\u001b[1;32m    969\u001b[0m                                    \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "####### CHOIX DE LA TIME FRAME\n",
    "_period = 'm5'\n",
    "##### Récupération de la liste des tickers à partir du répertoire où sont rangées les bases\n",
    "TICKER_LIST = ['EUR/USD']\n",
    "TIK = ['JPY','CHF','CAD','GBP','AUD','NZD','SEK','NOK','MXN','ZAR','TRY','ILS','CNH','USD','HKD','0']\n",
    "RATE = [105.4,0.8989,1.276,0.877,0.7676,0.7201,8.3684,8.5238,20.09,14.81,7.03,3.28,6.458,1,7.7527,1.21]\n",
    "df_ratefx = pd.DataFrame(index=TIK)\n",
    "df_ratefx['rate'] = RATE\n",
    "#############################\n",
    "##### F E A T U R I N G #####\n",
    "#############################\n",
    "\n",
    "print(df_all.shape,daily_all.shape,weekly_all.shape)\n",
    "# Make our choice for the split, compliant to our backtest \n",
    "_start = '2010-01-01' # start the train there '2010-01-01'\n",
    "_mid = '2018-08-29' # stop the train and begin the test there '2016-08-31'\n",
    "_stop = '2019-12-30' # stop the test there. After that, it is kept for oos\n",
    "_last = '2021-02-15' # '2020-12-31'\n",
    "#### S T R A T E G Y\n",
    "\n",
    "df_all = stochastic(df_all)\n",
    "##### On backtest selon le ticker selectionné sur la période déterminée\n",
    "\n",
    "x=TICKER_LIST[0]\n",
    "_ticker = x.replace('/','')\n",
    "print(x)\n",
    "_year_bottom = _stop\n",
    "_year_top = _last\n",
    "_nb_bougie_exit = 2000000\n",
    "_trigger_reengage = 0\n",
    "_trigger_target = 1\n",
    "_trigger_invers = 0\n",
    "_trigger_sl = 1\n",
    "_verbose = 0\n",
    "_cash_ini = 200000\n",
    "_rate = 1/df_ratefx.loc[x[4:],'rate']\n",
    "backtest = df_all[df_all.Symbol == x.replace('/','')]\n",
    "_target =  0.002\n",
    "_exposure = 10\n",
    "_size = 200000\n",
    "_sl =  0.001\n",
    "_open_hour = 8 # day only\n",
    "_close_hour = 23 # day only\n",
    "_window = 0 # day only\n",
    "print()\n",
    "print(col.Fore.RED,'###############################################################################################')\n",
    "print(' ####################################### OOS WITHOUT AI ########################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "##### Backtest Over Night\n",
    "TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "print()\n",
    "print(col.Fore.CYAN,'###############################################################################################')\n",
    "print(' #################################### TRAIN/TEST WITHOUT AI ####################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "_year_bottom = _start\n",
    "_year_top = _stop\n",
    "\n",
    "TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "print()\n",
    "print(col.Fore.BLUE,'###############################################################################################')\n",
    "print(' #################################### DENOISING & ENHANCING ####################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "while _nb_looser > 0 :\n",
    "    \n",
    "    df_all['TRACKER'] = np.where(df_all.index.isin(TRACKER),1,0)\n",
    "    df_all['Valid'] = np.where(((df_all.Signal!=0)&(df_all.TRACKER==1)),1,0)\n",
    "    df_all['Signal'] = np.where(((df_all.Valid==1)&(df_all.Signal==1)),1,np.where(((df_all.Valid==1)&(df_all.Signal==-1)),-1,0))\n",
    "    backtest = df_all[df_all.Symbol == x.replace('/','')]\n",
    "    ##### Purification of signal by denoising and enhancing\n",
    "    TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "            _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "df_all = joblib.load(x.replace('/','')+'_DF_ALL')\n",
    "df_all = stochastic(df_all)\n",
    "#features = featuring(df_all)\n",
    "\n",
    "\n",
    "\n",
    "features = pd.DataFrame(index=df_all.index)\n",
    "features['Symbol'] = df_all['Symbol']\n",
    "features['Date'] = df_all['Date']\n",
    "features['FEMA_21'] = np.nan_to_num((df_all['Close'] - df_all['EMA_21']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FEMA_8'] = np.nan_to_num((df_all['Close'] - df_all['EMA_8']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FADRLo'] = np.nan_to_num((df_all['Close'] - df_all['ADR_Low']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FADRHi'] = np.nan_to_num((df_all['Close'] - df_all['ADR_High']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FRVI40'] = np.nan_to_num((df_all['RVI'] - 40).astype(np.float32)).reshape(-1, 1)\n",
    "features['FRVI60'] = np.nan_to_num((df_all['RVI'] - 60).astype(np.float32)).reshape(-1, 1)\n",
    "features['FONLOSMA5'] = np.nan_to_num((df_all['Low'] - df_all['ONLOSMA_5']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FONHISMA5'] = np.nan_to_num((df_all['High'] - df_all['ONHISMA_5']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FONLOSMA21'] = np.nan_to_num((df_all['Low'] - df_all['ONLOSMA_21']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FONHISMA21'] = np.nan_to_num((df_all['High'] - df_all['ONHISMA_21']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FONLOSMA34'] = np.nan_to_num((df_all['Low'] - df_all['ONLOSMA_34']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FONHISMA34'] = np.nan_to_num((df_all['High'] - df_all['ONHISMA_34']).astype(np.float32)).reshape(-1, 1)\n",
    "#features['FSBGAMMA'] = np.nan_to_num(df_all.SB_Gamma.astype(np.float32)).reshape(-1, 1)\n",
    "features['FOPENWEEKLY'] = np.nan_to_num((df_all['Close'] - df_all['Open_weekly']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FHIGHWEEKLY'] = np.nan_to_num((df_all['Close'] - df_all['High_weekly']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FLOWWEEKLY'] = np.nan_to_num((df_all['Close'] - df_all['Low_weekly']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FCLOSEWEEKLY'] = np.nan_to_num((df_all['Close'] - df_all['Close_weekly']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FOPENDAILY'] = np.nan_to_num((df_all['Close'] - df_all['Open_daily']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FHIGHDAILY'] = np.nan_to_num((df_all['Close'] - df_all['High_daily']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FLOWDAILY'] = np.nan_to_num((df_all['Close'] - df_all['Low_daily']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FCLOSEDAILY'] = np.nan_to_num((df_all['Close'] - df_all['Close_daily']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FOPENHOURLY'] = np.nan_to_num((df_all['Close'] - df_all['Open_daily']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FHIGHHOURLY'] = np.nan_to_num((df_all['Close'] - df_all['High_daily']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FLOWHOURLY'] = np.nan_to_num((df_all['Close'] - df_all['Low_daily']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FCLOSEHOURLY'] = np.nan_to_num((df_all['Close'] - df_all['Close_daily']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FSMA200'] = np.nan_to_num((df_all['Close'] - df_all['SMA_200']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FBOLUP20'] = np.nan_to_num((df_all['Close'] - df_all['UpperBand']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FBOLLOW20'] = np.nan_to_num((df_all['Close'] - df_all['LowerBand']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FPP'] = np.nan_to_num((df_all['Close'] - df_all['PP']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FS38'] = np.nan_to_num((df_all['Close'] - df_all['S38']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FS62'] = np.nan_to_num((df_all['Close'] - df_all['S62']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FS100'] = np.nan_to_num((df_all['Close'] - df_all['S100']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FS138'] = np.nan_to_num((df_all['Close'] - df_all['S138']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FS162'] = np.nan_to_num((df_all['Close'] - df_all['S162']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FS200'] = np.nan_to_num((df_all['Close'] - df_all['S200']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FR38'] = np.nan_to_num((df_all['Close'] - df_all['R38']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FR62'] = np.nan_to_num((df_all['Close'] - df_all['R62']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FR100'] = np.nan_to_num((df_all['Close'] - df_all['R100']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FR138'] = np.nan_to_num((df_all['Close'] - df_all['R138']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FR162'] = np.nan_to_num((df_all['Close'] - df_all['R162']).astype(np.float32)).reshape(-1, 1)\n",
    "features['FR200'] = np.nan_to_num((df_all['Close'] - df_all['R200']).astype(np.float32)).reshape(-1, 1)\n",
    "features['SBATR'] = np.nan_to_num(((df_all['Close'] - df_all['Open']) / df_all['ATR_14']).astype(np.float32)).reshape(-1, 1)\n",
    "features['Signal'] = np.nan_to_num((df_all['Signal']).astype(np.float32)).reshape(-1, 1)\n",
    "\n",
    "features['TRACKER'] = np.nan_to_num(np.where(features.index.isin(TRACKER),1,0).astype(np.float32)).reshape(-1, 1)\n",
    "\n",
    "# First, we must have an output. We'll call it 'Valid'. It wil be where Tracker & Signal are both to 1\n",
    "features['Valid'] = np.nan_to_num(np.where(((features.Signal!=0)&(features.TRACKER==1)),1,0).astype(np.float32)).reshape(-1, 1) # Don't miss the point that even a Signal -1 must be considered as a good one by TRACKER\n",
    "\n",
    "# Let's isolate the ticker on which we made the test\n",
    "features = features[features.Symbol==x.replace('/','')]\n",
    "\n",
    "# And drop the nan\n",
    "features = features.dropna()\n",
    "##### Signal is from strategy. This is potential good one. But we have to create the TRACKER column where the Signal where efficient\n",
    "\n",
    "a = len(features)\n",
    "features['FEMA_21'] = np.nan_to_num(features['FEMA_21'] / features['FEMA_21'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FEMA_8'] = np.nan_to_num(features['FEMA_8'] / features['FEMA_8'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FADRLo'] = np.nan_to_num(features['FADRLo'] / features['FADRLo'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FADRHi'] = np.nan_to_num(features['FADRHi'] / features['FADRHi'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FRVI40'] = np.nan_to_num(features['FRVI40'] / features['FRVI40'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FRVI60'] = np.nan_to_num(features['FRVI60'] / features['FRVI60'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FONLOSMA5'] = np.nan_to_num(features['FONLOSMA5'] / features['FONLOSMA5'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FONHISMA5'] = np.nan_to_num(features['FONHISMA5'] / features['FONHISMA5'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FONLOSMA21'] = np.nan_to_num(features['FONLOSMA21'] / features['FONLOSMA21'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FONHISMA21'] = np.nan_to_num(features['FONHISMA21'] / features['FONHISMA21'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FONLOSMA34'] = np.nan_to_num(features['FONLOSMA34'] / features['FONLOSMA34'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FONHISMA34'] = np.nan_to_num(features['FONHISMA34'] / features['FONHISMA34'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "#features['FSBGAMMA'] = np.nan_to_num(df_all.SB_Gamma.astype(np.float32)).reshape(-1, 1)\n",
    "features['FOPENWEEKLY'] = np.nan_to_num(features['FOPENWEEKLY'] / features['FOPENWEEKLY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FHIGHWEEKLY'] = np.nan_to_num(features['FHIGHWEEKLY'] / features['FHIGHWEEKLY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FLOWWEEKLY'] = np.nan_to_num(features['FLOWWEEKLY'] / features['FLOWWEEKLY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FCLOSEWEEKLY'] = np.nan_to_num(features['FCLOSEWEEKLY'] / features['FCLOSEWEEKLY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FOPENDAILY'] = np.nan_to_num(features['FOPENDAILY'] / features['FOPENDAILY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FHIGHDAILY'] = np.nan_to_num(features['FHIGHDAILY'] / features['FHIGHDAILY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FLOWDAILY'] = np.nan_to_num(features['FLOWDAILY'] / features['FLOWDAILY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FCLOSEDAILY'] = np.nan_to_num(features['FCLOSEDAILY'] / features['FCLOSEDAILY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FOPENHOURLY'] = np.nan_to_num(features['FOPENHOURLY'] / features['FOPENHOURLY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FHIGHHOURLY'] = np.nan_to_num(features['FHIGHHOURLY'] / features['FHIGHHOURLY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FLOWHOURLY'] = np.nan_to_num(features['FLOWHOURLY'] / features['FLOWHOURLY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FCLOSEHOURLY'] = np.nan_to_num(features['FCLOSEHOURLY'] / features['FCLOSEHOURLY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FSMA200'] = np.nan_to_num(features['FSMA200'] / features['FSMA200'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FBOLUP20'] = np.nan_to_num(features['FBOLUP20'] / features['FBOLUP20'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FBOLLOW20'] = np.nan_to_num(features['FBOLLOW20'] / features['FBOLLOW20'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FPP'] = np.nan_to_num(features['FPP'] / features['FPP'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FS38'] = np.nan_to_num(features['FS38'] / features['FS38'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FS62'] = np.nan_to_num(features['FS62'] / features['FS62'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FS100'] = np.nan_to_num(features['FS100'] / features['FS100'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FS138'] = np.nan_to_num(features['FS138'] / features['FS138'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FS162'] = np.nan_to_num(features['FS162'] / features['FS162'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FS200'] = np.nan_to_num(features['FS200'] / features['FS200'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FR38'] = np.nan_to_num(features['FR38'] / features['FR38'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FR62'] = np.nan_to_num(features['FR62'] / features['FR62'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FR100'] = np.nan_to_num(features['FR100'] / features['FR100'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FR138'] = np.nan_to_num(features['FR138'] / features['FR138'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FR162'] = np.nan_to_num(features['FR162'] / features['FR162'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FR200'] = np.nan_to_num(features['FR200'] / features['FR200'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['SBATR'] = np.nan_to_num(features['SBATR'] / features['SBATR'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "features_train = features[(features.Date>=_start)&(features.Date<=_mid)]\n",
    "features_test = features[(features.Date>_mid)&(features.Date<=_stop)]\n",
    "features_oos = features[(features.Date>_stop)&(features.Date <= _last)]\n",
    "features_train = features_train[features_train.Signal!=0]\n",
    "features_test = features_test[features_test.Signal!=0]\n",
    "# Proceed an MaxAbsScaler on features\n",
    "#features_train,features_test,features_oos = scaling(features,x.replace('/',''),TRACKER,_start,_mid,_stop,_last,scaler=MaxAbsScaler())\n",
    "#features_train,features_test,features_oos = quantile(features_train,features_test,features_oos,quantile_transform)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "_name = 'MLPClassifier'\n",
    "_model = MLPClassifier(hidden_layer_sizes=(300,15), activation='relu', solver='adam', alpha=0.000001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=400, shuffle=True, random_state=26, tol=0.0000001, verbose=False, warm_start=True, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.8, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "_model.fit(features_train.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1),features_train.Valid)\n",
    "yhat = _model.predict(features_test.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1))\n",
    "accu = round(accuracy_score(features_test.Valid, yhat) * 100,2)\n",
    "prec = round(precision_score(features_test.Valid, yhat,pos_label=1) * 100,2)\n",
    "recall = round(recall_score(features_test.Valid, yhat) * 100,2)\n",
    "f1 = round(f1_score(features_test.Valid, yhat) * 100,2)\n",
    "\n",
    "\n",
    "print('Signaux - Accuracy :' ,accu,'%')\n",
    "print('Signaux - Precision :',prec,'%')\n",
    "print('Signaux - Recall :', recall,'%')\n",
    "print('Achat - F-measure: :' ,f1,'%')\n",
    "print('\\n')\n",
    "print(classification_report(features_test.Valid, yhat, target_names=['Neg', 'Pos']))\n",
    "conf_matrix = pd.DataFrame(columns=['Bonnes_Estimations','Mauvaises_Estimations'])\n",
    "tt = confusion_matrix(features_test.Valid, yhat, labels=[1,0])    #_model.classes_)\n",
    "\n",
    "conf_matrix.loc['vrais-réels'] = tt[0]\n",
    "conf_matrix.loc['faux-réels'] = tt[1]\n",
    "\n",
    "print(conf_matrix)\n",
    "print()\n",
    "print(col.Fore.BLUE,'Signaux pour',col.Fore.YELLOW,x,col.Style.RESET_ALL)\n",
    "_tp = tt[0][0]\n",
    "_fn = tt[0][1]\n",
    "_prec = round((tt[0][0]/(tt[0][0]+tt[0][1]))*100,2)\n",
    "_rec = round((tt[0][1]/(tt[0][0]+tt[0][1]))*100,2)\n",
    "\n",
    "print('Vrais signaux trouvés    : ',tt[0][0])\n",
    "print('Vrais signaux non trouvé :',tt[0][1])\n",
    "print('Total des vrais signaux  :',tt[0][0]+tt[0][1])\n",
    "if _prec > 69 and prec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif _prec < 51 or prec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "print('Precision : ',_prec,'%')\n",
    "if _rec > 69 and _rec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif _rec < 51 or _rec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "print('Recall',_rec,'%')\n",
    "print(col.Style.RESET_ALL)\n",
    "\n",
    "df_all_oos = df_all[(df_all.Date > _stop)&(df_all.Date <= _last)]#.dropna()\n",
    "\n",
    "df_all_oos = df_all_oos[df_all_oos.Symbol==x.replace('/','')]\n",
    "\n",
    "\n",
    "df_all_oos['Valid'] = _model.predict(features_oos.drop(['Date','Symbol','Signal','TRACKER','Valid'],axis=1))\n",
    "\n",
    "\n",
    "df_all_oos['Signal'] = np.where((df_all_oos.Signal==1)&(df_all_oos.Valid==1),1,np.where((df_all_oos.Signal==-1)&(df_all_oos.Valid==1),-1,0))\n",
    " \n",
    "##### On backtest selon le ticker selectionné sur la période déterminée\n",
    "\n",
    "#joblib.dump(_model,'Save_'+x.replace('/','')+'_m5.sav')\n",
    "\n",
    "print(x)\n",
    "_year_bottom = _stop\n",
    "_year_top = _last\n",
    "\n",
    "#_trigger_invers = 1\n",
    "#_target = 0.017\n",
    "##### Backtest Over Night\n",
    "FINAL_TRACKER = bt(df_all_oos,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "checking for infinity\n",
      "\n",
      "printing the count of infinity values\n",
      "It contains 9208 infinite values\n",
      "It contains 9208 infinite values\n",
      "\n",
      "printing column name where infinity is present\n",
      "SB_Gamma    SB_Gamma\n",
      "dtype: object\n",
      "\n",
      "printing len(r),len(ds),len(df)\n",
      "9208 803814 803814\n",
      "CPU times: user 2.1 s, sys: 817 ms, total: 2.92 s\n",
      "Wall time: 3.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_all = joblib.load(x.replace('/','')+'_DF_ALL')\n",
    "df_all = stochastic(df_all)\n",
    "check_inf(df_all.drop(['Date','Symbol'],axis=1).dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 4.57 s, sys: 2.56 s, total: 7.13 s\nWall time: 6.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_all = joblib.load(x.replace('/','')+'_DF_ALL')\n",
    "df_all = stochastic(df_all)\n",
    "#features = featuring(df_all)\n",
    "\n",
    "\n",
    "\n",
    "features = pd.DataFrame(index=df_all.index)\n",
    "features['Symbol'] = df_all['Symbol']\n",
    "features['Date'] = df_all['Date']\n",
    "features['FEMA_21'] = (df_all['Close'] - df_all['EMA_21'])\n",
    "features['FEMA_8'] = (df_all['Close'] - df_all['EMA_8'])\n",
    "features['FADRLo'] = (df_all['Close'] - df_all['ADR_Low'])\n",
    "features['FADRHi'] = (df_all['Close'] - df_all['ADR_High'])\n",
    "features['FRVI40'] = (df_all['RVI'] - 40)\n",
    "features['FRVI60'] = (df_all['RVI'] - 60)\n",
    "features['FONLOSMA5'] = (df_all['Low'] - df_all['ONLOSMA_5'])\n",
    "features['FONHISMA5'] = (df_all['High'] - df_all['ONHISMA_5'])\n",
    "features['FONLOSMA21'] = (df_all['Low'] - df_all['ONLOSMA_21'])\n",
    "features['FONHISMA21'] = (df_all['High'] - df_all['ONHISMA_21'])\n",
    "features['FONLOSMA34'] = (df_all['Low'] - df_all['ONLOSMA_34'])\n",
    "features['FONHISMA34'] = (df_all['High'] - df_all['ONHISMA_34'])\n",
    "features['FSBGAMMA'] = df_all.SB_Gamma\n",
    "features['FOPENWEEKLY'] = (df_all['Close'] - df_all['Open_weekly'])\n",
    "features['FHIGHWEEKLY'] = (df_all['Close'] - df_all['High_weekly'])\n",
    "features['FLOWWEEKLY'] = (df_all['Close'] - df_all['Low_weekly'])\n",
    "features['FCLOSEWEEKLY'] = (df_all['Close'] - df_all['Close_weekly'])\n",
    "features['FOPENDAILY'] = (df_all['Close'] - df_all['Open_daily'])\n",
    "#features['FHIGHDAILY'] = (df_all['Close'] - df_all['High_daily'])\n",
    "features['FLOWDAILY'] = (df_all['Close'] - df_all['Low_daily'])\n",
    "features['FCLOSEDAILY'] = (df_all['Close'] - df_all['Close_daily'])\n",
    "features['FOPENHOURLY'] = (df_all['Close'] - df_all['Open_daily'])\n",
    "#features['FHIGHHOURLY'] = (df_all['Close'] - df_all['High_daily'])\n",
    "features['FLOWHOURLY'] = (df_all['Close'] - df_all['Low_daily'])\n",
    "features['FCLOSEHOURLY'] = (df_all['Close'] - df_all['Close_daily'])\n",
    "features['FSMA200'] = (df_all['Close'] - df_all['SMA_200'])\n",
    "features['FBOLUP20'] = (df_all['Close'] - df_all['UpperBand'])\n",
    "features['FBOLLOW20'] = (df_all['Close'] - df_all['LowerBand'])\n",
    "features['FPP'] = (df_all['Close'] - df_all['PP'])\n",
    "features['FS38'] = (df_all['Close'] - df_all['S38'])\n",
    "features['FS62'] = (df_all['Close'] - df_all['S62'])\n",
    "features['FS100'] = (df_all['Close'] - df_all['S100'])\n",
    "features['FS138'] = (df_all['Close'] - df_all['S138'])\n",
    "features['FS162'] = (df_all['Close'] - df_all['S162'])\n",
    "features['FS200'] = (df_all['Close'] - df_all['S200'])\n",
    "features['FR38'] = (df_all['Close'] - df_all['R38'])\n",
    "features['FR62'] = (df_all['Close'] - df_all['R62'])\n",
    "features['FR100'] = (df_all['Close'] - df_all['R100'])\n",
    "features['FR138'] = (df_all['Close'] - df_all['R138'])\n",
    "features['FR162'] = (df_all['Close'] - df_all['R162'])\n",
    "features['FR200'] = (df_all['Close'] - df_all['R200'])\n",
    "features['SBATR'] = ((df_all['Close'] - df_all['Open']) / df_all['ATR_14'])\n",
    "features['Signal'] = (df_all['Signal'])\n",
    "\n",
    "features['TRACKER'] = np.where(features.index.isin(TRACKER),1,0)\n",
    "\n",
    "# First, we must have an output. We'll call it 'Valid'. It wil be where Tracker & Signal are both to 1\n",
    "features['Valid'] = np.where(((features.Signal!=0)&(features.TRACKER==1)),1,0) # Don't miss the point that even a Signal -1 must be considered as a good one by TRACKER\n",
    "\n",
    "# Let's isolate the ticker on which we made the test\n",
    "features = features[features.Symbol==x.replace('/','')]\n",
    "\n",
    "# And drop the nan\n",
    "features = features.dropna()\n",
    "##### Signal is from strategy. This is potential good one. But we have to create the TRACKER column where the Signal where efficient\n",
    "\n",
    "a = len(features)\n",
    "features['FEMA_21'] = np.nan_to_num(features['FEMA_21'] / abs(features['FEMA_21'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FEMA_8'] = np.nan_to_num(features['FEMA_8'] / abs(features['FEMA_8'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FADRLo'] = np.nan_to_num(features['FADRLo'] / abs(features['FADRLo'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FADRHi'] = np.nan_to_num(features['FADRHi'] / abs(features['FADRHi'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FRVI40'] = np.nan_to_num(features['FRVI40'] / abs(features['FRVI40'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FRVI60'] = np.nan_to_num(features['FRVI60'] / abs(features['FRVI60'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FONLOSMA5'] = np.nan_to_num(features['FONLOSMA5'] / abs(features['FONLOSMA5'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FONHISMA5'] = np.nan_to_num(features['FONHISMA5'] / abs(features['FONHISMA5'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FONLOSMA21'] = np.nan_to_num(features['FONLOSMA21'] / abs(features['FONLOSMA21'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FONHISMA21'] = np.nan_to_num(features['FONHISMA21'] / abs(features['FONHISMA21'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FONLOSMA34'] = np.nan_to_num(features['FONLOSMA34'] / abs(features['FONLOSMA34'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FONHISMA34'] = np.nan_to_num(features['FONHISMA34'] / abs(features['FONHISMA34'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FSBGAMMA'] = np.nan_to_num(features['FSBGAMMA'] / abs(features['FSBGAMMA'].rolling(a, min_periods=1).max()).astype(np.float32)).reshape(-1, 1)\n",
    "features['FOPENWEEKLY'] = np.nan_to_num(features['FOPENWEEKLY'] / abs(features['FOPENWEEKLY'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FHIGHWEEKLY'] = np.nan_to_num(features['FHIGHWEEKLY'] / abs(features['FHIGHWEEKLY'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FLOWWEEKLY'] = np.nan_to_num(features['FLOWWEEKLY'] / abs(features['FLOWWEEKLY'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FCLOSEWEEKLY'] = np.nan_to_num(features['FCLOSEWEEKLY'] / abs(features['FCLOSEWEEKLY'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FOPENDAILY'] = np.nan_to_num(features['FOPENDAILY'] / abs(features['FOPENDAILY'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "#features['FHIGHDAILY'] = np.nan_to_num(features['FHIGHDAILY'] / features['FHIGHDAILY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FLOWDAILY'] = np.nan_to_num(features['FLOWDAILY'] / abs(features['FLOWDAILY'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FCLOSEDAILY'] = np.nan_to_num(features['FCLOSEDAILY'] / abs(features['FCLOSEDAILY'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FOPENHOURLY'] = np.nan_to_num(features['FOPENHOURLY'] / abs(features['FOPENHOURLY'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "#features['FHIGHHOURLY'] = np.nan_to_num(features['FHIGHHOURLY'] / features['FHIGHHOURLY'].rolling(a, min_periods=1).max()).astype(np.float32).reshape(-1, 1)\n",
    "features['FLOWHOURLY'] = np.nan_to_num(features['FLOWHOURLY'] / abs(features['FLOWHOURLY'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FCLOSEHOURLY'] = np.nan_to_num(features['FCLOSEHOURLY'] / abs(features['FCLOSEHOURLY'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FSMA200'] = np.nan_to_num(features['FSMA200'] / abs(features['FSMA200'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FBOLUP20'] = np.nan_to_num(features['FBOLUP20'] / abs(features['FBOLUP20'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FBOLLOW20'] = np.nan_to_num(features['FBOLLOW20'] / abs(features['FBOLLOW20'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FPP'] = np.nan_to_num(features['FPP'] / abs(features['FPP'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FS38'] = np.nan_to_num(features['FS38'] / abs(features['FS38'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FS62'] = np.nan_to_num(features['FS62'] / abs(features['FS62'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FS100'] = np.nan_to_num(features['FS100'] / abs(features['FS100'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FS138'] = np.nan_to_num(features['FS138'] / abs(features['FS138'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FS162'] = np.nan_to_num(features['FS162'] / abs(features['FS162'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FS200'] = np.nan_to_num(features['FS200'] / abs(features['FS200'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FR38'] = np.nan_to_num(features['FR38'] / abs(features['FR38'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FR62'] = np.nan_to_num(features['FR62'] / abs(features['FR62'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FR100'] = np.nan_to_num(features['FR100'] / abs(features['FR100'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FR138'] = np.nan_to_num(features['FR138'] / abs(features['FR138'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FR162'] = np.nan_to_num(features['FR162'] / abs(features['FR162'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['FR200'] = np.nan_to_num(features['FR200'] / abs(features['FR200'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n",
    "features['SBATR'] = np.nan_to_num(features['SBATR'] / abs(features['SBATR'].rolling(a, min_periods=1).max())).astype(np.float32).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "checking for NaN\n",
      "\n",
      "printing the count of NaN values\n",
      "It contains 0 NaN values\n",
      "It contains 0 NaN values\n",
      "\n",
      "printing column name where NaN is present\n",
      "Series([], dtype: object)\n",
      "\n",
      "printing len(r),len(ds),len(df)\n",
      "0 803814 803814\n"
     ]
    }
   ],
   "source": [
    "check_nan(features.drop(['Date','Symbol','TRACKER'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "checking for infinity\n",
      "\n",
      "printing the count of infinity values\n",
      "It contains 0 infinite values\n",
      "It contains 0 infinite values\n",
      "\n",
      "printing column name where infinity is present\n",
      "Series([], dtype: object)\n",
      "\n",
      "printing len(r),len(ds),len(df)\n",
      "0 803814 803814\n"
     ]
    }
   ],
   "source": [
    "check_inf(features.drop(['Date','Symbol','TRACKER'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(808006, 59) (2884, 7) (581, 19)\n",
      "EUR/USD\n",
      "\n",
      "\u001b[31m ###############################################################################################\n",
      " ####################################### OOS WITHOUT AI ########################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-05 16:31:08.368860\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 82009/82009 [00:07<00:00, 11197.11it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[31m -52796.0 . En  1429 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 443 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 986 \u001b[0m\n",
      "BT's execution time 0:00:13.761840\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2019-12-30  à 2021-02-15 \u001b[0m\n",
      "\u001b[36m Total Number of trades 1429 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m -52796.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m -26.4% \u001b[0m\n",
      "Average trade duration 43.9\n",
      "# Winners  443.0\n",
      "# Winners long  209.0\n",
      "# Winners short  234.0\n",
      "# Loosers  986.0\n",
      "# Loosers  long 459.0\n",
      "# Loosers  short 527.0\n",
      "Cumulated gains 233124.0\n",
      "Cumulated losses -285920.0\n",
      "\u001b[34m PROFIT FACTOR :  0.82 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 31.0 % \u001b[0m\n",
      "Average Winners 526.24\n",
      "% Average Winners 0.26\n",
      "Average Loosers -289.98\n",
      "% Average Loosers -0.14\n",
      "Average pnl -36.95\n",
      "% Average pnl -0.06\n",
      "Number of opened trades 1429\n",
      "Number of closed trades 1429\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "\n",
      "\u001b[36m ###############################################################################################\n",
      " #################################### TRAIN/TEST WITHOUT AI ####################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-05 16:31:25.365916\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 721519/721519 [01:20<00:00, 8920.42it/s] \n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[31m -598160.0 . En  12887 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 4052 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 8835 \u001b[0m\n",
      "BT's execution time 0:01:28.003891\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2010-01-01  à 2019-12-30 \u001b[0m\n",
      "\u001b[36m Total Number of trades 12887 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m -598160.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m -299.08% \u001b[0m\n",
      "Average trade duration 40.1\n",
      "# Winners  4052.0\n",
      "# Winners long  2045.0\n",
      "# Winners short  2007.0\n",
      "# Loosers  8835.0\n",
      "# Loosers  long 4620.0\n",
      "# Loosers  short 4215.0\n",
      "Cumulated gains 2322714.0\n",
      "Cumulated losses -2920874.0\n",
      "\u001b[34m PROFIT FACTOR :  0.8 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 31.44 % \u001b[0m\n",
      "Average Winners 573.23\n",
      "% Average Winners 0.29\n",
      "Average Loosers -330.6\n",
      "% Average Loosers -0.17\n",
      "Average pnl -46.42\n",
      "% Average pnl -0.26\n",
      "Number of opened trades 12887\n",
      "Number of closed trades 12887\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "\n",
      "\u001b[34m ###############################################################################################\n",
      " #################################### DENOISING & ENHANCING ####################################\n",
      " ############################################################################################### \u001b[0m\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-05 16:32:58.045287\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 721519/721519 [00:24<00:00, 28880.07it/s]\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[32m 2322700.0 . En  4051 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 4051 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 0 \u001b[0m\n",
      "BT's execution time 0:00:32.136064\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2010-01-01  à 2019-12-30 \u001b[0m\n",
      "\u001b[36m Total Number of trades 4051 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L  in currency: \u001b[32m 2322700.0$ \u001b[0m\n",
      "P&L in %: \u001b[32m 1161.35% \u001b[0m\n",
      "Average trade duration 55.59\n",
      "# Winners  4051.0\n",
      "# Winners long  2044.0\n",
      "# Winners short  2007.0\n",
      "# Loosers  0.0\n",
      "# Loosers  long 0.0\n",
      "# Loosers  short 0.0\n",
      "Cumulated gains 2322700.0\n",
      "Cumulated losses 0.0\n",
      "\u001b[34m PROFIT FACTOR :  inf \u001b[0m\n",
      "\u001b[36m Winners Ratio : 100.0 % \u001b[0m\n",
      "Average Winners 573.36\n",
      "% Average Winners 0.29\n",
      "No looser\n",
      "Average pnl 573.36\n",
      "% Average pnl 2.15\n",
      "Number of opened trades 4051\n",
      "Number of closed trades 4051\n",
      "Max Exposure 1 x  200000 =  200000 $\n",
      "Signaux - Accuracy : 92.28 %\n",
      "Signaux - Precision : 0.0 %\n",
      "Signaux - Recall : 0.0 %\n",
      "Achat - F-measure: : 0.0 %\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.92      1.00      0.96      4100\n",
      "         Pos       0.00      0.00      0.00       343\n",
      "\n",
      "    accuracy                           0.92      4443\n",
      "   macro avg       0.46      0.50      0.48      4443\n",
      "weighted avg       0.85      0.92      0.89      4443\n",
      "\n",
      "            Bonnes_Estimations Mauvaises_Estimations\n",
      "vrais-réels                  0                   343\n",
      "faux-réels                   0                  4100\n",
      "\n",
      "\u001b[34m Signaux pour \u001b[33m EUR/USD \u001b[0m\n",
      "Vrais signaux trouvés    :  0\n",
      "Vrais signaux non trouvé : 343\n",
      "Total des vrais signaux  : 343\n",
      "\u001b[31m\n",
      "Precision :  0.0 %\n",
      "\u001b[32m\n",
      "Recall 100.0 %\n",
      "\u001b[0m\n",
      "EUR/USD\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-03-05 16:34:33.708800\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker EUR/USD est à  1.0 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 81722/81722 [00:01<00:00, 70823.53it/s]\n",
      "(No Duration)\n",
      "\u001b[34m For ticker \u001b[33m EUR/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Total Profit & Loss : $ \u001b[31m 0.0 . En  0 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Winners Number : 0 \u001b[0m\n",
      "\u001b[31m \n",
      "Loosers number : 0 \u001b[0m\n",
      "BT's execution time 0:00:07.450203\n",
      "\u001b[33m EUR/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2019-12-30  à 2021-02-15 \u001b[0m\n",
      "\u001b[36m Total Number of trades 0 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m 0.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m 0.0% \u001b[0m\n",
      "Average trade duration NA\n",
      "# Winners  0\n",
      "# Winners long  0\n",
      "# Winners short  0\n",
      "# Loosers  0\n",
      "# Loosers  long 0\n",
      "# Loosers  short 0\n",
      "Cumulated gains 0.0\n",
      "Cumulated losses 0.0\n",
      "\u001b[34m PROFIT FACTOR :  nan \u001b[0m\n",
      "\u001b[36m Winners Ratio  :None \u001b[0m\n",
      "No winner\n",
      "No looser\n",
      "No trade\n",
      "Number of opened trades 0\n",
      "Number of closed trades 0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/Volumes/DATA_SCIENCES/DEV_EN_COURS/librairies/bt.py\u001b[0m in \u001b[0;36mbt\u001b[0;34m(price, _year_bottom, _year_top, _nb_bougie_exit, _trigger_reengage, _trigger_target, _trigger_invers, _trigger_sl, _verbose, _cash_ini, _rate, x, _target, _exposure, _size, _sl)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of opened trades'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOTAL_OPEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of closed trades'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOTAL_CLOSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Max Exposure'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPO_MAX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'= '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPO_MAX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRACKER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_resultats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Nbre Loosers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "####### CHOIX DE LA TIME FRAME\n",
    "_period = 'm5'\n",
    "##### Récupération de la liste des tickers à partir du répertoire où sont rangées les bases\n",
    "TICKER_LIST = ['EUR/USD']\n",
    "TIK = ['JPY','CHF','CAD','GBP','AUD','NZD','SEK','NOK','MXN','ZAR','TRY','ILS','CNH','USD','HKD','0']\n",
    "RATE = [105.4,0.8989,1.276,0.877,0.7676,0.7201,8.3684,8.5238,20.09,14.81,7.03,3.28,6.458,1,7.7527,1.21]\n",
    "df_ratefx = pd.DataFrame(index=TIK)\n",
    "df_ratefx['rate'] = RATE\n",
    "#############################\n",
    "##### F E A T U R I N G #####\n",
    "#############################\n",
    "\n",
    "print(df_all.shape,daily_all.shape,weekly_all.shape)\n",
    "# Make our choice for the split, compliant to our backtest \n",
    "_start = '2010-01-01' # start the train there '2010-01-01'\n",
    "_mid = '2018-08-29' # stop the train and begin the test there '2016-08-31'\n",
    "_stop = '2019-12-30' # stop the test there. After that, it is kept for oos\n",
    "_last = '2021-02-15' # '2020-12-31'\n",
    "#### S T R A T E G Y\n",
    "\n",
    "df_all = stochastic(df_all)\n",
    "##### On backtest selon le ticker selectionné sur la période déterminée\n",
    "\n",
    "x=TICKER_LIST[0]\n",
    "_ticker = x.replace('/','')\n",
    "print(x)\n",
    "_year_bottom = _stop\n",
    "_year_top = _last\n",
    "_nb_bougie_exit = 2000000\n",
    "_trigger_reengage = 0\n",
    "_trigger_target = 1\n",
    "_trigger_invers = 0\n",
    "_trigger_sl = 1\n",
    "_verbose = 0\n",
    "_cash_ini = 200000\n",
    "_rate = 1/df_ratefx.loc[x[4:],'rate']\n",
    "backtest = df_all[df_all.Symbol == x.replace('/','')]\n",
    "_target =  0.002\n",
    "_exposure = 10\n",
    "_size = 200000\n",
    "_sl =  0.001\n",
    "_open_hour = 8 # day only\n",
    "_close_hour = 23 # day only\n",
    "_window = 0 # day only\n",
    "print()\n",
    "print(col.Fore.RED,'###############################################################################################')\n",
    "print(' ####################################### OOS WITHOUT AI ########################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "##### Backtest Over Night\n",
    "TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "print()\n",
    "print(col.Fore.CYAN,'###############################################################################################')\n",
    "print(' #################################### TRAIN/TEST WITHOUT AI ####################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "_year_bottom = _start\n",
    "_year_top = _stop\n",
    "\n",
    "TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "print()\n",
    "print(col.Fore.BLUE,'###############################################################################################')\n",
    "print(' #################################### DENOISING & ENHANCING ####################################')\n",
    "print(' ###############################################################################################',col.Style.RESET_ALL)\n",
    "while _nb_looser > 0 :\n",
    "    \n",
    "    df_all['TRACKER'] = np.where(df_all.index.isin(TRACKER),1,0)\n",
    "    df_all['Valid'] = np.where(((df_all.Signal!=0)&(df_all.TRACKER==1)),1,0)\n",
    "    df_all['Signal'] = np.where(((df_all.Valid==1)&(df_all.Signal==1)),1,np.where(((df_all.Valid==1)&(df_all.Signal==-1)),-1,0))\n",
    "    backtest = df_all[df_all.Symbol == x.replace('/','')]\n",
    "    ##### Purification of signal by denoising and enhancing\n",
    "    TRACKER,_nb_looser = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "            _rate,x,_target,_exposure,_size,_sl)\n",
    "\n",
    "df_all = stochastic(df_all)\n",
    "\n",
    "\n",
    "features_train = features[(features.Date>=_start)&(features.Date<=_mid)]\n",
    "features_test = features[(features.Date>_mid)&(features.Date<=_stop)]\n",
    "features_oos = features[(features.Date>_stop)&(features.Date <= _last)]\n",
    "features_train = features_train[features_train.Signal!=0]\n",
    "features_test = features_test[features_test.Signal!=0]\n",
    "# Proceed an MaxAbsScaler on features\n",
    "#features_train,features_test,features_oos = scaling(features,x.replace('/',''),TRACKER,_start,_mid,_stop,_last,scaler=MaxAbsScaler())\n",
    "#features_train,features_test,features_oos = quantile(features_train,features_test,features_oos,quantile_transform)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "_name = 'MLPClassifier'\n",
    "_model = MLPClassifier(hidden_layer_sizes=(300,15), activation='relu', solver='adam', alpha=0.000001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=400, shuffle=True, random_state=26, tol=0.0000001, verbose=False, warm_start=True, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.8, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "_model.fit(features_train.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1),features_train.Valid)\n",
    "yhat = _model.predict(features_test.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1))\n",
    "accu = round(accuracy_score(features_test.Valid, yhat) * 100,2)\n",
    "prec = round(precision_score(features_test.Valid, yhat,pos_label=1) * 100,2)\n",
    "recall = round(recall_score(features_test.Valid, yhat) * 100,2)\n",
    "f1 = round(f1_score(features_test.Valid, yhat) * 100,2)\n",
    "\n",
    "\n",
    "print('Signaux - Accuracy :' ,accu,'%')\n",
    "print('Signaux - Precision :',prec,'%')\n",
    "print('Signaux - Recall :', recall,'%')\n",
    "print('Achat - F-measure: :' ,f1,'%')\n",
    "print('\\n')\n",
    "print(classification_report(features_test.Valid, yhat, target_names=['Neg', 'Pos']))\n",
    "conf_matrix = pd.DataFrame(columns=['Bonnes_Estimations','Mauvaises_Estimations'])\n",
    "tt = confusion_matrix(features_test.Valid, yhat, labels=[1,0])    #_model.classes_)\n",
    "\n",
    "conf_matrix.loc['vrais-réels'] = tt[0]\n",
    "conf_matrix.loc['faux-réels'] = tt[1]\n",
    "\n",
    "print(conf_matrix)\n",
    "print()\n",
    "print(col.Fore.BLUE,'Signaux pour',col.Fore.YELLOW,x,col.Style.RESET_ALL)\n",
    "_tp = tt[0][0]\n",
    "_fn = tt[0][1]\n",
    "_prec = round((tt[0][0]/(tt[0][0]+tt[0][1]))*100,2)\n",
    "_rec = round((tt[0][1]/(tt[0][0]+tt[0][1]))*100,2)\n",
    "\n",
    "print('Vrais signaux trouvés    : ',tt[0][0])\n",
    "print('Vrais signaux non trouvé :',tt[0][1])\n",
    "print('Total des vrais signaux  :',tt[0][0]+tt[0][1])\n",
    "if _prec > 69 and prec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif _prec < 51 or prec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "print('Precision : ',_prec,'%')\n",
    "if _rec > 69 and _rec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif _rec < 51 or _rec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "print('Recall',_rec,'%')\n",
    "print(col.Style.RESET_ALL)\n",
    "\n",
    "df_all_oos = df_all[(df_all.Date > _stop)&(df_all.Date <= _last)].dropna()\n",
    "\n",
    "df_all_oos = df_all_oos[df_all_oos.Symbol==x.replace('/','')]\n",
    "\n",
    "\n",
    "df_all_oos['Valid'] = _model.predict(features_oos.drop(['Date','Symbol','Signal','TRACKER','Valid'],axis=1))\n",
    "\n",
    "\n",
    "df_all_oos['Signal'] = np.where((df_all_oos.Signal==1)&(df_all_oos.Valid==1),1,np.where((df_all_oos.Signal==-1)&(df_all_oos.Valid==1),-1,0))\n",
    " \n",
    "##### On backtest selon le ticker selectionné sur la période déterminée\n",
    "\n",
    "#joblib.dump(_model,'Save_'+x.replace('/','')+'_m5.sav')\n",
    "\n",
    "print(x)\n",
    "_year_bottom = _stop\n",
    "_year_top = _last\n",
    "\n",
    "#_trigger_invers = 1\n",
    "#_target = 0.017\n",
    "##### Backtest Over Night\n",
    "FINAL_TRACKER = bt(df_all_oos,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-456fb0bd3a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00007\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Symbol'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'TRACKER'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Signal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Symbol'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'TRACKER'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Signal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0maccu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \"\"\"\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    899\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mcheck_X_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             X, y = self._validate_data(X, y,\n\u001b[0m\u001b[1;32m    157\u001b[0m                                        validate_separately=(check_X_params,\n\u001b[1;32m    158\u001b[0m                                                             check_y_params))\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;31m# :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier,RadiusNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "_model = DecisionTreeClassifier(criterion='entropy',random_state=26,splitter='best',ccp_alpha=0.00007,max_features=7,class_weight={0: 1, 1: 1000})\n",
    "_model.fit(features_train.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1),features_train.Valid)\n",
    "yhat = _model.predict(features_test.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1))\n",
    "accu = round(accuracy_score(features_test.Valid, yhat) * 100,2)\n",
    "prec = round(precision_score(features_test.Valid, yhat,pos_label=1) * 100,2)\n",
    "recall = round(recall_score(features_test.Valid, yhat) * 100,2)\n",
    "f1 = round(f1_score(features_test.Valid, yhat) * 100,2)\n",
    "\n",
    "\n",
    "print('Signaux - Accuracy :' ,accu,'%')\n",
    "print('Signaux - Precision :',prec,'%')\n",
    "print('Signaux - Recall :', recall,'%')\n",
    "print('Achat - F-measure: :' ,f1,'%')\n",
    "print('\\n')\n",
    "print(classification_report(features_test.Valid, yhat, target_names=['Neg', 'Pos']))\n",
    "conf_matrix = pd.DataFrame(columns=['Bonnes_Estimations','Mauvaises_Estimations'])\n",
    "tt = confusion_matrix(features_test.Valid, yhat, labels=[1,0])    #_model.classes_)\n",
    "\n",
    "conf_matrix.loc['vrais-réels'] = tt[0]\n",
    "conf_matrix.loc['faux-réels'] = tt[1]\n",
    "\n",
    "print(conf_matrix)\n",
    "print()\n",
    "print(col.Fore.BLUE,'Signaux pour',col.Fore.YELLOW,x,col.Style.RESET_ALL)\n",
    "_tp = tt[0][0]\n",
    "_fn = tt[0][1]\n",
    "_prec = round((tt[0][0]/(tt[0][0]+tt[0][1]))*100,2)\n",
    "_rec = round((tt[0][1]/(tt[0][0]+tt[0][1]))*100,2)\n",
    "\n",
    "print('Vrais signaux trouvés    : ',tt[0][0])\n",
    "print('Vrais signaux non trouvé :',tt[0][1])\n",
    "print('Total des vrais signaux  :',tt[0][0]+tt[0][1])\n",
    "if _prec > 69 and prec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif _prec < 51 or prec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "print('Precision : ',_prec,'%')\n",
    "if _rec > 69 and _rec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif _rec < 51 or _rec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "print('Recall',_rec,'%')\n",
    "print(col.Style.RESET_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "np.nan_to_num(df_all_oos[df_all_oos.index=='2020-01-02 11:40:00'].SB_Gamma.astype(np.float32)).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}