{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('DataSciences': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7f9c69b77f8cb78a9d8b8acc2d09c3972908e6673afd8bfd04ee2f6acaaac495"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Importing Librairies...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import librairies.dagfeaturingfx \n",
    "from  librairies.dagfeaturingfx import *\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel,delayed\n",
    "import pyttsx3\n",
    "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler,quantile_transform,PolynomialFeatures\n",
    "from librairies.bt import *\n",
    "from sklearn.metrics import accuracy_score, make_scorer, precision_score, recall_score, precision_recall_curve, confusion_matrix, classification_report\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "import colorama as col\n",
    "from collections import Counter\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "engine = pyttsx3.init()\n",
    "engine.say(\"Librairies loaded\")\n",
    "engine.runAndWait()"
   ]
  },
  {
   "source": [
    "###### RELOAD LIBRAIRIE #####\n",
    "from importlib import reload\n",
    "sys.path.append('../')\n",
    "librairies.dagfeaturingfx= reload(librairies.dagfeaturingfx)\n",
    "from  librairies.dagfeaturingfx import *"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### CHOIX DE LA TIME FRAME\n",
    "_period = 'm15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34m\nRécupération des tickers\u001b[0m\nCPU times: user 1.15 ms, sys: 1.81 ms, total: 2.96 ms\nWall time: 2.46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##### Récupération de la liste des tickers à partir du répertoire où sont rangées les bases\n",
    "TICKER_LIST = list(sorted(set(get_ticker_list())))\n",
    "TICKER_LIST = ['GBP/USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "##### F E A T U R I N G #####\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2.88 s, sys: 3.52 s, total: 6.4 s\nWall time: 6.4 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((11069853, 56), (119160, 7), (23981, 19))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "%%time\n",
    "df_all = joblib.load('JOBLIB/Built_bases/df_all')\n",
    "daily_all = joblib.load('JOBLIB/Built_bases/daily_all')\n",
    "weekly_all = joblib.load('JOBLIB/Built_bases/weekly_all')\n",
    "df_all.shape,daily_all.shape,weekly_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 565 ms, sys: 74.2 ms, total: 639 ms\nWall time: 377 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### S T R A T E G Y\n",
    "\n",
    "##### CONDITIONS LONG\n",
    "_condition_1 = (df_all.slow_K5 < 20) & (df_all.slow_K5.shift(1) < df_all.slow_D5.shift(1)) & (df_all.slow_K5 > df_all.slow_D5)\n",
    "\n",
    "##### CONDITIONS SHORT\n",
    "_condition_1_bar = (df_all.slow_K5 > 80) & (df_all.slow_K5.shift(1) > df_all.slow_D5.shift(1)) & (df_all.slow_K5 < df_all.slow_D5)\n",
    "\n",
    "##### 1 condition\n",
    "df_all['Signal'] = np.where(_condition_1,1,np.where(_condition_1_bar,-1,0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GBP/USD\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-02-05 16:05:31.415380\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker GBP/USD est à  1 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 193628/193628 [00:07<00:00, 24491.89it/s]\n",
      "\u001b[34m Pour le ticker \u001b[33m GBP/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Les gains faramineux s'élèvent à : $ \u001b[31m -31754.0 !. En  2920 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Nbre de winners : 1331 \u001b[0m\n",
      "\u001b[31m \n",
      "Nbre de loosers : 1589 \u001b[0m\n",
      "Temps d'excution du BT 0:00:14.378045\n",
      "\u001b[33m GBP/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2010-01-01  à 2017-12-31 \u001b[0m\n",
      "\u001b[36m Total Number of trades 2920 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L in currency: \u001b[31m -31754.0$ \u001b[0m\n",
      "P&L in %: \u001b[31m -15.88% \u001b[0m\n",
      "Average trade duration 32.1\n",
      "# Winners  1331.0\n",
      "# Loosers  1589.0\n",
      "Cumulated gains 258100.0\n",
      "Cumulated losses -289854.0\n",
      "\u001b[34m PROFIT FACTOR :  0.89 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 45.58 % \u001b[0m\n",
      "Average Winners 193.91\n",
      "% Average Winners 0.1\n",
      "Average Loosers -182.41\n",
      "% Average Loosers -0.09\n",
      "Average pnl -10.87\n",
      "% Average pnl -0.01\n",
      "Number of opened trades 2920\n",
      "Number of closed trades 2920\n",
      "Max Exposure 1 x  50000 =  50000 $\n"
     ]
    }
   ],
   "source": [
    "##### On backtest selon le ticker selectionné sur la période déterminée\n",
    "\n",
    "x=TICKER_LIST[0]\n",
    "print(x)\n",
    "_year_bottom = '2010-01-01'\n",
    "_year_top = '2017-12-31'\n",
    "_nb_bougie_exit = 4200000000\n",
    "_trigger_reengage = 0\n",
    "_trigger_target = 1\n",
    "_trigger_invers = 1\n",
    "_trigger_sl = 1\n",
    "_verbose = 0\n",
    "_cash_ini = 200000\n",
    "_rate = 1\n",
    "backtest = df_all[df_all.Symbol == x.replace('/','')]\n",
    "_target = 0.004\n",
    "_exposure = 10\n",
    "_size = 50000\n",
    "_sl = 0.002\n",
    "_open_hour = 8 # day only\n",
    "_close_hour = 23 # day only\n",
    "_window = 0 # day only\n",
    "\n",
    "##### Backtest Over Night\n",
    "TRACKER = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GBP/USD\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-02-05 16:08:37.310943\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker GBP/USD est à  1 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 193628/193628 [00:05<00:00, 37058.71it/s]\n",
      "\u001b[34m Pour le ticker \u001b[33m GBP/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Les gains faramineux s'élèvent à : $ \u001b[32m 260930.5 !. En  757 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Nbre de winners : 757 \u001b[0m\n",
      "\u001b[31m \n",
      "Nbre de loosers : 0 \u001b[0m\n",
      "Temps d'excution du BT 0:00:11.642027\n",
      "\u001b[33m GBP/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2010-01-01  à 2017-12-31 \u001b[0m\n",
      "\u001b[36m Total Number of trades 757 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L  in currency: \u001b[32m 260930.5$ \u001b[0m\n",
      "P&L in %: \u001b[32m 130.47% \u001b[0m\n",
      "Average trade duration 59.89\n",
      "# Winners  757.0\n",
      "# Loosers  0.0\n",
      "Cumulated gains 260930.5\n",
      "Cumulated losses 0.0\n",
      "\u001b[34m PROFIT FACTOR :  inf \u001b[0m\n",
      "\u001b[36m Winners Ratio : 100.0 % \u001b[0m\n",
      "Average Winners 344.69\n",
      "% Average Winners 0.17\n",
      "Pas de loosers\n",
      "Average pnl 344.69\n",
      "% Average pnl 0.3\n",
      "Number of opened trades 757\n",
      "Number of closed trades 757\n",
      "Max Exposure 1 x  50000 =  50000 $\n",
      "CPU times: user 6.87 s, sys: 142 ms, total: 7.02 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time  \n",
    "df_all['TRACKER'] = np.where(df_all.index.isin(TRACKER),1,0)\n",
    "df_all['Valid'] = np.where(((df_all.Signal!=0)&(df_all.TRACKER==1)),1,0)\n",
    "df_all['Signal'] = np.where(((df_all.Valid==1)&(df_all.Signal==1)),1,np.where(((df_all.Valid==1)&(df_all.Signal==-1)),-1,0))\n",
    "#df_all['Signal'] = np.where(df_all.index.isin(features[features.Signal==1].index),1,np.where(df_all.index.isin(features[features.Signal==-1].index),-1,0))\n",
    "##### On backtest selon le ticker selectionné sur la période déterminée\n",
    "\n",
    "x=TICKER_LIST[0]\n",
    "print(x)\n",
    "_year_bottom = '2010-01-01'\n",
    "_year_top = '2017-12-31'\n",
    "_nb_bougie_exit = 4200000000\n",
    "_trigger_reengage = 0\n",
    "_trigger_target = 1\n",
    "_trigger_invers = 1\n",
    "_trigger_sl = 1\n",
    "_verbose = 0\n",
    "_cash_ini = 200000\n",
    "_rate = 1\n",
    "backtest = df_all[df_all.Symbol == x.replace('/','')]\n",
    "_target = 0.004\n",
    "_exposure = 10\n",
    "_size = 50000\n",
    "_sl = 0.002\n",
    "_open_hour = 8 # day only\n",
    "_close_hour = 23 # day only\n",
    "_window = 0 # day only\n",
    "\n",
    "##### Backtest Over Night\n",
    "TRACKER = bt(backtest,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 9.39 s, sys: 5.4 s, total: 14.8 s\nWall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_all = joblib.load('JOBLIB/Built_bases/df_all')\n",
    "df_all['Signal'] = np.where(_condition_1,1,np.where(_condition_1_bar,-1,0))\n",
    "features = featuring(df_all)\n",
    "features['TRACKER'] = np.where(features.index.isin(TRACKER),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 103 ms, sys: 8.68 ms, total: 111 ms\nWall time: 55 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# First, we must have an output. We'll call it 'Valid'. It wil be where Tracker & Signal are both to 1\n",
    "features['Valid'] = np.where(((features.Signal!=0)&(features.TRACKER==1)),1,0) # Don't miss the point that even a Signal -1 must be considered as a good one by TRACKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make our choice for the split, compliant to our backtest\n",
    "_start = '2010-01-01' # start the train there\n",
    "_mid = '2016-08-31' # stop the train and begin the test there\n",
    "_stop = '2017-12-31' # stop the test there. After that, it is kept for oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3.1 s, sys: 2.34 s, total: 5.44 s\nWall time: 5.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's isolate the ticker on which we made the test\n",
    "features = features[features.Symbol==x.replace('/','')]\n",
    "\n",
    "# And drop the nan\n",
    "features = features.dropna()\n",
    "##### Signal is from strategy. This is potential good one. But we have to create the TRACKER column where the Signal where efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 323 ms, sys: 80.1 ms, total: 404 ms\nWall time: 402 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Proceed an MaxAbsScaler on features\n",
    "features_train,features_test,features_oos = scaling(features,x.replace('/',''),TRACKER,_start,_mid,_stop,scaler=MaxAbsScaler())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 504 ms, sys: 2.13 ms, total: 506 ms\nWall time: 505 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_train,features_test,features_oos = quantile(features_train,features_test,features_oos,quantile_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 20.7 ms, sys: 8.05 ms, total: 28.7 ms\nWall time: 27.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "_name = 'GradientBoostingClassifier'\n",
    "_model = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=100, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=26, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 34 µs, sys: 0 ns, total: 34 µs\nWall time: 37.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from pycaret analysis\n",
    "_name = 'GradientBoostingClassifier'\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "_model = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
    "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
    "                           max_features=None, max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                           min_samples_leaf=1, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                           n_iter_no_change=None, presort='deprecated',\n",
    "                           random_state=26, subsample=1.0, tol=0.0001,\n",
    "                           validation_fraction=0.1, verbose=0,\n",
    "                           warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 31 µs, sys: 1e+03 ns, total: 32 µs\nWall time: 33.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "_name = 'DecisionTreeClassifier'\n",
    "_model = DecisionTreeClassifier(criterion='gini', splitter='random', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='log2', random_state=26, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2.44 ms, sys: 2.65 ms, total: 5.09 ms\nWall time: 3.82 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "_name = 'MLPClassifier'\n",
    "_model = MLPClassifier(hidden_layer_sizes=(200,8), activation='relu', solver='adam', alpha=0.000001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=26, tol=0.0000001, verbose=False, warm_start=True, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.8, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34m Signaux pour \u001b[33m GBP/USD \u001b[0m\n\u001b[31m\nSignaux - Accuracy : 84.62 %\nSignaux - Precision : 50.0 %\nSignaux - Recall : 32.06 %\nAchat - F-measure: : 39.07 % \u001b[0m\n\n\n              precision    recall  f1-score   support\n\n         Neg       0.88      0.94      0.91       721\n         Pos       0.50      0.32      0.39       131\n\n    accuracy                           0.85       852\n   macro avg       0.69      0.63      0.65       852\nweighted avg       0.83      0.85      0.83       852\n\nVrais signaux trouvés    :  \u001b[32m 42 \u001b[0m\nVrais signaux non trouvé : \u001b[31m 89 \u001b[0m\nTotal des vrais signaux  : \u001b[34m 131 \u001b[0m\n\n\u001b[34m Joblib Model  JOBLIB/MLPClassifier/Save_GBPUSD_m15.sav  dumped to disk \u001b[0m\nCPU times: user 36.7 s, sys: 5.65 s, total: 42.3 s\nWall time: 5.34 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Bonnes_Estimations Mauvaises_Estimations\n",
       "vrais-réels                 42                    89\n",
       "faux-réels                  42                   679"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bonnes_Estimations</th>\n      <th>Mauvaises_Estimations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>vrais-réels</th>\n      <td>42</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>faux-réels</th>\n      <td>42</td>\n      <td>679</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "%%time\n",
    "_model.fit(features_train.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1),features_train.Valid)\n",
    "yhat = _model.predict(features_test.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1))\n",
    "accu = round(accuracy_score(features_test.Valid, yhat) * 100,2)\n",
    "prec = round(precision_score(features_test.Valid, yhat,pos_label=1) * 100,2)\n",
    "recall = round(recall_score(features_test.Valid, yhat) * 100,2)\n",
    "f1 = round(f1_score(features_test.Valid, yhat) * 100,2)\n",
    "\n",
    "print(col.Fore.BLUE,'Signaux pour',col.Fore.YELLOW,x,col.Style.RESET_ALL)\n",
    "if accu > 69 and prec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif accu < 51 or prec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "\n",
    "print('Signaux - Accuracy :' ,accu,'%')\n",
    "print('Signaux - Precision :',prec,'%')\n",
    "print('Signaux - Recall :', recall,'%')\n",
    "print('Achat - F-measure: :' ,f1,'%',col.Style.RESET_ALL)\n",
    "print('\\n')\n",
    "print(classification_report(features_test.Valid, yhat, target_names=['Neg', 'Pos']))\n",
    "conf_matrix = pd.DataFrame(columns=['Bonnes_Estimations','Mauvaises_Estimations'])\n",
    "tt = confusion_matrix(features_test.Valid, yhat, labels=[1,0])    #_model.classes_)\n",
    "print('Vrais signaux trouvés    : ',col.Fore.GREEN,tt[0][0],col.Style.RESET_ALL)\n",
    "print('Vrais signaux non trouvé :',col.Fore.RED,tt[0][1],col.Style.RESET_ALL)\n",
    "print('Total des vrais signaux  :',col.Fore.BLUE,tt[0][0]+tt[0][1],col.Style.RESET_ALL)\n",
    "print()\n",
    "conf_matrix.loc['vrais-réels'] = tt[0]\n",
    "conf_matrix.loc['faux-réels'] = tt[1]\n",
    "savename = 'JOBLIB/'+_name+'/Save_'+x.replace('/','')+'_m15.sav'\n",
    "joblib.dump(_model, savename)\n",
    "print(col.Fore.BLUE,\"Joblib Model \",savename,\" dumped to disk\",col.Style.RESET_ALL)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34m Achat pour \u001b[33m EUR/USD \u001b[0m\n\u001b[33m\nAchat - Accuracy : 87.93 %\nAchat - Precision : 53.12 %\nAchat - Recall : 14.41 %\nAchat - F-measure: : 22.67 % \u001b[0m\n\n\n              precision    recall  f1-score   support\n\n         Neg       0.89      0.98      0.93       843\n         Pos       0.53      0.14      0.23       118\n\n    accuracy                           0.88       961\n   macro avg       0.71      0.56      0.58       961\nweighted avg       0.85      0.88      0.85       961\n\nVrais signaux trouvés    :  \u001b[32m 17 \u001b[0m\nVrais signaux non trouvé : \u001b[31m 101 \u001b[0m\nTotal des vrais signaux  : \u001b[34m 118 \u001b[0m\n\n\u001b[34m Joblib Model  JOBLIB/RandomForestClassifier/Save_EURUSD_m15.sav  dumped to disk \u001b[0m\nCPU times: user 2.57 s, sys: 81.9 ms, total: 2.65 s\nWall time: 373 ms\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Bonnes_Estimations Mauvaises_Estimations\n",
       "vrais-réels                 17                   101\n",
       "faux-réels                  15                   828"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bonnes_Estimations</th>\n      <th>Mauvaises_Estimations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>vrais-réels</th>\n      <td>17</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>faux-réels</th>\n      <td>15</td>\n      <td>828</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "_name = 'RandomForestClassifier'\n",
    "_model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='gini', max_depth=None, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                       n_jobs=-1, oob_score=False, random_state=26, verbose=0,\n",
    "                       warm_start=False)\n",
    "\n",
    "_model.fit(features_train.drop(['TRACKER','Symbol','Date','Valid','Signal'],axis=1),features_train.Valid)\n",
    "yhat = _model.predict(features_test.drop(['TRACKER','Symbol','Date','Valid','Signal'],axis=1))\n",
    "accu = round(accuracy_score(features_test.Valid, yhat) * 100,2)\n",
    "prec = round(precision_score(features_test.Valid, yhat,pos_label=1) * 100,2)\n",
    "recall = round(recall_score(features_test.Valid, yhat) * 100,2)\n",
    "f1 = round(f1_score(features_test.Valid, yhat) * 100,2)\n",
    "\n",
    "print(col.Fore.BLUE,'Achat pour',col.Fore.YELLOW,x,col.Style.RESET_ALL)\n",
    "if accu > 69 and prec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif accu < 51 or prec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "\n",
    "print('Achat - Accuracy :' ,accu,'%')\n",
    "print('Achat - Precision :',prec,'%')\n",
    "print('Achat - Recall :', recall,'%')\n",
    "print('Achat - F-measure: :' ,f1,'%',col.Style.RESET_ALL)\n",
    "print('\\n')\n",
    "print(classification_report(features_test.Valid, yhat, target_names=['Neg', 'Pos']))\n",
    "conf_matrix = pd.DataFrame(columns=['Bonnes_Estimations','Mauvaises_Estimations'])\n",
    "tt = confusion_matrix(features_test.Valid, yhat, labels=[1,0])    #_model.classes_)\n",
    "print('Vrais signaux trouvés    : ',col.Fore.GREEN,tt[0][0],col.Style.RESET_ALL)\n",
    "print('Vrais signaux non trouvé :',col.Fore.RED,tt[0][1],col.Style.RESET_ALL)\n",
    "print('Total des vrais signaux  :',col.Fore.BLUE,tt[0][0]+tt[0][1],col.Style.RESET_ALL)\n",
    "print()\n",
    "conf_matrix.loc['vrais-réels'] = tt[0]\n",
    "conf_matrix.loc['faux-réels'] = tt[1]\n",
    "savename = 'JOBLIB/'+_name+'/Save_'+x.replace('/','')+'_m15.sav'\n",
    "joblib.dump(_model, savename)\n",
    "print(col.Fore.BLUE,\"Joblib Model \",savename,\" dumped to disk\",col.Style.RESET_ALL)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "%%time\n",
    "f_importance = pd.DataFrame(columns=features_train.drop(['Date','Symbol','TRACKER','Valid'],axis=1).columns.values.tolist()).T\n",
    "f_importance['Importance'] = _model.feature_importances_\n",
    "f_importance = f_importance\n",
    "TOP_TEN = f_importance.sort_values('Importance',ascending=False).iloc[:12].index.to_list()\n",
    "X_train = features_train[features_train.columns.intersection(TOP_TEN)]\n",
    "y_train = features_train.Valid\n",
    "X_test = features_test[features_test.columns.intersection(TOP_TEN)]\n",
    "y_test = features_test.Valid\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape\n",
    "_model.fit(X_train,y_train)\n",
    "yhat = _model.predict(X_test)\n",
    "accu = round(accuracy_score(y_test, yhat) * 100,2)\n",
    "prec = round(precision_score(y_test, yhat,pos_label=1) * 100,2)\n",
    "recall = round(recall_score(y_test, yhat) * 100,2)\n",
    "f1 = round(f1_score(y_test, yhat) * 100,2)\n",
    "\n",
    "print(col.Fore.BLUE,'Achat pour',col.Fore.YELLOW,x,col.Style.RESET_ALL)\n",
    "if accu > 69 and prec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif accu < 51 or prec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "\n",
    "print('Achat - Accuracy :' ,accu,'%')\n",
    "print('Achat - Precision :',prec,'%')\n",
    "print('Achat - Recall :', recall,'%')\n",
    "print('Achat - F-measure: :' ,f1,'%',col.Style.RESET_ALL)\n",
    "print('\\n')\n",
    "print(classification_report(y_test, yhat, target_names=['Neg', 'Pos']))\n",
    "conf_matrix = pd.DataFrame(columns=['Bonnes_Estimations','Mauvaises_Estimations'])\n",
    "tt = confusion_matrix(y_test, yhat, labels=[1,0])    #_model.classes_)\n",
    "print('Vrais signaux trouvés :',tt[0][0])\n",
    "print('Vrais signaux non trouvé :',tt[0][1])\n",
    "print('Total des vrais signaux :',tt[0][0]+tt[0][1])\n",
    "print()\n",
    "conf_matrix.loc['vrais-réels'] = tt[0]\n",
    "conf_matrix.loc['faux-réels'] = tt[1]\n",
    "conf_matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'depth': 5,\n",
       " 'iterations': 500,\n",
       " 'l2_leaf_reg': 1e-20,\n",
       " 'leaf_estimation_iterations': 10,\n",
       " 'logging_level': 'Silent',\n",
       " 'loss_function': 'Logloss',\n",
       " 'random_seed': 42}"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = CatBoostClassifier()\n",
    "params = {'iterations': [500],\n",
    "          'depth': [5, 6],\n",
    "          'loss_function': ['Logloss', 'CrossEntropy'],\n",
    "          'l2_leaf_reg': np.logspace(-20, -19, 3),\n",
    "          'leaf_estimation_iterations': [10,100,500],\n",
    "#           'eval_metric': ['Accuracy'],\n",
    "#           'use_best_model': ['True'],\n",
    "          'logging_level':['Silent'],\n",
    "          'random_seed': [42]\n",
    "         }\n",
    "scorer = make_scorer(precision_score,greater_is_better=True,  pos_label=0)\n",
    "clf_grid = GridSearchCV(estimator=clf, param_grid=params, scoring=scorer, cv=5)\n",
    "\n",
    "clf_grid.fit(features_train.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1), features_train.Valid)\n",
    "best_param = clf_grid.best_params_\n",
    "best_param\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34m Achat pour \u001b[33m EUR/USD \u001b[0m\n\u001b[31m\nAchat - Accuracy : 87.1 %\nAchat - Precision : 45.83 %\nAchat - Recall : 27.97 %\nAchat - F-measure: : 34.74 % \u001b[0m\n\n\n              precision    recall  f1-score   support\n\n         Neg       0.90      0.95      0.93       843\n         Pos       0.46      0.28      0.35       118\n\n    accuracy                           0.87       961\n   macro avg       0.68      0.62      0.64       961\nweighted avg       0.85      0.87      0.86       961\n\nVrais signaux trouvés    :  \u001b[32m 33 \u001b[0m\nVrais signaux non trouvé : \u001b[31m 85 \u001b[0m\nTotal des vrais signaux  : \u001b[34m 118 \u001b[0m\n\n\u001b[34m Joblib Model  JOBLIB/CatBoostClassifier/Save_EURUSD_m15.sav  dumped to disk \u001b[0m\nCPU times: user 43.9 s, sys: 1.46 s, total: 45.3 s\nWall time: 4.31 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            Bonnes_Estimations Mauvaises_Estimations\n",
       "vrais-réels                 33                    85\n",
       "faux-réels                  39                   804"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bonnes_Estimations</th>\n      <th>Mauvaises_Estimations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>vrais-réels</th>\n      <td>33</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>faux-réels</th>\n      <td>39</td>\n      <td>804</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "%%time\n",
    "from catboost import CatBoostClassifier\n",
    "_name = 'CatBoostClassifier'\n",
    "\n",
    "_model = CatBoostClassifier(\n",
    "                             nan_mode = 'Min',\n",
    "                             eval_metric = 'Logloss',\n",
    "                                iterations = 1000,\n",
    "                                sampling_frequency = 'PerTree',\n",
    "                                leaf_estimation_method = 'Newton',\n",
    "                                grow_policy = 'SymmetricTree',\n",
    "                                penalties_coefficient = 1,\n",
    "                                boosting_type = 'Plain',\n",
    "                                model_shrink_mode = 'Constant',\n",
    "                                feature_border_type = 'GreedyLogSum',\n",
    "                                #bayesian_matrix_reg = 0.10000000149011612,\n",
    "                                l2_leaf_reg = 3,\n",
    "                                random_strength = 1,\n",
    "                                rsm = 1,\n",
    "                                boost_from_average = False,\n",
    "                                model_size_reg = 0.5,\n",
    "                                subsample = 0.800000011920929,\n",
    "                                use_best_model = False,\n",
    "                                class_names = [0,1],\n",
    "                                random_seed = 42,\n",
    "                                depth = 6,\n",
    "                                posterior_sampling = False,\n",
    "                                border_count = 254,\n",
    "                                classes_count = 0,\n",
    "                                auto_class_weights = None,\n",
    "                                sparse_features_conflict_fraction = 0,\n",
    "                                leaf_estimation_backtracking = 'AnyImprovement',\n",
    "                                best_model_min_trees = 1,\n",
    "                                model_shrink_rate = 0,\n",
    "                                min_data_in_leaf = 1,\n",
    "                                loss_function = 'Logloss',\n",
    "                                learning_rate = 0.08382000029087067,\n",
    "                                score_function = 'Cosine',\n",
    "                                task_type = 'CPU',\n",
    "                                leaf_estimation_iterations = 10,\n",
    "                                bootstrap_type = 'MVS',\n",
    "                                max_leaves = 64\n",
    ")\n",
    "\n",
    "\n",
    "_model.fit(features_train.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1),y=features_train.Valid,verbose=0)\n",
    "\n",
    "\n",
    "''',\n",
    "    cat_features=None, \n",
    "    text_features=None,\n",
    "    sample_weight=None,\n",
    "    baseline=None,\n",
    "    use_best_model=None,\n",
    "    eval_set=None,\n",
    "    verbose=False,\n",
    "    logging_level=None,\n",
    "    plot=False,\n",
    "    column_description=None,\n",
    "    verbose_eval=None,\n",
    "    metric_period=None,\n",
    "    silent=None,\n",
    "    early_stopping_rounds=None,\n",
    "    save_snapshot=None,\n",
    "    snapshot_file=None,\n",
    "    snapshot_interval=None,\n",
    "    init_model=None)'''\n",
    "\n",
    "yhat = _model.predict(features_test.drop(['Date','Symbol','TRACKER','Valid','Signal'],axis=1))\n",
    "accu = round(accuracy_score(features_test.Valid, yhat) * 100,2)\n",
    "prec = round(precision_score(features_test.Valid, yhat,pos_label=1) * 100,2)\n",
    "recall = round(recall_score(features_test.Valid, yhat) * 100,2)\n",
    "f1 = round(f1_score(features_test.Valid, yhat) * 100,2)\n",
    "\n",
    "print(col.Fore.BLUE,'Achat pour',col.Fore.YELLOW,x,col.Style.RESET_ALL)\n",
    "if accu > 69 and prec > 69 :\n",
    "    print(col.Fore.GREEN)\n",
    "elif accu < 51 or prec < 51 :\n",
    "    print(col.Fore.RED)\n",
    "else:\n",
    "    print(col.Fore.YELLOW)\n",
    "\n",
    "print('Achat - Accuracy :' ,accu,'%')\n",
    "print('Achat - Precision :',prec,'%')\n",
    "print('Achat - Recall :', recall,'%')\n",
    "print('Achat - F-measure: :' ,f1,'%',col.Style.RESET_ALL)\n",
    "print('\\n')\n",
    "print(classification_report(features_test.Valid, yhat, target_names=['Neg', 'Pos']))\n",
    "conf_matrix = pd.DataFrame(columns=['Bonnes_Estimations','Mauvaises_Estimations'])\n",
    "tt = confusion_matrix(features_test.Valid, yhat, labels=[1,0])    #_model.classes_)\n",
    "print('Vrais signaux trouvés    : ',col.Fore.GREEN,tt[0][0],col.Style.RESET_ALL)\n",
    "print('Vrais signaux non trouvé :',col.Fore.RED,tt[0][1],col.Style.RESET_ALL)\n",
    "print('Total des vrais signaux  :',col.Fore.BLUE,tt[0][0]+tt[0][1],col.Style.RESET_ALL)\n",
    "print()\n",
    "conf_matrix.loc['vrais-réels'] = tt[0]\n",
    "conf_matrix.loc['faux-réels'] = tt[1]\n",
    "savename = 'JOBLIB/'+_name+'/Save_'+x.replace('/','')+'_m15.sav'\n",
    "joblib.dump(_model, savename)\n",
    "print(col.Fore.BLUE,\"Joblib Model \",savename,\" dumped to disk\",col.Style.RESET_ALL)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1.51 s, sys: 1.2 s, total: 2.7 s\nWall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_all_oos = df_all[df_all.index >= _stop]\n",
    "df_all_oos = df_all_oos[df_all_oos.Symbol==x.replace('/','')].dropna()\n",
    "df_all_oos['Valid'] = _model.predict(features_oos.drop(['Date','Symbol','Signal','TRACKER','Valid'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 14 ms, sys: 13.4 ms, total: 27.5 ms\nWall time: 3.36 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_all_oos['Signal'] = np.where((df_all_oos.Signal==1)&(df_all_oos.Valid==1),1,np.where((df_all_oos.Signal==-1)&(df_all_oos.Valid==1),-1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GBP/USD\n",
      "Librairies imported\n",
      "\n",
      "Début des opérations horodatée à 2021-02-05 16:09:49.105386\n",
      "\n",
      "Chargement de la nouvelle base\n",
      "\n",
      "\n",
      "\u001b[35m Le rate du ticker GBP/USD est à  1 \u001b[0m\n",
      "Bases chargées\n",
      "TETEL process effectué\n",
      "\u001b[36m ENTERING THE BACKTEST \u001b[0m\n",
      "100%|██████████| 72449/72449 [00:01<00:00, 44900.32it/s]\n",
      "\u001b[34m Pour le ticker \u001b[33m GBP/USD \u001b[0m\n",
      "\u001b[35m \n",
      "Les gains faramineux s'élèvent à : $ \u001b[32m 43995.5 !. En  239 \u001b[0m  transactions.\n",
      "\u001b[32m \n",
      "Nbre de winners : 181 \u001b[0m\n",
      "\u001b[31m \n",
      "Nbre de loosers : 58 \u001b[0m\n",
      "Temps d'excution du BT 0:00:07.908074\n",
      "\u001b[33m GBP/USD \u001b[34m results \u001b[0m\n",
      "\u001b[35m Tested Period 2018-01-01  à 2020-12-31 \u001b[0m\n",
      "\u001b[36m Total Number of trades 239 \u001b[0m\n",
      "Started Cash : 200000\n",
      "P&L  in currency: \u001b[32m 43995.5$ \u001b[0m\n",
      "P&L in %: \u001b[32m 22.0% \u001b[0m\n",
      "Average trade duration 41.53\n",
      "# Winners  181.0\n",
      "# Loosers  58.0\n",
      "Cumulated gains 53747.0\n",
      "Cumulated losses -9751.5\n",
      "\u001b[34m PROFIT FACTOR :  5.51 \u001b[0m\n",
      "\u001b[36m Winners Ratio : 75.73 % \u001b[0m\n",
      "Average Winners 296.94\n",
      "% Average Winners 0.15\n",
      "Average Loosers -168.13\n",
      "% Average Loosers -0.08\n",
      "Average pnl 184.08\n",
      "% Average pnl 0.11\n",
      "Number of opened trades 239\n",
      "Number of closed trades 239\n",
      "Max Exposure 1 x  50000 =  50000 $\n",
      "CPU times: user 2.39 s, sys: 109 ms, total: 2.5 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time  \n",
    "df_all['TRACKER'] = np.where(df_all.index.isin(TRACKER),1,0)\n",
    "df_all['Valid'] = np.where(((df_all.Signal!=0)&(df_all.TRACKER==1)),1,0)\n",
    "df_all['Signal'] = np.where(((df_all.Valid==1)&(df_all.Signal==1)),1,np.where(((df_all.Valid==1)&(df_all.Signal==-1)),-1,0))\n",
    "#df_all['Signal'] = np.where(df_all.index.isin(features[features.Signal==1].index),1,np.where(df_all.index.isin(features[features.Signal==-1].index),-1,0))\n",
    "##### On backtest selon le ticker selectionné sur la période déterminée\n",
    "\n",
    "x=TICKER_LIST[0]\n",
    "print(x)\n",
    "_year_bottom = '2018-01-01'\n",
    "_year_top = '2020-12-31'\n",
    "_nb_bougie_exit = 4200000000\n",
    "_trigger_reengage = 0\n",
    "_trigger_target = 1\n",
    "_trigger_invers = 1\n",
    "_trigger_sl = 1\n",
    "_verbose = 0\n",
    "_cash_ini = 200000\n",
    "_rate = 1\n",
    "_target = 0.004\n",
    "_exposure = 10\n",
    "_size = 50000\n",
    "_sl = 0.002\n",
    "_open_hour = 8 # day only\n",
    "_close_hour = 23 # day only\n",
    "_window = 0 # day only\n",
    "\n",
    "##### Backtest Over Night\n",
    "FINAL_TRACKER = bt(df_all_oos,_year_bottom,_year_top,_nb_bougie_exit,_trigger_reengage,_trigger_target,_trigger_invers,_trigger_sl,_verbose,_cash_ini,\\\n",
    "        _rate,x,_target,_exposure,_size,_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "df_all_oos[df_all_oos.Signal==1].Signal.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}