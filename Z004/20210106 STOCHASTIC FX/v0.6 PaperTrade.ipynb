{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('DataSciences': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7f9c69b77f8cb78a9d8b8acc2d09c3972908e6673afd8bfd04ee2f6acaaac495"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Importing Librairies...\nversion fxcmpy : 1.2.6\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import colorama as col\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel,delayed\n",
    "import datetime as dt\n",
    "import fxcmpy\n",
    "import pyttsx3\n",
    "import datetime as dt\n",
    "from Live import *\n",
    "from librairies.strategy import *\n",
    "from librairies.bt import *\n",
    "from sklearn.metrics import accuracy_score, make_scorer, precision_score, recall_score, precision_recall_curve, confusion_matrix, classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler,quantile_transform,PolynomialFeatures\n",
    "engine = pyttsx3.init()\n",
    "print('version fxcmpy :',fxcmpy.__version__)\n",
    "#print('version socketio :',socketio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_token = 'dbdc379ce7761772c662c3e92250a0ae38385b2c'\n",
    "_server = 'demo'\n",
    "_user_id = 'D261282181'\n",
    "_compte = '01215060'\n",
    "_password = 'waXz1'\n",
    "\n",
    "_period = 'm15'\n",
    "_name = 'MLPClassifier'\n",
    "\n",
    "TICKER_LIST = ['EUR/USD']\n",
    "x = TICKER_LIST[0]\n",
    "TIK = ['AUD','NZD','GBP','JPY','CHF','CAD','SEK','NOK','ILS','MXN','USD']\n",
    "RATE = [0.776,0.721,1.3912,1/105.91,1/0.892,1/1.2681,1/8.2884,1/8.4261,1/3.2385,1/20.1564,1]\n",
    "df_ratefx = pd.DataFrame(index=TIK)\n",
    "df_ratefx['rate'] = RATE\n",
    "_scaler = MaxAbsScaler()\n",
    "savename = 'JOBLIB/'+_name+'/Save_'+x.replace('/','')+'_m15.sav'\n",
    "_model = joblib.load(savename)\n",
    "_path = 'JOBLIB/Ticker_'+_period+'/df_'+x.replace('/','')\n",
    "df_all = joblib.load('JOBLIB/EURUSD_m15')\n",
    "df_all = df_all[['Close','CloseAsk','CloseBid','High','HighAsk','HighBid','Low','LowAsk','LowBid','Open','OpenAsk','OpenBid','Symbol','Date']]\n",
    "hourly_all = joblib.load('JOBLIB/EURUSD_H1')\n",
    "hourly_all = hourly_all[['Close','CloseAsk','CloseBid','High','HighAsk','HighBid','Low','LowAsk','LowBid','Open','OpenAsk','OpenBid','Symbol','Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conX():\n",
    "    con = fxcmpy.fxcmpy(access_token=_token, log_level='error',server=_server)\n",
    "    if con.is_connected() == True:\n",
    "        print(col.Fore.GREEN+'Connexion établie'+col.Style.RESET_ALL)\n",
    "        print('Compte utilisé : ',con.get_account_ids())\n",
    "    else:\n",
    "        print(col.Fore.RED+'Connexion non établie'+col.Style.RESET_ALL)\n",
    "    return(con)\n",
    "\n",
    "def deconX(con):\n",
    "    con = con.close()\n",
    "    if con.is_connected() == True:\n",
    "        print(col.Fore.GREEN+'Connexion non intérrompue'+col.Style.RESET_ALL)\n",
    "        print('Compte utilisé : ',con.get_account_ids())\n",
    "    else:\n",
    "        print(col.Fore.RED+'Connexion intérrompue'+col.Style.RESET_ALL)\n",
    "    return()\n",
    "\n",
    "def scrap_hist(ticker,invers = 'non'):\n",
    "    #_debut = pd.to_datetime((dt.datetime.now()-dt.timedelta(minutes=3987165)).strftime('%Y-%m-%d'))\n",
    "    #_fin = pd.to_datetime((dt.datetime.now().strftime('%Y-%m-%d')))\n",
    "    data = con.get_candles(ticker,period=_period,start=_debut,end=_fin)\n",
    "    data['Open'] = (data['bidopen']+data['askopen'])/2\n",
    "    data['High'] = (data['bidhigh']+data['askhigh'])/2\n",
    "    data['Low'] = (data['bidlow']+data['asklow'])/2\n",
    "    data['Close'] = (data['bidclose']+data['askclose'])/2\n",
    "    return(data)\n",
    "\n",
    "def buy(df_all):\n",
    "    print(dt.datetime.now())\n",
    "    ##### BUY \n",
    "    _price = round(con.get_candles(x,period='m15',number=1).askclose[-1],5)\n",
    "    _time = con.get_candles(x,period='m15',number=1).index[-1]\n",
    "    _amount = 200\n",
    "    _limit = round(_price + _price * 0.004,5)\n",
    "    _stop = round(_price  - _price * 0.002,5)\n",
    "    _atmarket = 3\n",
    "    order = con.open_trade(symbol=x,is_buy=True, is_in_pips=False, amount=20, time_in_force='IOC',order_type='MarketRange',limit=_limit,stop=_stop, at_market=3)\n",
    "    print(\" Bougie de l'opération d'éxecution\",col.Fore.BLUE,_time,col.Style.RESET_ALL)\n",
    "    print(col.Fore.GREEN,'Achat sur le ticker',col.Fore.YELLOW,x,col.Fore.GREEN,'demandé à ',col.Fore.CYAN,_price,col.Style.RESET_ALL)\n",
    "\n",
    "def sell(df_all):\n",
    "    print(dt.datetime.now())\n",
    "    _atmarket = 3\n",
    "    _price = round(con.get_candles(x,period='m15',number=1).bidclose[-1],5)\n",
    "    _amount = 200\n",
    "    _stop = round(_price + _price * 0.002,5)\n",
    "    _limit = round(_price  - _price * 0.004,5)\n",
    "    order = con.open_trade(symbol=x,is_buy=False, is_in_pips=False, amount=20, time_in_force='IOC',order_type='MarketRange',limit=_limit,stop=_stop, at_market=3)\n",
    "    print(\" Bougie de l'opération d'éxecution\",col.Fore.BLUE,_time,col.Style.RESET_ALL)\n",
    "    print(col.Fore.RED,'Vente sur le ticker',col.Fore.YELLOW,x,col.Fore.RED,'demandé à ',col.Fore.CYAN,_price,col.Style.RESET_ALL)\n",
    "\n",
    "    #expiration = (dt.datetime.now() + dt.timedelta(hours=4)).strftime(format='%Y-%m-%d %H:%M'))\n",
    "    return()\n",
    "\n",
    "def close():\n",
    "    con.close_all_for_symbol(x)\n",
    "    return()\n",
    "\n",
    "def init_df():\n",
    "    _path = 'JOBLIB/Ticker_'+_period+'/df_'+x.replace('/','')\n",
    "    df_all = joblib.load('JOBLIB/Built_bases/df_all')\n",
    "    df_all = df_all[df_all.Symbol==x.replace('/','')]\n",
    "    df_all = df_all[['Close','CloseAsk','CloseBid','High','HighAsk','HighBid','Low','LowAsk','LowBid','Open','OpenAsk','OpenBid','Symbol','Date']]\n",
    "    _fin = dt.datetime.now()\n",
    "    _deb = df_all.index[-1]\n",
    "    _debut = dt.datetime(_deb.year,_deb.month,_deb.day,_deb.hour,_deb.minute)\n",
    "    addon = con.get_candles(x,period='m15',start=_debut,end=_fin).drop(['tickqty'],axis=1)\n",
    "    addon = addon.rename(columns={'bidopen':'OpenBid','bidclose':'CloseBid','bidhigh':'HighBid','bidlow':'LowBid','askopen':'OpenAsk','askclose':'CloseAsk','askhigh':'HighAsk','asklow':'LowAsk'})\n",
    "    addon['Open'] = (addon.OpenAsk + addon.OpenBid)/2\n",
    "    addon['High'] = (addon.HighAsk + addon.HighBid)/2\n",
    "    addon['Low'] = (addon.LowAsk + addon.LowBid)/2\n",
    "    addon['Close'] = (addon.CloseAsk + addon.CloseBid)/2\n",
    "    addon['Symbol'] = x.replace('/','')\n",
    "    addon['Date'] = addon.index\n",
    "    addon['Date'] = pd.to_datetime(addon['Date'].dt.strftime(date_format='%Y-%m-%d'))\n",
    "    df_all = df_all.append(addon.iloc[1:-1,:])\n",
    "    df_all['WE'] = np.where(((df_all.index.weekday == 5) | (df_all.index.weekday == 6)),None,df_all.index.weekday)\n",
    "    df_all = df_all.dropna()\n",
    "    df_all =df_all.drop(['WE'],axis=1)\n",
    "    joblib.dump(df_all,_path)\n",
    "    return(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Global Optimized LumberJack Environment Motor for For_Ex\n",
      "LumberJack Jyss 5781(c)\n",
      "\u001b[34m °0Oo_D.A.G._26_oO0°\n",
      "\u001b[33m \u001b[44m --- Bigfoot 1. #v0.60 --- \u001b[0m\n",
      "\n",
      "\u001b[32mConnexion rétablie\u001b[0m\n",
      "Compte utilisé :  [1277536]\n",
      "\n",
      "Base Chargée.\n",
      "Construction de la base...\n",
      "GBP/AUD\n",
      "\n",
      "Waiting for the candle...\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MaxAbsScaler.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-37cf7b5fad74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# Proceed an MaxAbsScaler on features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_scaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquantile_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/DATA_SCIENCES/DEV_EN_COURS/2021-01-06 ADR MULTI TF STRAT/Live.py\u001b[0m in \u001b[0;36mscaling\u001b[0;34m(features, scaler)\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m     \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FEMA_21'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFEMA_21\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FEMA_8'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFEMA_8\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FADRLo'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFADRLo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \"\"\"\n\u001b[1;32m   1068\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_samples_seen_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         X = self._validate_data(X, reset=first_pass,\n\u001b[0m\u001b[1;32m   1070\u001b[0m                                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'no_validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DataSciences/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[1;32m    670\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by MaxAbsScaler."
     ]
    }
   ],
   "source": [
    "___Author___='LumberJack Jyss'\n",
    "print('Global Optimized LumberJack Environment Motor for For_Ex\\nLumberJack Jyss 5781(c)')\n",
    "print(col.Fore.BLUE,'°0Oo_D.A.G._26_oO0°')\n",
    "print(col.Fore.YELLOW,col.Back.BLUE,'--- Bigfoot 1. #v0.60 ---',col.Style.RESET_ALL)\n",
    "\n",
    "\n",
    "print('')\n",
    "engine.say(\" Initialization of Bigfoot 1, FX system\")\n",
    "engine.say(\"Bigfoot's Connexion to the a p i\")\n",
    "engine.runAndWait()\n",
    "\n",
    "try:\n",
    "    con.is_connected() == True\n",
    "    \n",
    "    engine.say(\"already Connected\")\n",
    "    engine.runAndWait()\n",
    "    print(col.Fore.GREEN+'Connexion rétablie'+col.Style.RESET_ALL)\n",
    "    print('Compte utilisé : ',con.get_account_ids())\n",
    "    print('')\n",
    "    \n",
    "except:\n",
    "    try:\n",
    "        con = conX()\n",
    "        con.is_connected() == True\n",
    "        print(col.Fore.GREEN+'Connexion établie'+col.Style.RESET_ALL)\n",
    "        print('Compte utilisé : ',con.get_account_ids())\n",
    "        engine.say(\"Bigfoot is Connected\")\n",
    "        engine.runAndWait()\n",
    "    except:\n",
    "        print(col.Fore.RED+'Connexion non établie'+col.Style.RESET_ALL)\n",
    "        engine.say(\"Mayday, mayday, Not Connected, mauzerfucker!\")\n",
    "        engine.say(\"Check your internet, and launch agin the Bigfoot\")\n",
    "        engine.runAndWait()\n",
    "        print('')\n",
    "        #os._exit(0)\n",
    "        con = deconX()\n",
    "        time.sleep(1)\n",
    "        con = conX()\n",
    "print('\\rChargement de la base...',end='',flush=True)\n",
    "engine.say(\"Ignition of Bigfoot. Loading the database.\")\n",
    "engine.runAndWait()\n",
    "\n",
    "#df_all = joblib.load(_path)\n",
    "#df_all = df_all[df_all.Symbol==x.replace('/','')]\n",
    "#df_all = df_all[['Close','CloseAsk','CloseBid','High','HighAsk','HighBid','Low','LowAsk','LowBid','Open','OpenAsk','OpenBid','Symbol','Date']]\n",
    "#engine.say(\"Database is loaded. Ready to enter Live\")\n",
    "#engine.runAndWait()\n",
    "print('\\rBase Chargée.',end='',flush=True)\n",
    "while True:\n",
    "    engine.say(\"Building the base\")\n",
    "    engine.runAndWait()\n",
    "    print('\\nConstruction de la base...')\n",
    "    ##########\n",
    "    \n",
    "    print(x)\n",
    "    \n",
    "    print('\\nWaiting for the candle...')\n",
    "    print()\n",
    "\n",
    "    ##########\n",
    "\n",
    "    while dt.datetime.now().minute not in [0,15,30,45]:\n",
    "        print('\\rTicker tracké :',x,' ',dt.datetime.now(),end='',flush=True)\n",
    "        time.sleep(1)\n",
    "    print()\n",
    "    while con.get_candles(x,period=_period,start=dt.datetime(df_all.index[-1].year,df_all.index[-1].month,df_all.index[-1].day,df_all.index[-1].hour,df_all.index[-1].minute)\\\n",
    "          ,end=dt.datetime.now()).index[-1].minute != dt.datetime.now().minute:\n",
    "\n",
    "          time.sleep(0.5)\n",
    "        \n",
    "    _fin = dt.datetime.now()\n",
    "    _deb = df_all.index[-1]\n",
    "    _debut = dt.datetime(_deb.year,_deb.month,_deb.day,_deb.hour,_deb.minute)\n",
    "    addon = con.get_candles(x,period='m15',start=_debut,end=_fin).drop(['tickqty'],axis=1)\n",
    "    addon = addon.rename(columns={'bidopen':'OpenBid','bidclose':'CloseBid','bidhigh':'HighBid','bidlow':'LowBid','askopen':'OpenAsk','askclose':'CloseAsk','askhigh':'HighAsk','asklow':'LowAsk'})\n",
    "    addon['Open'] = (addon.OpenAsk + addon.OpenBid)/2\n",
    "    addon['High'] = (addon.HighAsk + addon.HighBid)/2\n",
    "    addon['Low'] = (addon.LowAsk + addon.LowBid)/2\n",
    "    addon['Close'] = (addon.CloseAsk + addon.CloseBid)/2\n",
    "    addon['Symbol'] = x.replace('/','')\n",
    "    addon['Date'] = addon.index\n",
    "    addon['Date'] = pd.to_datetime(addon['Date'].dt.strftime(date_format='%Y-%m-%d'))\n",
    "    df_all = df_all[['Close','CloseAsk','CloseBid','High','HighAsk','HighBid','Low','LowAsk','LowBid','Open','OpenAsk','OpenBid','Symbol','Date']]\n",
    "    df_all = df_all.append(addon.iloc[1:-1,:])\n",
    "    #df_all = df_all.iloc[-263570:,:]\n",
    "    \n",
    "\n",
    "    ##### Si la période demandée est déjà H1, on peut construire directement la base daily pour tous les tickers => daily_all\n",
    "    if _period == 'H1':\n",
    "        df_all = timerange1D(df_all)\n",
    "        daily_all = get_daily(df_all,TICKER_LIST)\n",
    "\n",
    "    ##### Si la période n'est pas H1, on récupère d'abord les data en 1H pour tous les tickers, et on construit la base daily à partir du 1H => daily_all\n",
    "    else:\n",
    "        _period='H1'\n",
    "        df_all = timerange1D(df_all)\n",
    "        hourly_all = get_all_data(TICKER_LIST,_period='H1')\n",
    "        _fin = dt.datetime.now()\n",
    "        _deb = hourly_all.index[-1]\n",
    "        _debut = dt.datetime(_deb.year,_deb.month,_deb.day,_deb.hour)\n",
    "        hourly_add = con.get_candles(x,period='H1',start=_debut,end=_fin).drop(['tickqty'],axis=1) # df_all[df_all.index.minute==0] # scrap_hist(x)\n",
    "        hourly_add = hourly_add.rename(columns={'bidopen':'OpenBid','bidclose':'CloseBid','bidhigh':'HighBid','bidlow':'LowBid','askopen':'OpenAsk','askclose':'CloseAsk','askhigh':'HighAsk','asklow':'LowAsk'})\n",
    "        hourly_add['Open'] = (hourly_add.OpenAsk + hourly_add.OpenBid)/2\n",
    "        hourly_add['High'] = (hourly_add.HighAsk + hourly_add.HighBid)/2\n",
    "        hourly_add['Low'] = (hourly_add.LowAsk + hourly_add.LowBid)/2\n",
    "        hourly_add['Close'] = (hourly_add.CloseAsk + hourly_add.CloseBid)/2\n",
    "        hourly_add['Symbol'] = x.replace('/','')\n",
    "        hourly_all = hourly_all.append(hourly_add.iloc[1:-1,:])\n",
    "        hourly_all = timerange1D(hourly_all)\n",
    "        _period='m15'\n",
    "        daily_all = get_daily(hourly_all,TICKER_LIST)\n",
    "        #del hourly_all\n",
    "    daily_all = timerange1W(daily_all)\n",
    "    weekly_all = get_weekly(daily_all,TICKER_LIST)\n",
    "    daily_all = adr(daily_all,_window=14)\n",
    "    df_all = getadr(daily_all,df_all,TICKER_LIST)\n",
    "    df_all = adrhnl(daily_all,df_all,TICKER_LIST)\n",
    "    df_all = sma(df_all=df_all,_window=200)\n",
    "    df_all = bollinger(df_all,_slow=20)\n",
    "    df_all = slowstochastic(df_all,TICKER_LIST)\n",
    "    df_all = ema(df_all,21,TICKER_LIST)\n",
    "    df_all = ema(df_all,8,TICKER_LIST)\n",
    "    weekly_all = pivot(weekly_all,TICKER_LIST)\n",
    "    df_all = pivotimportdf(df_all,weekly_all,TICKER_LIST)\n",
    "    df_all = atr(df_all,TICKER_LIST,14)\n",
    "    df_all = rvi(df_all,TICKER_LIST,_window=14)\n",
    "    df_all = sbgamma(df_all,TICKER_LIST)\n",
    "    df_all = onhisma(df_all,TICKER_LIST,_window=5)\n",
    "    df_all = onlosma(df_all,TICKER_LIST,_window=5)\n",
    "    df_all = onhisma(df_all,TICKER_LIST,_window=21)\n",
    "    df_all = onlosma(df_all,TICKER_LIST,_window=21)\n",
    "    df_all = onhisma(df_all,TICKER_LIST,_window=34)\n",
    "    df_all = onlosma(df_all,TICKER_LIST,_window=34)\n",
    "    df_all = importohlc(df_all,weekly_all,TICKER_LIST,_suffix='_weekly')\n",
    "    df_all = importohlc(df_all=df_all,other_all=daily_all,TICKER_LIST=TICKER_LIST,_suffix='_daily')\n",
    "    \n",
    "    df_all = stochastic(df_all)\n",
    "    features = featuring(df_all)\n",
    "\n",
    "    # And drop the nan\n",
    "    features = features.dropna()\n",
    "    ##### Signal is from strategy. This is potential good one. But we have to create the TRACKER column where the Signal where efficient\n",
    "\n",
    "    # Proceed an MaxAbsScaler on features\n",
    "    features = scaling(features,scaler=_scaler)\n",
    "\n",
    "    features = quantile(features,quantile_transform)\n",
    "\n",
    "    _valid = _model.predict(features.drop(['Date','Symbol','Signal'],axis=1))[-1]\n",
    "\n",
    "    _signal = df_all.Signal[-1]\n",
    "\n",
    "    print('\\nTest sur la bougie',features.index[-1])\n",
    "    if _valid == 1 and _signal == 1 :\n",
    "        buy()\n",
    "\n",
    "    elif _valid == 1 and _signal == -1 :\n",
    "        sell()\n",
    "\n",
    "    else:\n",
    "        print(col.Fore.LIGHTWHITE_EX,'\\nnothing for',x,'\\n',col.Style.RESET_ALL)\n",
    "    \n",
    "    print()\n",
    "   \n",
    "    df_all = df_all[['Close','CloseAsk','CloseBid','High','HighAsk','HighBid','Low','LowAsk','LowBid','Open','OpenAsk','OpenBid','Symbol','Date']]  \n",
    "    #df_all = joblib.load(_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 4.05 s, sys: 4.07 s, total: 8.12 s\nWall time: 8.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_all = init_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}