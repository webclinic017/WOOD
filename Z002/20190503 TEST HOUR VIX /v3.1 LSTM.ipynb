{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "___Author___='LumberJack Jyss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LumberJack BRUTAL AtidotCom LSTM\n",
      "LumberJack Jyss 5579(c)\n"
     ]
    }
   ],
   "source": [
    "print('LumberJack BRUTAL AtidotCom LSTM\\nLumberJack Jyss 5579(c)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(palette='bright',style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('VIX.csv')\n",
    "df = pd.read_csv('dataset_Ruled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "#df.iloc[0].column=['Date']\n",
    "#df = df.drop(['Open_spy','High_spy','Low_spy'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spy_oc</th>\n",
       "      <th>spy_hl</th>\n",
       "      <th>spy_volume</th>\n",
       "      <th>spy_close</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>SPY</th>\n",
       "      <th>short</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20160504  15:30:00</th>\n",
       "      <td>-3.115814</td>\n",
       "      <td>0.124980</td>\n",
       "      <td>-0.344501</td>\n",
       "      <td>-0.003881</td>\n",
       "      <td>-0.016413</td>\n",
       "      <td>205.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160504  16:00:00</th>\n",
       "      <td>-1.750486</td>\n",
       "      <td>0.291626</td>\n",
       "      <td>0.538004</td>\n",
       "      <td>-0.001266</td>\n",
       "      <td>0.021632</td>\n",
       "      <td>205.09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160504  17:00:00</th>\n",
       "      <td>-0.444280</td>\n",
       "      <td>-0.279540</td>\n",
       "      <td>-0.443255</td>\n",
       "      <td>-0.000780</td>\n",
       "      <td>-0.000605</td>\n",
       "      <td>204.93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160504  18:00:00</th>\n",
       "      <td>0.666223</td>\n",
       "      <td>-0.104462</td>\n",
       "      <td>-0.369692</td>\n",
       "      <td>-0.001269</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>204.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20160504  19:00:00</th>\n",
       "      <td>-0.959616</td>\n",
       "      <td>-0.233294</td>\n",
       "      <td>0.077159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>204.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      spy_oc    spy_hl  spy_volume  spy_close  vix_close  \\\n",
       "Date                                                                       \n",
       "20160504  15:30:00 -3.115814  0.124980   -0.344501  -0.003881  -0.016413   \n",
       "20160504  16:00:00 -1.750486  0.291626    0.538004  -0.001266   0.021632   \n",
       "20160504  17:00:00 -0.444280 -0.279540   -0.443255  -0.000780  -0.000605   \n",
       "20160504  18:00:00  0.666223 -0.104462   -0.369692  -0.001269   0.004237   \n",
       "20160504  19:00:00 -0.959616 -0.233294    0.077159   0.000000   0.004219   \n",
       "\n",
       "                       SPY  short  long  \n",
       "Date                                     \n",
       "20160504  15:30:00  205.35      1     0  \n",
       "20160504  16:00:00  205.09      1     0  \n",
       "20160504  17:00:00  204.93      1     0  \n",
       "20160504  18:00:00  204.67      1     0  \n",
       "20160504  19:00:00  204.67      1     0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index('Date', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Période d'étude :  5256 périodes\n",
      "Sur un découpage 80% - 20% de la période : \n",
      "Bloc 1 :  4730  périodes \n",
      "Bloc 2 : 526  périodes\n"
     ]
    }
   ],
   "source": [
    "delta = df.shape[0]\n",
    "bloc1 = round(delta*0.9)\n",
    "bloc2 = delta - bloc1\n",
    "print(\"Période d'étude : \",delta,'périodes')\n",
    "print('Sur un découpage 80% - 20% de la période : ')\n",
    "print('Bloc 1 : ',bloc1,' périodes \\nBloc 2 :',bloc2,' périodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_long = df.iloc[:bloc1,:5]\n",
    "Xtest_long = df.iloc[bloc1:,:5]\n",
    "ytrain_long = df.iloc[:bloc1,7]\n",
    "ytest_long = df.iloc[bloc1:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split effectué\n"
     ]
    }
   ],
   "source": [
    "Xtrain_short = df.iloc[:bloc1,:5]\n",
    "Xtest_short = df.iloc[bloc1:,:5]\n",
    "ytrain_short = df.iloc[:bloc1,6]\n",
    "ytest_short = df.iloc[bloc1:,6]\n",
    "print('Split effectué')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4730, 5), (526, 5), (4730,), (526,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_long.shape,Xtest_long.shape,ytrain_long.shape,ytest_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4730, 1, 5), (526, 1, 5), (4730,), (526,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "Xtrain_long = Xtrain_long.values.reshape((Xtrain_long.shape[0], 1, Xtrain_long.shape[1]))\n",
    "Xtest_long = Xtest_long.values.reshape((Xtest_long.shape[0], 1, Xtest_long.shape[1]))\n",
    "Xtrain_long.shape,Xtest_long.shape,ytrain_long.shape,ytest_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4730, 1, 5), (526, 1, 5), (4730,), (526,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_short = Xtrain_short.values.reshape((Xtrain_short.shape[0], 1, Xtrain_short.shape[1]))\n",
    "Xtest_short = Xtest_short.values.reshape((Xtest_short.shape[0], 1, Xtest_short.shape[1]))\n",
    "Xtrain_short.shape,Xtest_short.shape,ytrain_short.shape,ytest_short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/YTsBaCh/Applications/anaconda3/envs/LumberJack/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/YTsBaCh/Applications/anaconda3/envs/LumberJack/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4730 samples, validate on 526 samples\n",
      "Epoch 1/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.4473 - acc: 0.5628 - val_loss: 0.4104 - val_acc: 0.6065\n",
      "Epoch 2/200\n",
      "4730/4730 [==============================] - 32s 7ms/step - loss: 0.4339 - acc: 0.5973 - val_loss: 0.4045 - val_acc: 0.6293\n",
      "Epoch 3/200\n",
      "4730/4730 [==============================] - 32s 7ms/step - loss: 0.4239 - acc: 0.6154 - val_loss: 0.4006 - val_acc: 0.6464\n",
      "Epoch 4/200\n",
      "4730/4730 [==============================] - 32s 7ms/step - loss: 0.4191 - acc: 0.6224 - val_loss: 0.3998 - val_acc: 0.6559\n",
      "Epoch 5/200\n",
      "4730/4730 [==============================] - 32s 7ms/step - loss: 0.4173 - acc: 0.6233 - val_loss: 0.4018 - val_acc: 0.6483\n",
      "Epoch 6/200\n",
      "4730/4730 [==============================] - 32s 7ms/step - loss: 0.4147 - acc: 0.6300 - val_loss: 0.3997 - val_acc: 0.6559\n",
      "Epoch 7/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.4127 - acc: 0.6304 - val_loss: 0.3988 - val_acc: 0.6597\n",
      "Epoch 8/200\n",
      "4730/4730 [==============================] - 32s 7ms/step - loss: 0.4101 - acc: 0.6423 - val_loss: 0.4008 - val_acc: 0.6616\n",
      "Epoch 9/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.4078 - acc: 0.6433 - val_loss: 0.4002 - val_acc: 0.6578\n",
      "Epoch 10/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.4070 - acc: 0.6448 - val_loss: 0.3982 - val_acc: 0.6578\n",
      "Epoch 11/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.4050 - acc: 0.6488 - val_loss: 0.3982 - val_acc: 0.6578\n",
      "Epoch 12/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.4049 - acc: 0.6501 - val_loss: 0.3975 - val_acc: 0.6597\n",
      "Epoch 13/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.4017 - acc: 0.6560 - val_loss: 0.3960 - val_acc: 0.6616\n",
      "Epoch 14/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.4015 - acc: 0.6533 - val_loss: 0.3977 - val_acc: 0.6635\n",
      "Epoch 15/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.4007 - acc: 0.6564 - val_loss: 0.3954 - val_acc: 0.6654\n",
      "Epoch 16/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3987 - acc: 0.6588 - val_loss: 0.3971 - val_acc: 0.6540\n",
      "Epoch 17/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3978 - acc: 0.6579 - val_loss: 0.3945 - val_acc: 0.6597\n",
      "Epoch 18/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3952 - acc: 0.6634 - val_loss: 0.3943 - val_acc: 0.6616\n",
      "Epoch 19/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3947 - acc: 0.6655 - val_loss: 0.3938 - val_acc: 0.6654\n",
      "Epoch 20/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3941 - acc: 0.6668 - val_loss: 0.3937 - val_acc: 0.6616\n",
      "Epoch 21/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3929 - acc: 0.6658 - val_loss: 0.3943 - val_acc: 0.6597\n",
      "Epoch 22/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3917 - acc: 0.6658 - val_loss: 0.3924 - val_acc: 0.6540\n",
      "Epoch 23/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3917 - acc: 0.6679 - val_loss: 0.3929 - val_acc: 0.6730\n",
      "Epoch 24/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3899 - acc: 0.6698 - val_loss: 0.3912 - val_acc: 0.6673\n",
      "Epoch 25/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3914 - acc: 0.6702 - val_loss: 0.3910 - val_acc: 0.6616\n",
      "Epoch 26/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3906 - acc: 0.6668 - val_loss: 0.3906 - val_acc: 0.6597\n",
      "Epoch 27/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3901 - acc: 0.6687 - val_loss: 0.3900 - val_acc: 0.6597\n",
      "Epoch 28/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3888 - acc: 0.6706 - val_loss: 0.3912 - val_acc: 0.6597\n",
      "Epoch 29/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3876 - acc: 0.6710 - val_loss: 0.3906 - val_acc: 0.6445\n",
      "Epoch 30/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3871 - acc: 0.6727 - val_loss: 0.3910 - val_acc: 0.6635\n",
      "Epoch 31/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3860 - acc: 0.6725 - val_loss: 0.3898 - val_acc: 0.6559\n",
      "Epoch 32/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3844 - acc: 0.6744 - val_loss: 0.3891 - val_acc: 0.6616\n",
      "Epoch 33/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3839 - acc: 0.6734 - val_loss: 0.3882 - val_acc: 0.6616\n",
      "Epoch 34/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3846 - acc: 0.6742 - val_loss: 0.3891 - val_acc: 0.6578\n",
      "Epoch 35/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3835 - acc: 0.6744 - val_loss: 0.3871 - val_acc: 0.6616\n",
      "Epoch 36/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3825 - acc: 0.6782 - val_loss: 0.3868 - val_acc: 0.6692\n",
      "Epoch 37/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3811 - acc: 0.6774 - val_loss: 0.3889 - val_acc: 0.6521\n",
      "Epoch 38/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3797 - acc: 0.6797 - val_loss: 0.3894 - val_acc: 0.6578\n",
      "Epoch 39/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3802 - acc: 0.6797 - val_loss: 0.3887 - val_acc: 0.6616\n",
      "Epoch 40/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3806 - acc: 0.6784 - val_loss: 0.3888 - val_acc: 0.6635\n",
      "Epoch 41/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3800 - acc: 0.6770 - val_loss: 0.3897 - val_acc: 0.6635\n",
      "Epoch 42/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3787 - acc: 0.6818 - val_loss: 0.3902 - val_acc: 0.6616\n",
      "Epoch 43/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3771 - acc: 0.6833 - val_loss: 0.3893 - val_acc: 0.6559\n",
      "Epoch 44/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3768 - acc: 0.6869 - val_loss: 0.3900 - val_acc: 0.6635\n",
      "Epoch 45/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3765 - acc: 0.6837 - val_loss: 0.3895 - val_acc: 0.6692\n",
      "Epoch 46/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3758 - acc: 0.6854 - val_loss: 0.3878 - val_acc: 0.6730\n",
      "Epoch 47/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3766 - acc: 0.6833 - val_loss: 0.3894 - val_acc: 0.6730\n",
      "Epoch 48/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3759 - acc: 0.6835 - val_loss: 0.3874 - val_acc: 0.6578\n",
      "Epoch 49/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3750 - acc: 0.6858 - val_loss: 0.3876 - val_acc: 0.6654\n",
      "Epoch 50/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3752 - acc: 0.6854 - val_loss: 0.3867 - val_acc: 0.6692\n",
      "Epoch 51/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3731 - acc: 0.6875 - val_loss: 0.3867 - val_acc: 0.6692\n",
      "Epoch 52/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3724 - acc: 0.6888 - val_loss: 0.3873 - val_acc: 0.6692\n",
      "Epoch 53/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3719 - acc: 0.6890 - val_loss: 0.3879 - val_acc: 0.6616\n",
      "Epoch 54/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3724 - acc: 0.6848 - val_loss: 0.3925 - val_acc: 0.6825\n",
      "Epoch 55/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3736 - acc: 0.6856 - val_loss: 0.3870 - val_acc: 0.6806\n",
      "Epoch 56/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3715 - acc: 0.6886 - val_loss: 0.3874 - val_acc: 0.6806\n",
      "Epoch 57/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3702 - acc: 0.6873 - val_loss: 0.3870 - val_acc: 0.6825\n",
      "Epoch 58/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3690 - acc: 0.6915 - val_loss: 0.3871 - val_acc: 0.6730\n",
      "Epoch 59/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3684 - acc: 0.6896 - val_loss: 0.3867 - val_acc: 0.6768\n",
      "Epoch 60/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3685 - acc: 0.6877 - val_loss: 0.3860 - val_acc: 0.6806\n",
      "Epoch 61/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3681 - acc: 0.6890 - val_loss: 0.3866 - val_acc: 0.6730\n",
      "Epoch 62/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3675 - acc: 0.6918 - val_loss: 0.3861 - val_acc: 0.6825\n",
      "Epoch 63/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3675 - acc: 0.6905 - val_loss: 0.3872 - val_acc: 0.6768\n",
      "Epoch 64/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3672 - acc: 0.6903 - val_loss: 0.3868 - val_acc: 0.6787\n",
      "Epoch 65/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3664 - acc: 0.6930 - val_loss: 0.3855 - val_acc: 0.6825\n",
      "Epoch 66/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3650 - acc: 0.6949 - val_loss: 0.3862 - val_acc: 0.6749\n",
      "Epoch 67/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3682 - acc: 0.6932 - val_loss: 0.3868 - val_acc: 0.6882\n",
      "Epoch 68/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3646 - acc: 0.6966 - val_loss: 0.3862 - val_acc: 0.6806\n",
      "Epoch 69/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3638 - acc: 0.6962 - val_loss: 0.3864 - val_acc: 0.6806\n",
      "Epoch 70/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3633 - acc: 0.6970 - val_loss: 0.3847 - val_acc: 0.6806\n",
      "Epoch 71/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3629 - acc: 0.6987 - val_loss: 0.3869 - val_acc: 0.6787\n",
      "Epoch 72/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3623 - acc: 0.7006 - val_loss: 0.3852 - val_acc: 0.6825\n",
      "Epoch 73/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3621 - acc: 0.6998 - val_loss: 0.3852 - val_acc: 0.6825\n",
      "Epoch 74/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3641 - acc: 0.6962 - val_loss: 0.3821 - val_acc: 0.6863\n",
      "Epoch 75/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3621 - acc: 0.6996 - val_loss: 0.3859 - val_acc: 0.6806\n",
      "Epoch 76/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3611 - acc: 0.7015 - val_loss: 0.3862 - val_acc: 0.6768\n",
      "Epoch 77/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3608 - acc: 0.7027 - val_loss: 0.3852 - val_acc: 0.6787\n",
      "Epoch 78/200\n",
      "4730/4730 [==============================] - 35s 7ms/step - loss: 0.3609 - acc: 0.7017 - val_loss: 0.3840 - val_acc: 0.6806\n",
      "Epoch 79/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3609 - acc: 0.7011 - val_loss: 0.3844 - val_acc: 0.6844\n",
      "Epoch 80/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3591 - acc: 0.7019 - val_loss: 0.3847 - val_acc: 0.6806\n",
      "Epoch 81/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3587 - acc: 0.7030 - val_loss: 0.3848 - val_acc: 0.6806\n",
      "Epoch 82/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3586 - acc: 0.7019 - val_loss: 0.3846 - val_acc: 0.6825\n",
      "Epoch 83/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3585 - acc: 0.7023 - val_loss: 0.3853 - val_acc: 0.6825\n",
      "Epoch 84/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3581 - acc: 0.7040 - val_loss: 0.3841 - val_acc: 0.6806\n",
      "Epoch 85/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3569 - acc: 0.7036 - val_loss: 0.3819 - val_acc: 0.6825\n",
      "Epoch 86/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3577 - acc: 0.7053 - val_loss: 0.3846 - val_acc: 0.6825\n",
      "Epoch 87/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3567 - acc: 0.7053 - val_loss: 0.3857 - val_acc: 0.6825\n",
      "Epoch 88/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3564 - acc: 0.7063 - val_loss: 0.3860 - val_acc: 0.6787\n",
      "Epoch 89/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3567 - acc: 0.7053 - val_loss: 0.3836 - val_acc: 0.6730\n",
      "Epoch 90/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3561 - acc: 0.7049 - val_loss: 0.3840 - val_acc: 0.6730\n",
      "Epoch 91/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3554 - acc: 0.7057 - val_loss: 0.3847 - val_acc: 0.6768\n",
      "Epoch 92/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3556 - acc: 0.7087 - val_loss: 0.3836 - val_acc: 0.6749\n",
      "Epoch 93/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3551 - acc: 0.7076 - val_loss: 0.3841 - val_acc: 0.6749\n",
      "Epoch 94/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3535 - acc: 0.7114 - val_loss: 0.3852 - val_acc: 0.6844\n",
      "Epoch 95/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3575 - acc: 0.7068 - val_loss: 0.3864 - val_acc: 0.6768\n",
      "Epoch 96/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3551 - acc: 0.7070 - val_loss: 0.3840 - val_acc: 0.6844\n",
      "Epoch 97/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3544 - acc: 0.7104 - val_loss: 0.3833 - val_acc: 0.6749\n",
      "Epoch 98/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3543 - acc: 0.7070 - val_loss: 0.3813 - val_acc: 0.6882\n",
      "Epoch 99/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3553 - acc: 0.7085 - val_loss: 0.3864 - val_acc: 0.6882\n",
      "Epoch 100/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3540 - acc: 0.7116 - val_loss: 0.3895 - val_acc: 0.6844\n",
      "Epoch 101/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3547 - acc: 0.7066 - val_loss: 0.3865 - val_acc: 0.6844\n",
      "Epoch 102/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3528 - acc: 0.7104 - val_loss: 0.3843 - val_acc: 0.6920\n",
      "Epoch 103/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3525 - acc: 0.7127 - val_loss: 0.3825 - val_acc: 0.6806\n",
      "Epoch 104/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3547 - acc: 0.7078 - val_loss: 0.3844 - val_acc: 0.6787\n",
      "Epoch 105/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3515 - acc: 0.7173 - val_loss: 0.3831 - val_acc: 0.6844\n",
      "Epoch 106/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3509 - acc: 0.7142 - val_loss: 0.3841 - val_acc: 0.6825\n",
      "Epoch 107/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3517 - acc: 0.7108 - val_loss: 0.3881 - val_acc: 0.6882\n",
      "Epoch 108/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3521 - acc: 0.7125 - val_loss: 0.3834 - val_acc: 0.6882\n",
      "Epoch 109/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3509 - acc: 0.7173 - val_loss: 0.3834 - val_acc: 0.6863\n",
      "Epoch 110/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3549 - acc: 0.7093 - val_loss: 0.3859 - val_acc: 0.6787\n",
      "Epoch 111/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3504 - acc: 0.7144 - val_loss: 0.3825 - val_acc: 0.6882\n",
      "Epoch 112/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3503 - acc: 0.7142 - val_loss: 0.3836 - val_acc: 0.6863\n",
      "Epoch 113/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3505 - acc: 0.7148 - val_loss: 0.3839 - val_acc: 0.6806\n",
      "Epoch 114/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3499 - acc: 0.7133 - val_loss: 0.3829 - val_acc: 0.6844\n",
      "Epoch 115/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3489 - acc: 0.7154 - val_loss: 0.3854 - val_acc: 0.6844\n",
      "Epoch 116/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3482 - acc: 0.7169 - val_loss: 0.3836 - val_acc: 0.6882\n",
      "Epoch 117/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3478 - acc: 0.7150 - val_loss: 0.3830 - val_acc: 0.6825\n",
      "Epoch 118/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3490 - acc: 0.7118 - val_loss: 0.3844 - val_acc: 0.6920\n",
      "Epoch 119/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3497 - acc: 0.7150 - val_loss: 0.3845 - val_acc: 0.6882\n",
      "Epoch 120/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3491 - acc: 0.7161 - val_loss: 0.3859 - val_acc: 0.6844\n",
      "Epoch 121/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3485 - acc: 0.7129 - val_loss: 0.3851 - val_acc: 0.6920\n",
      "Epoch 122/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3482 - acc: 0.7152 - val_loss: 0.3833 - val_acc: 0.6768\n",
      "Epoch 123/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3502 - acc: 0.7121 - val_loss: 0.3850 - val_acc: 0.6730\n",
      "Epoch 124/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3482 - acc: 0.7161 - val_loss: 0.3821 - val_acc: 0.6825\n",
      "Epoch 125/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3475 - acc: 0.7182 - val_loss: 0.3832 - val_acc: 0.6806\n",
      "Epoch 126/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3466 - acc: 0.7167 - val_loss: 0.3839 - val_acc: 0.6768\n",
      "Epoch 127/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3478 - acc: 0.7184 - val_loss: 0.3833 - val_acc: 0.6787\n",
      "Epoch 128/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3468 - acc: 0.7137 - val_loss: 0.3833 - val_acc: 0.6806\n",
      "Epoch 129/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3471 - acc: 0.7180 - val_loss: 0.3818 - val_acc: 0.6844\n",
      "Epoch 130/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3463 - acc: 0.7195 - val_loss: 0.3816 - val_acc: 0.6825\n",
      "Epoch 131/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3462 - acc: 0.7195 - val_loss: 0.3866 - val_acc: 0.6844\n",
      "Epoch 132/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3476 - acc: 0.7175 - val_loss: 0.3804 - val_acc: 0.6920\n",
      "Epoch 133/200\n",
      "4730/4730 [==============================] - 34s 7ms/step - loss: 0.3460 - acc: 0.7220 - val_loss: 0.3847 - val_acc: 0.6825\n",
      "Epoch 134/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3459 - acc: 0.7205 - val_loss: 0.3881 - val_acc: 0.6882\n",
      "Epoch 135/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3462 - acc: 0.7197 - val_loss: 0.3847 - val_acc: 0.6787\n",
      "Epoch 136/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3450 - acc: 0.7175 - val_loss: 0.3850 - val_acc: 0.6787\n",
      "Epoch 137/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3470 - acc: 0.7150 - val_loss: 0.3828 - val_acc: 0.6768\n",
      "Epoch 138/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3441 - acc: 0.7220 - val_loss: 0.3853 - val_acc: 0.6787\n",
      "Epoch 139/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3435 - acc: 0.7211 - val_loss: 0.3839 - val_acc: 0.6749\n",
      "Epoch 140/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3435 - acc: 0.7216 - val_loss: 0.3830 - val_acc: 0.6844\n",
      "Epoch 141/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3443 - acc: 0.7199 - val_loss: 0.3850 - val_acc: 0.6882\n",
      "Epoch 142/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3448 - acc: 0.7205 - val_loss: 0.3848 - val_acc: 0.6844\n",
      "Epoch 143/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3449 - acc: 0.7241 - val_loss: 0.3847 - val_acc: 0.6749\n",
      "Epoch 144/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3425 - acc: 0.7211 - val_loss: 0.3830 - val_acc: 0.6825\n",
      "Epoch 145/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3416 - acc: 0.7224 - val_loss: 0.3841 - val_acc: 0.6863\n",
      "Epoch 146/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3421 - acc: 0.7262 - val_loss: 0.3836 - val_acc: 0.6863\n",
      "Epoch 147/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3422 - acc: 0.7264 - val_loss: 0.3848 - val_acc: 0.6749\n",
      "Epoch 148/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3416 - acc: 0.7245 - val_loss: 0.3837 - val_acc: 0.6768\n",
      "Epoch 149/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3414 - acc: 0.7247 - val_loss: 0.3846 - val_acc: 0.6882\n",
      "Epoch 150/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3412 - acc: 0.7245 - val_loss: 0.3846 - val_acc: 0.6787\n",
      "Epoch 151/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3435 - acc: 0.7245 - val_loss: 0.3862 - val_acc: 0.6806\n",
      "Epoch 152/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3415 - acc: 0.7249 - val_loss: 0.3848 - val_acc: 0.6787\n",
      "Epoch 153/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3428 - acc: 0.7218 - val_loss: 0.3865 - val_acc: 0.6806\n",
      "Epoch 154/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3414 - acc: 0.7237 - val_loss: 0.3827 - val_acc: 0.6901\n",
      "Epoch 155/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3430 - acc: 0.7245 - val_loss: 0.3880 - val_acc: 0.6787\n",
      "Epoch 156/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3409 - acc: 0.7254 - val_loss: 0.3847 - val_acc: 0.6806\n",
      "Epoch 157/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3421 - acc: 0.7230 - val_loss: 0.3868 - val_acc: 0.6787\n",
      "Epoch 158/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3419 - acc: 0.7211 - val_loss: 0.3886 - val_acc: 0.6787\n",
      "Epoch 159/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3419 - acc: 0.7239 - val_loss: 0.3874 - val_acc: 0.6768\n",
      "Epoch 160/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3410 - acc: 0.7224 - val_loss: 0.3845 - val_acc: 0.6806\n",
      "Epoch 161/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3409 - acc: 0.7237 - val_loss: 0.3876 - val_acc: 0.6787\n",
      "Epoch 162/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3397 - acc: 0.7266 - val_loss: 0.3873 - val_acc: 0.6711\n",
      "Epoch 163/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3380 - acc: 0.7262 - val_loss: 0.3867 - val_acc: 0.6711\n",
      "Epoch 164/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3418 - acc: 0.7211 - val_loss: 0.3838 - val_acc: 0.6787\n",
      "Epoch 165/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3403 - acc: 0.7228 - val_loss: 0.3855 - val_acc: 0.6806\n",
      "Epoch 166/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3384 - acc: 0.7249 - val_loss: 0.3822 - val_acc: 0.6844\n",
      "Epoch 167/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3406 - acc: 0.7243 - val_loss: 0.3838 - val_acc: 0.6825\n",
      "Epoch 168/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3414 - acc: 0.7214 - val_loss: 0.3845 - val_acc: 0.6806\n",
      "Epoch 169/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3402 - acc: 0.7245 - val_loss: 0.3807 - val_acc: 0.6863\n",
      "Epoch 170/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3389 - acc: 0.7256 - val_loss: 0.3828 - val_acc: 0.6863\n",
      "Epoch 171/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3396 - acc: 0.7239 - val_loss: 0.3854 - val_acc: 0.6844\n",
      "Epoch 172/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3395 - acc: 0.7258 - val_loss: 0.3816 - val_acc: 0.6901\n",
      "Epoch 173/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3387 - acc: 0.7264 - val_loss: 0.3849 - val_acc: 0.6806\n",
      "Epoch 174/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3379 - acc: 0.7233 - val_loss: 0.3808 - val_acc: 0.6863\n",
      "Epoch 175/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3375 - acc: 0.7262 - val_loss: 0.3830 - val_acc: 0.6863\n",
      "Epoch 176/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3394 - acc: 0.7277 - val_loss: 0.3848 - val_acc: 0.6768\n",
      "Epoch 177/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3382 - acc: 0.7260 - val_loss: 0.3828 - val_acc: 0.6844\n",
      "Epoch 178/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3366 - acc: 0.7317 - val_loss: 0.3819 - val_acc: 0.6882\n",
      "Epoch 179/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3376 - acc: 0.7268 - val_loss: 0.3882 - val_acc: 0.6711\n",
      "Epoch 180/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3378 - acc: 0.7275 - val_loss: 0.3858 - val_acc: 0.6711\n",
      "Epoch 181/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3377 - acc: 0.7230 - val_loss: 0.3843 - val_acc: 0.6806\n",
      "Epoch 182/200\n",
      "4730/4730 [==============================] - 33s 7ms/step - loss: 0.3390 - acc: 0.7264 - val_loss: 0.3816 - val_acc: 0.6825\n",
      "Epoch 183/200\n",
      " 777/4730 [===>..........................] - ETA: 27s - loss: 0.3374 - acc: 0.7181"
     ]
    }
   ],
   "source": [
    "# design network\n",
    "model_long = Sequential()\n",
    "model_long.add(LSTM(units=100,\n",
    "    activation='tanh',\n",
    "    recurrent_activation='hard_sigmoid',\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros',\n",
    "    unit_forget_bias=True,\n",
    "    kernel_regularizer=None,\n",
    "    recurrent_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    recurrent_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    dropout=0.0,\n",
    "    recurrent_dropout=0.0,\n",
    "    implementation=1,\n",
    "    return_sequences=False,\n",
    "    return_state=False,\n",
    "    go_backwards=False,\n",
    "    stateful=False,\n",
    "    unroll=False,\n",
    "    input_shape=(Xtrain_long.shape[1],\n",
    "    Xtrain_long.shape[2])))\n",
    "\n",
    "model_long.add(Dense(1))\n",
    "model_long.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "history_long = model_long.fit(Xtrain_long, ytrain_long, epochs=200, batch_size=1,\\\n",
    "                    validation_data=(Xtest_long, ytest_long), verbose=1, shuffle=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "model_short = Sequential()\n",
    "model_short.add(LSTM(100, input_shape=(Xtrain_short.shape[1], Xtrain_short.shape[2])))\n",
    "model_short.add(Dense(1))\n",
    "model_short.compile(loss='mae', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "history_short = model_short.fit(Xtrain_short, ytrain_short, epochs=200, batch_size=1,\\\n",
    "                    validation_data=(Xtest_short, ytest_short), verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(history_long.history['loss'], label='train_long')\n",
    "plt.plot(history_long.history['val_loss'], label='test_long')\n",
    "plt.plot(history_short.history['loss'], label='train_test')\n",
    "plt.plot(history_short.history['val_loss'], label='test_test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "predictions_long = model_long.predict(Xtest_long)\n",
    "predictions_short = model_long.predict(Xtest_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest = pd.DataFrame()\n",
    "backtest['Close_SPY'] = df.iloc[-526:,5]\n",
    "backtest['long'] = predictions_long\n",
    "backtest['short'] = predictions_short\n",
    "backtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_pos = 0\n",
    "short_pos = 0\n",
    "equity = 200000\n",
    "price_buy_long = 0\n",
    "price_buy_short = 0\n",
    "equity_list = []\n",
    "nb_transaction = 0\n",
    "col_profit = []\n",
    "max_gain = (0,0)\n",
    "max_loss = (0,0)\n",
    "stop_loss_short = 0.002\n",
    "stop_loss_long = -0.002\n",
    "ticket = 100000\n",
    "target_long = 0.006\n",
    "target_short = -0.006\n",
    "longueur = (backtest.shape[0])-1\n",
    "long_exit = []\n",
    "short_exit = []\n",
    "long_sl = []\n",
    "short_sl = []\n",
    "go_long = []\n",
    "go_short = []\n",
    "lea = 0\n",
    "sea = 0\n",
    "gain_sum = 0\n",
    "loss_sum = 0\n",
    "\n",
    "print(\"Boucle de backtest initiée!\")\n",
    "\n",
    "import sys\n",
    "orig_stdout = sys.stdout\n",
    "f = open('backtestv2_5.txt', 'w')\n",
    "sys.stdout = f\n",
    "\n",
    "for i in range(0,longueur):\n",
    "    \n",
    "    if backtest.iloc[i,1] == 1 and long_pos == 0:\n",
    "        long_pos = 1\n",
    "        nb_transaction += 1\n",
    "        price_buy_long = backtest.iloc[i,0]\n",
    "        n = (ticket/price_buy_long)\n",
    "        equity = equity - n/200\n",
    "        equity_list.append(equity)\n",
    "        col_profit.append(0)\n",
    "        go_long.append(1)\n",
    "        print('\\n Go Long enclenché le ',backtest.index[i],' pour $',price_buy_long,'et ',n,' actions')\n",
    "        print('Nouvel equity :',equity)\n",
    "        \n",
    "    elif backtest.iloc[i,1] == 1 and long_pos == 1 :\n",
    "        equity_list.append(equity)\n",
    "        print('Le',backtest.index[i],', On maintient la position long ouverte')\n",
    "        col_profit.append(0)\n",
    "    \n",
    "    elif backtest.iloc[i,1] == 0 and long_pos == 1:\n",
    "        \n",
    "        if (backtest.iloc[i,0] - price_buy_long)/price_buy_long > target_long:\n",
    "            print('\\n sortie de position long ',backtest.index[i],' pour $',backtest.iloc[i,0])    \n",
    "            print(\"Le prix d'achat était de \",price_buy_long)\n",
    "            print('variation des deux ', backtest.iloc[i,0] - price_buy_long)\n",
    "            print(\"variation sur prix d'achat \", (backtest.iloc[i,0] - price_buy_long)/price_buy_long)\n",
    "            print(' pnl :',( backtest.iloc[i,0] - price_buy_long)*n)\n",
    "            equity = equity - n/200 + ((backtest.iloc[i,0] - price_buy_long) * n)\n",
    "            print('Le nouvel equity est de :',equity,'$')\n",
    "            nb_transaction += 1\n",
    "            long_exit.append(1)\n",
    "            col_profit.append(( backtest.iloc[i,0] - price_buy_long) * n)\n",
    "            if ((backtest.iloc[i-5,0] - price_buy_long) * n) > max_gain[1]:\n",
    "                max_gain = (i,((backtest.iloc[i,0] - price_buy_long) * n))\n",
    "                \n",
    "            elif ((backtest.iloc[i-5,0] - price_buy_long) * n)< max_loss[1]:\n",
    "                max_loss = (i,((backtest.iloc[i,0] - price_buy_long) * n))\n",
    "            equity_list.append(equity)\n",
    "            gain_sum = gain_sum + (backtest.iloc[i,0] - price_buy_long)*n\n",
    "            long_pos = 0\n",
    "        \n",
    "        elif (backtest.iloc[i,0] - price_buy_long)/price_buy_long < stop_loss_long:\n",
    "            equity = equity - n/200 + ((backtest.iloc[i,0] - price_buy_long) * n)\n",
    "            print('\\n sortie stop_loss du long ',backtest.index[i],' pour $',backtest.iloc[i,0])    \n",
    "            print(\"Le prix d'achat était de \",price_buy_long)\n",
    "            print(' pnl :',(backtest.iloc[i,0] - price_buy_long)*n)\n",
    "            print('Le nouvel equity est de :',equity,'$')\n",
    "            nb_transaction += 1\n",
    "            long_sl.append(1)\n",
    "            col_profit.append(((backtest.iloc[i,0] - price_buy_long) * n))\n",
    "            if ((backtest.iloc[i,0] - price_buy_long) * n) > max_gain[1]:\n",
    "                max_gain = (i-5,((backtest.iloc[i,0] - price_buy_long) * n))\n",
    "                \n",
    "            elif ((backtest.iloc[i,0] - price_buy_long) * n)< max_loss[1]:\n",
    "                max_loss = (i-5,((backtest.iloc[i,0] - price_buy_long) * n))\n",
    "            equity_list.append(equity)\n",
    "            long_pos = 0\n",
    "            loss_sum = loss_sum + abs(( backtest.iloc[i,0] - price_buy_long)*n)\n",
    "        \n",
    "    elif backtest.iloc[i,1] == 0 and long_pos == 0 :\n",
    "        equity_list.append(equity)\n",
    "        print('Le',backtest.index[i],', il ne se passe rien')\n",
    "        col_profit.append(0)\n",
    "            \n",
    "for i in range(0,longueur):\n",
    "    \n",
    "    if backtest.iloc[i,2] == 1 and short_pos == 0:\n",
    "        short_pos = 1\n",
    "        nb_transaction += 1\n",
    "        price_buy_short = backtest.iloc[i,0]\n",
    "        n = ticket/price_buy_short\n",
    "        equity = equity - n/200\n",
    "        equity_list.append(equity)\n",
    "        col_profit.append(0)\n",
    "        go_short.append(1)\n",
    "        print('\\n Go Short le ',backtest.index[i],' pour $',price_buy_short)\n",
    "        print('Le nouvel equity est de :',equity,'$')\n",
    "        \n",
    "    elif backtest.iloc[i,2] == 1 and short_pos == 1 :\n",
    "        equity_list.append(equity)\n",
    "        print('Le',backtest.index[i],', on reste en position Short')\n",
    "        col_profit.append(0)\n",
    "    \n",
    "    elif backtest.iloc[i,2] == 0 and short_pos == 1:\n",
    "        \n",
    "        if (backtest.iloc[i,0] - price_buy_short)/price_buy_short < target_short:\n",
    "            print('\\n sortie du position short ',backtest.index[i],' pour $',backtest.iloc[i,0])    \n",
    "            print(\"Le prix d'achat était de \",price_buy_short)\n",
    "            print(' pnl :',( backtest.iloc[i,0] - price_buy_short)*n)\n",
    "            equity = equity - n/200 + abs((backtest.iloc[i,0] - price_buy_short) * n)\n",
    "            print('Le nouvel equity est de :',equity,'$')\n",
    "            nb_transaction += 1\n",
    "            short_exit.append(1)\n",
    "            col_profit.append(abs((( backtest.iloc[i,0] - price_buy_short) * n)))\n",
    "            if ((backtest.iloc[i,0] - price_buy_short) * n) > max_gain[1]:\n",
    "                max_gain = (i,((backtest.iloc[i,0] - price_buy_short) * n))\n",
    "                \n",
    "            elif ((backtest.iloc[i,0] - price_buy_short) * n)< max_loss[1]:\n",
    "                max_loss = (i,((backtest.iloc[i,0] - price_buy_short) * n))\n",
    "            equity_list.append(equity)\n",
    "            short_pos = 0\n",
    "            gain_sum = gain_sum + abs((backtest.iloc[i,0] - price_buy_short) * n)\n",
    "        \n",
    "        elif (backtest.iloc[i,0] - price_buy_short) > stop_loss_short:\n",
    "            equity = equity - n/200 - abs((backtest.iloc[i,0] - price_buy_short)* n)\n",
    "            print('\\n sortie stop_loss du short ',backtest.index[i],' pour $',backtest.iloc[i,0])    \n",
    "            print(\"Le prix d'achat était de \",price_buy_short)\n",
    "            print(' pnl :',-(backtest.iloc[i,0] - price_buy_short)*n)\n",
    "            print('Le nouvel equity est de :',equity,'$')\n",
    "            nb_transaction += 1\n",
    "            short_sl.append(1)\n",
    "            col_profit.append(((backtest.iloc[i,0] - price_buy_short) * n))\n",
    "            if ((backtest.iloc[i,0] - price_buy_short) * n) > max_gain[1]:\n",
    "                max_gain = (i,((backtest.iloc[i,0] - price_buy_short) * n))\n",
    "                \n",
    "            elif ((backtest.iloc[i,0] - price_buy_short) * n)< max_loss[1]:\n",
    "                max_loss = (i,((backtest.iloc[i,0] - price_buy_short) * n))\n",
    "            equity_list.append(equity)\n",
    "            short_pos = 0\n",
    "            loss_sum = loss_sum + abs((backtest.iloc[i,0] - price_buy_short) * n)\n",
    "        \n",
    "    elif backtest.iloc[i,2] == 0 and short_pos == 0 :\n",
    "        equity_list.append(equity)\n",
    "        print('Le',backtest.index[i],', il ne se passe rien')\n",
    "        col_profit.append(0)    \n",
    "\n",
    "\n",
    "if long_pos == 1 :\n",
    "    print(\"Sortie d'une position long en l'air\")\n",
    "    equity = equity - n/200 + ((backtest.iloc[longueur,0]- price_buy_long) * n)\n",
    "    lea = 1\n",
    "if short_pos == 1 :\n",
    "    print(\"Sortie d'une position short en l'air\")\n",
    "    equity = equity - n/200 + ((backtest.iloc[longueur,0]- price_buy_short) * n)\n",
    "    sea = 1\n",
    "    \n",
    "            \n",
    "print(\"\\n\\033[95m \\033[1m Les gains faramineux s'élèvent à : $\",equity-200000,'!. En ',nb_transaction,' transactions.' )\n",
    "f.close()\n",
    "sys.stdout = orig_stdout\n",
    "print('Boucle terminée')\n",
    "print(\"\\n\\033[95m \\033[1m Les gains faramineux s'élèvent à : $\",equity-200000,'!. En ',nb_transaction,' transactions.' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_graph = pd.DataFrame()\n",
    "backtest_graph['equity'] = equity_list\n",
    "backtest_graph['col_profit'] = col_profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "backtest_graph['equity'].plot(ylim=(180000))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(backtest_graph['col_profit'])\n",
    "plt.scatter(max_gain[0],max_gain[1],c='y')\n",
    "plt.scatter(max_loss[0],max_loss[1],c='r')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumul_profit = sum(col_profit)\n",
    "nul_profit = col_profit.count(0)\n",
    "len_profit = len(col_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Période testée :')\n",
    "print('Début : ',backtest.index[0])\n",
    "print('Fin : ',backtest.index[-1])\n",
    "print('ROC long : ',round(roc_long*100,2),'%')\n",
    "print('ROC short : ',round(roc_short*100,2),'%')\n",
    "print('Transaction à $100,000')\n",
    "print('Equity initiale : $200,000')\n",
    "print('Equity finale : ',round(equity,2))\n",
    "print('Gain généré :',round(equity-200000,2),'$')\n",
    "print('Nombre de transactions : ',nb_transaction)\n",
    "print('Nombre de Long : ',sum(go_long))\n",
    "print('Nombre de Short : ',sum(go_short))\n",
    "print('Nombre winners : ',sum(long_exit) + sum(short_exit))\n",
    "print('Nombre loosers : ',sum(long_sl) + sum(short_sl))\n",
    "print('Nombre de long winners : ',sum(long_exit))\n",
    "print('Nombre de short winners : ',sum(short_exit))\n",
    "print('Nombre de long loosers : ',sum(long_sl))\n",
    "print('Nombre de short loosers : ',sum(short_sl))\n",
    "print(\"Nombre de short en l'air : \", sea)\n",
    "print(\"Nombre long en l'air : \",lea)\n",
    "print('Fees : ',nb_transaction*1.5)\n",
    "#print('Nombre de positions gagnées',sum(col_is_win))\n",
    "#print('Nombre de positions perdues',sum(col_close_pos)-sum(col_is_win))\n",
    "print('Gain maximal',round(max(col_profit),2),' ',round(max_gain[1],2))\n",
    "print('Perte maximale',round(min(col_profit),2),' ',round(max_loss[1],2))\n",
    "print('Moyenne du gain',round(((equity-200000)/(nb_transaction/2)),2))\n",
    "print('Somme de tous les gains',round(gain_sum,2))\n",
    "print('Somme de tous le loss',round(abs(loss_sum),2))\n",
    "print('profit factor',round(abs(gain_sum/loss_sum),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model_long.sav'\n",
    "pickle.dump(model_long, open(filename, 'wb'))\n",
    "filename = 'finalized_model_short.sav'\n",
    "pickle.dump(model_short, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
