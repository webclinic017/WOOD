{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "___Author___='LumberJack Jyss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Optimized LumberJack Environment Motor\n",
      "°OoO_FXCM_Oo0°\n",
      "LumberJack Jyss 5779(c)\n",
      "Version v0.02\n"
     ]
    }
   ],
   "source": [
    "print('Global Optimized LumberJack Environment Motor\\n°OoO_FXCM_Oo0°\\nLumberJack Jyss 5779(c)')\n",
    "print('Version v0.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing librairies...\n",
      "version fxcmpy : 1.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librairies imported\n"
     ]
    }
   ],
   "source": [
    "print('Importing librairies...')\n",
    "import fxcmpy\n",
    "print('version fxcmpy :',fxcmpy.__version__)\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import talib\n",
    "from zigzag import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "#import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score,roc_curve,confusion_matrix,classification_report\n",
    "print('Librairies imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = 'e053ac1597cef331df9429ac8151100ea9f1c411'\n",
    "server = 'demo'\n",
    "\n",
    "# minutes: 'm1' , 'm5' , 'm15' , 'm30' \n",
    "# hours: 'H1' , 'H2', 'H3', 'H4', 'H6' 'H8'\n",
    "# one day: 'D1'\n",
    "# one week: 'W1'\n",
    "# one month: 'M1'\n",
    "\n",
    "period = 'm1'\n",
    "number = 10000\n",
    "# Time Windows\n",
    "# start = dt.datetime(2017, 7, 15)\n",
    "# stop = dt.datetime(2017, 8, 1)\n",
    "# con.get_candles('EUR/USD', period='D1',start=start, stop=stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0613 13:32:15.475352 4694128064 fxcmpy.py:222] Default account set to 1147539, to change use set_default_account().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established for [1147539]  - Mode : demo\n",
      "executed in = 42.380000 secondes\n"
     ]
    }
   ],
   "source": [
    "tmps1=time.time()\n",
    "print('Connecting server...')\n",
    "con = fxcmpy.fxcmpy(access_token=TOKEN, log_level='error',server= server)\n",
    "if con.is_connected():\n",
    "    print('Connection established for',con.get_account_ids(),' - Mode :',server)\n",
    "else:\n",
    "    print('Not connected')\n",
    "    \n",
    "df = con.get_candles('SPX500', period=period,number=number) #start=start,stop=stop)\n",
    "data = pd.DataFrame()\n",
    "data['Open'] = df['askopen']\n",
    "data['High'] = df['askhigh']\n",
    "data['Low'] = df['asklow']\n",
    "data['Close'] = df['askclose']\n",
    "data['Date'] = df.index\n",
    "data = data[['Date'] + data.columns[:-1].tolist()]\n",
    "df = data.copy()\n",
    "\n",
    "tmps2=round(time.time()-tmps1,2)\n",
    "print (\"executed in = %f\" %tmps2,'secondes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GOLEM begins Computing...\n",
      "\n",
      "\n",
      "Processing move_up\n",
      "\n",
      "\n",
      "Processing move_down\n",
      "\n",
      "\n",
      "Computing done\n",
      "\n",
      "\n",
      "9950/9950 [==============================] - 0s 16us/step\n",
      "9950/9950 [==============================] - 0s 17us/step\n",
      "1/1 [==============================] - 0s 539us/step\n",
      "1/1 [==============================] - 0s 430us/step\n",
      "\n",
      "\n",
      "RESULTATS UP\n",
      "\n",
      "Accuracy: 100.00%\n",
      "Precision: 0.00%   => Discrimnination des vrais positifs parmi les faux positifs\n",
      "Recall: 0.00%   => Positifs trouvés par Golem sur tous les positifs existants\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n",
      "[[1]]\n",
      "\n",
      "\n",
      "_______________________________________________________________________________________________________________________________________________________________\n",
      "\n",
      "RESULTATS DOWN\n",
      "\n",
      "Accuracy: 0.00%\n",
      "Precision: 0.00%   => Discrimnination des vrais positifs parmi les faux positifs\n",
      "Recall: 0.00%   => Positifs trouvés par Golem sur tous les positifs existants\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.00      0.00      0.00         1\n",
      "   macro avg       0.00      0.00      0.00         1\n",
      "weighted avg       0.00      0.00      0.00         1\n",
      "\n",
      "[[0 1]\n",
      " [0 0]]\n",
      "\n",
      "\n",
      "executed in = 28.860000 secondes\n"
     ]
    }
   ],
   "source": [
    "tmps1=time.time()\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "dataset_1D = df.copy()\n",
    "dataset_1D = dataset_1D.dropna()\n",
    "rsi = talib.RSI(dataset_1D['Close'],timeperiod=2)\n",
    "upper, middle, lower =  talib.BBANDS(dataset_1D['Close'], timeperiod=9, nbdevup=2, nbdevdn=2,matype=0)\n",
    "sma5 = talib.SMA(dataset_1D['Close'],timeperiod=5)\n",
    "sma8 = talib.SMA(dataset_1D['Close'],timeperiod=8)\n",
    "sma10 = talib.SMA(dataset_1D['Close'],timeperiod=10)\n",
    "sma12 = talib.SMA(dataset_1D['Close'],timeperiod=12)\n",
    "sma15 = talib.SMA(dataset_1D['Close'],timeperiod=15)\n",
    "sma30 = talib.SMA(dataset_1D['Close'],timeperiod=30)\n",
    "sma35 = talib.SMA(dataset_1D['Close'],timeperiod=35)\n",
    "sma40 = talib.SMA(dataset_1D['Close'],timeperiod=40)\n",
    "sma45 = talib.SMA(dataset_1D['Close'],timeperiod=45)\n",
    "sma50 = talib.SMA(dataset_1D['Close'],timeperiod=50)\n",
    "\n",
    "delta5_8 = sma5 - sma8\n",
    "delta8_10 = sma8 - sma10\n",
    "delta10_12 = sma10 - sma12\n",
    "delta12_15 = sma12 - sma15\n",
    "delta15_30 = sma15 - sma30\n",
    "delta30_35 = sma30 - sma35\n",
    "delta35_40 = sma35 - sma40\n",
    "delta40_45 = sma40 - sma45\n",
    "delta45_50 = sma45 - sma50\n",
    "\n",
    "X = dataset_1D['Close']\n",
    "pivots = peak_valley_pivots(X.values, 0.001, -0.001)\n",
    "ts_pivots = pd.Series(X, index=X.index)\n",
    "ts_pivots = ts_pivots[pivots != 0]\n",
    "\n",
    "peak = []\n",
    "valley = []\n",
    "\n",
    "for i in range(0,dataset_1D.shape[0]):\n",
    "    if pivots[i] == 1:\n",
    "        peak.append(pivots[i])\n",
    "        valley.append(0)\n",
    "    elif pivots[i] == -1:\n",
    "        peak.append(0)\n",
    "        valley.append(pivots[i])\n",
    "    else:\n",
    "        peak.append(0)\n",
    "        valley.append(0)\n",
    "\n",
    "bbdelta = upper - middle\n",
    "price_bolup = dataset_1D['Close'] - lower\n",
    "price_bolow = dataset_1D['Close'] - upper\n",
    "\n",
    "rsi5_list = []\n",
    "rsi95_list = []\n",
    "for i in range(0,dataset_1D.shape[0]):\n",
    "    try:\n",
    "        rsi95_list.append(95 - rsi[i])\n",
    "        rsi5_list.append(rsi[i] - 5)\n",
    "    except:\n",
    "        rsi95_list.append(0)\n",
    "        rsi5_list.append(0)\n",
    "varop_spy = dataset_1D['Open'] - dataset_1D['Close']\n",
    "varhl_spy = dataset_1D['High'] - dataset_1D['Low']\n",
    "dataset_1D['Varop_Spy'] = varop_spy\n",
    "dataset_1D['Varhl_spy'] = varhl_spy\n",
    "dataset_1D['RSI'] = rsi\n",
    "dataset_1D['95 - RSI'] = np.array(rsi95_list)\n",
    "dataset_1D['RSI - 5'] = np.array(rsi5_list)\n",
    "dataset_1D['BBD_Delta_Up'] = bbdelta\n",
    "dataset_1D['delta5_8'] = delta5_8\n",
    "dataset_1D['delta8_10'] = delta8_10\n",
    "dataset_1D['delta10_12'] = delta10_12\n",
    "dataset_1D['delta12_15'] = delta12_15\n",
    "dataset_1D['delta15_30'] = delta15_30\n",
    "dataset_1D['delta30_35'] = delta30_35\n",
    "dataset_1D['delta35_40'] = delta35_40\n",
    "dataset_1D['delta40_45'] = delta40_45\n",
    "dataset_1D['delta45_50'] = delta45_50\n",
    "dataset_1D['Peaks'] = abs(np.array(peak))\n",
    "dataset_1D['Valley'] = abs(np.array(valley))\n",
    "tsf = talib.TSF(dataset_1D['Close'],timeperiod=14)\n",
    "delta_tsf = dataset_1D['Close'] - tsf\n",
    "dataset_1D['delta_tsf'] = tsf\n",
    "forosc = []\n",
    "forosc.append(0)\n",
    "for i in range(1,dataset_1D.shape[0]):\n",
    "    try:\n",
    "        forosc.append((dataset_1D.iloc[i,4] - tsf[i-1])*100/dataset_1D.iloc[i,4])\n",
    "    except:\n",
    "        forosc.append(0)\n",
    "dataset_1D['Forcast Oscillator'] = (forosc)\n",
    "target_up = []\n",
    "target_down = []\n",
    "\n",
    "for i in range(0,dataset_1D.shape[0]-5):\n",
    "    \n",
    "    if (dataset_1D.iloc[i+1,5] * dataset_1D.iloc[i,5]) < 0 :\n",
    "        if (dataset_1D.iloc[i+1,4] - dataset_1D.iloc[i,4]) > 0.2 : \n",
    "            target_up.append(1)\n",
    "            target_down.append(0)\n",
    "        elif (dataset_1D.iloc[i+1,4] - dataset_1D.iloc[i,4]) < -0.1 : \n",
    "            target_up.append(0)\n",
    "            target_down.append(1)\n",
    "        else:\n",
    "            target_up.append(0)\n",
    "            target_down.append(0)\n",
    "    else :\n",
    "        target_up.append(0)\n",
    "        target_down.append(0) \n",
    "        \n",
    "target_up.append(0)\n",
    "target_down.append(0)\n",
    "target_up.append(0)\n",
    "target_down.append(0)\n",
    "target_up.append(0)\n",
    "target_down.append(0)\n",
    "target_up.append(0)\n",
    "target_down.append(0)\n",
    "target_up.append(0)\n",
    "target_down.append(0)\n",
    "dataset_1D['target_up'] = target_up  # target_up # abs(np.array(valley))#target_up\n",
    "dataset_1D['target_down'] = target_down # target_down # abs(np.array(peak))#target_down\n",
    "dataset_1D['target_value'] = dataset_1D['Close']\n",
    "dataset_1D = dataset_1D.drop(['Open','High','Low','Close'],axis=1)\n",
    "df_1D = dataset_1D.copy()\n",
    "df_1D = df_1D.iloc[:,1:]\n",
    "df_1D['Date'] = df['Date']\n",
    "df_1D = df_1D.dropna()\n",
    "df_1D = df_1D[['Date'] + df_1D.columns[:-1].tolist()]\n",
    "df_1D.reset_index(inplace=True,drop=True)\n",
    "print('\\n')\n",
    "print('GOLEM begins Computing...')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "X = df_1D.iloc[:,1:-3]\n",
    "y_up = df_1D.iloc[:,-3].values\n",
    "y_down = df_1D.iloc[:,-2].values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "y_up = np.array(y_up).reshape(-1,1)\n",
    "y_down = np.array(y_down).reshape(-1,1)\n",
    "\n",
    "Xtrain = X[:-1,:]\n",
    "Xtest = X[-1:,:]\n",
    "ytrain_up = y_up[:-1,:]\n",
    "ytest_up = y_up[-1:,:]\n",
    "ytrain_down = y_down[:-1,:]\n",
    "ytest_down = y_down[-1:,:]\n",
    "\n",
    "seed = 770\n",
    "np.random.seed(seed)\n",
    "\n",
    "ytrain_up = ytrain_up.reshape(ytrain_up.shape[0],)\n",
    "ytrain_down = ytrain_down.reshape(ytrain_down.shape[0],)\n",
    "\n",
    "Xtrain = Xtrain.reshape(Xtrain.shape[0],Xtrain.shape[1])\n",
    "\n",
    "model_up = Sequential()\n",
    "# Add an input layer \n",
    "model_up.add(Dense(23, activation='relu'))\n",
    "# Add one hidden layer \n",
    "model_up.add(Dense(50, activation='relu'))\n",
    "# Add an output layer \n",
    "model_up.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_down = Sequential()\n",
    "# Add an input layer \n",
    "model_down.add(Dense(23, activation='relu'))\n",
    "# Add one hidden layer \n",
    "model_down.add(Dense(50, activation='relu'))\n",
    "# Add an output layer \n",
    "model_down.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print('Processing move_up')\n",
    "model_up.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy','mse'])\n",
    "                   \n",
    "history_up = model_up.fit(Xtrain, ytrain_up,epochs=50, batch_size=32, verbose=0)\n",
    "print('\\n')\n",
    "print('Processing move_down')\n",
    "model_down.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy','mse'])\n",
    "                   \n",
    "history_down = model_down.fit(Xtrain, ytrain_down,epochs=50, batch_size=32, verbose=0)\n",
    "print('\\n')\n",
    "print('Computing done')\n",
    "print('\\n')\n",
    "\n",
    "train_acc_up = model_up.evaluate(Xtrain, ytrain_up,verbose=1)\n",
    "train_acc_down = model_down.evaluate(Xtrain, ytrain_down,verbose=1)\n",
    "\n",
    "yhat_up = model_up.predict_classes(Xtest)\n",
    "yhat_down = model_down.predict_classes(Xtest)\n",
    "\n",
    "score_up = model_up.evaluate(Xtest, ytest_up,verbose=1)\n",
    "score_down = model_down.evaluate(Xtest, ytest_down,verbose=1)\n",
    "\n",
    "predict_up = model_up.predict(Xtest)\n",
    "predict_down = model_down.predict(Xtest)\n",
    "\n",
    "accuracy_up = accuracy_score(ytest_up, yhat_up)\n",
    "accuracy_down = accuracy_score(ytest_down, yhat_down)\n",
    "\n",
    "# La précision permet de mesurer la capacité du modèle à refuser résultats non-pertinents : vrais_positifs/(vrais_positifs+faux_positifs)\n",
    "precision_up = precision_score(ytest_up, yhat_up)  \n",
    "precision_down = precision_score(ytest_down, yhat_down) \n",
    "\n",
    "\n",
    "# Recall : (vrai_positifs/(vrais_positifs+faux_négatifs))\n",
    "recall_up = recall_score(ytest_up, yhat_up) \n",
    "recall_down = recall_score(ytest_down, yhat_down) \n",
    "\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('RESULTATS UP\\n')\n",
    "print('Accuracy: %.2f%%' % (accuracy_up * 100.0))\n",
    "print(\"Precision: %.2f%% \" % (precision_up *100),' => Discrimnination des vrais positifs parmi les faux positifs')\n",
    "print(\"Recall: %.2f%% \" % (recall_up * 100),' => Positifs trouvés par Golem sur tous les positifs existants')\n",
    "# get probabilities for positive class\n",
    "\n",
    "print(classification_report(ytest_up, yhat_up))\n",
    "conf_matrix = pd.DataFrame(index = ['vrais_réels','Faux_réels'])\n",
    "conf_matrix['Vrais_estimés'] = ['Vrais_positifs','Faux_positifs']\n",
    "conf_matrix['Faux_estimés'] = ['Faux_négatif','Vrais-négatifs']\n",
    "print(confusion_matrix(ytest_up, yhat_up))\n",
    "\n",
    "print('\\n')\n",
    "print('_______________________________________________________________________________________________________________________________________________________________\\n')\n",
    "print('RESULTATS DOWN\\n')\n",
    "print('Accuracy: %.2f%%' % (accuracy_down * 100.0))\n",
    "print(\"Precision: %.2f%% \" % (precision_down *100),' => Discrimnination des vrais positifs parmi les faux positifs')\n",
    "print(\"Recall: %.2f%% \" % (recall_down * 100),' => Positifs trouvés par Golem sur tous les positifs existants')\n",
    "# get probabilities for positive class\n",
    "\n",
    "print(classification_report(ytest_down, yhat_down))\n",
    "conf_matrix = pd.DataFrame(index = ['vrais_réels','Faux_réels'])\n",
    "conf_matrix['Vrais_estimés'] = ['Vrais_positifs','Faux_positifs']\n",
    "conf_matrix['Faux_estimés'] = ['Faux_négatif','Vrais-négatifs']\n",
    "print(confusion_matrix(ytest_down, yhat_down))\n",
    "print('\\n')\n",
    "\n",
    "resultats = pd.DataFrame()\n",
    "resultats['Date'] = df_1D.iloc[-1:,0]\n",
    "resultats['Move Up'] = yhat_up\n",
    "resultats['Confiance up'] = (predict_up)*100\n",
    "resultats['Move Down'] = yhat_down\n",
    "resultats['Confiance Down'] = (predict_down)*100\n",
    "resultats['Actual'] = df_1D['target_value']\n",
    "\n",
    "tmps2=round(time.time()-tmps1,2)\n",
    "print (\"executed in = %f\" %tmps2,'secondes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Move Up</th>\n",
       "      <th>Confiance up</th>\n",
       "      <th>Move Down</th>\n",
       "      <th>Confiance Down</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9950</th>\n",
       "      <td>2019-06-13 11:31:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>1</td>\n",
       "      <td>92.008171</td>\n",
       "      <td>2890.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Date  Move Up  Confiance up  Move Down  Confiance Down  \\\n",
       "9950 2019-06-13 11:31:00        0      0.000233          1       92.008171   \n",
       "\n",
       "       Actual  \n",
       "9950  2890.42  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con.subscribe_market_data('SPX500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con.get_subscribed_symbols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con.get_prices('SPX500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con.get_last_price('SPX500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con.unsubscribe_market_data('SPX500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def print_data(data, dataframe):\n",
    " #   print('%3d | %s | %s, %s, %s, %s, %s'\n",
    "  #        % (len(dataframe), data['Symbol'],\n",
    "   #          pd.to_datetime(int(data['Updated']), unit='ms'),\n",
    "    #         data['Rates'][0], data['Rates'][1], data['Rates'][2],\n",
    "     #        data['Rates'][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con.subscribe_market_data('SPX500', (print_data,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0613 14:21:29.658311 123145542557696 logs.py:14] api-demo.fxcm.com:443/socket.io [connection error] recv disconnected ([Errno 60] Operation timed out)\n",
      "W0613 14:59:10.778059 123145542557696 logs.py:14] api-demo.fxcm.com:443/socket.io [connection error] recv disconnected (Connection is already closed.)\n",
      "W0613 14:59:12.769774 123145542557696 logs.py:14] api-demo.fxcm.com:443/socket.io [connection error] ('Connection aborted.', OSError(\"(54, 'ECONNRESET')\",))\n"
     ]
    }
   ],
   "source": [
    "#con.unsubscribe_market_data('SPX500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
