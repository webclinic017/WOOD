{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITERATE IT!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "___Author___='LumberJack Jyss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler, QuantileTransformer, Normalizer, PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, f1_score, recall_score, precision_score\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import Model, regularizers\n",
    "from keras.callbacks import  ModelCheckpoint, TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Lambda\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation, Dropout, InputLayer\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOLDER_PATH = r'FILE PATH'                             \n",
    "df = pd.read_csv(r'data.csv')\n",
    "data=df.copy()\n",
    "FEATURES = list(data)[:-1]\n",
    "#data = np.array(data)\n",
    "M = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close_VIX</th>\n",
       "      <th>Close_GLD</th>\n",
       "      <th>Close_OIL</th>\n",
       "      <th>Close_GSPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-03-10</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>129.130005</td>\n",
       "      <td>36.299999</td>\n",
       "      <td>1867.630005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-03-11</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>129.860001</td>\n",
       "      <td>35.810001</td>\n",
       "      <td>1868.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-03-12</td>\n",
       "      <td>14.470000</td>\n",
       "      <td>131.759995</td>\n",
       "      <td>35.349998</td>\n",
       "      <td>1846.339966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-03-13</td>\n",
       "      <td>16.219999</td>\n",
       "      <td>132.210007</td>\n",
       "      <td>35.340000</td>\n",
       "      <td>1841.130005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-03-14</td>\n",
       "      <td>17.820000</td>\n",
       "      <td>133.100006</td>\n",
       "      <td>35.529999</td>\n",
       "      <td>1858.829956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Close_VIX   Close_GLD  Close_OIL   Close_GSPC\n",
       "0  2014-03-10  14.200000  129.130005  36.299999  1867.630005\n",
       "1  2014-03-11  14.800000  129.860001  35.810001  1868.199951\n",
       "2  2014-03-12  14.470000  131.759995  35.349998  1846.339966\n",
       "3  2014-03-13  16.219999  132.210007  35.340000  1841.130005\n",
       "4  2014-03-14  17.820000  133.100006  35.529999  1858.829956"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date']=pd.to_datetime(data['Date'])\n",
    "data=data.drop(['Date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1258 entries, 0 to 1257\n",
      "Data columns (total 4 columns):\n",
      "Close_VIX     1258 non-null float64\n",
      "Close_GLD     1258 non-null float64\n",
      "Close_OIL     1258 non-null float64\n",
      "Close_GSPC    1258 non-null float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 39.4 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close_VIX</th>\n",
       "      <th>Close_GLD</th>\n",
       "      <th>Close_OIL</th>\n",
       "      <th>Close_GSPC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1258.000000</td>\n",
       "      <td>1258.000000</td>\n",
       "      <td>1258.000000</td>\n",
       "      <td>1258.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.970231</td>\n",
       "      <td>118.440032</td>\n",
       "      <td>16.174404</td>\n",
       "      <td>2287.515318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.298594</td>\n",
       "      <td>6.451324</td>\n",
       "      <td>8.562248</td>\n",
       "      <td>311.623094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.140000</td>\n",
       "      <td>100.500000</td>\n",
       "      <td>7.960000</td>\n",
       "      <td>1815.689941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.080000</td>\n",
       "      <td>114.532498</td>\n",
       "      <td>10.752500</td>\n",
       "      <td>2044.900055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.820000</td>\n",
       "      <td>118.979999</td>\n",
       "      <td>12.550000</td>\n",
       "      <td>2161.469971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.629999</td>\n",
       "      <td>123.412498</td>\n",
       "      <td>17.575000</td>\n",
       "      <td>2582.260010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.740002</td>\n",
       "      <td>133.100006</td>\n",
       "      <td>39.320000</td>\n",
       "      <td>2930.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Close_VIX    Close_GLD    Close_OIL   Close_GSPC\n",
       "count  1258.000000  1258.000000  1258.000000  1258.000000\n",
       "mean     14.970231   118.440032    16.174404  2287.515318\n",
       "std       4.298594     6.451324     8.562248   311.623094\n",
       "min       9.140000   100.500000     7.960000  1815.689941\n",
       "25%      12.080000   114.532498    10.752500  2044.900055\n",
       "50%      13.820000   118.979999    12.550000  2161.469971\n",
       "75%      16.629999   123.412498    17.575000  2582.260010\n",
       "max      40.740002   133.100006    39.320000  2930.750000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    " print(data.isnull().values.any())                                                                                     # check for na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(data, save_path, name):\n",
    "    \"\"\"\n",
    "    This function does the same thing as the function above, except more succinctly using the one-hot package from sklearn.\n",
    "    :param data:  original data we want to encode (raw data)\n",
    "    :param save_path: where we save\n",
    "    :param name: name of file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    labels = data[:, -1].reshape(data.shape[0], 1)                                                                      # separate labels from data\n",
    "    data = data[:, :-1]\n",
    "    nominal_cols = [1, 2, 3, 5, 6, 7, 8, 9, 10]                                                                         # columns that are categorical\n",
    "    cont_cols = [x for x in range(data.shape[1]) if x not in nominal_cols]                                              # columns that are continuous\n",
    "    encoder = OneHotEncoder(sparse= False)                                                                              # create encoder\n",
    "    list_of_one_hot_mats = []\n",
    "    continuous_vars = np.zeros(shape=(data.shape[0], data.shape[1] - len(nominal_cols)))                                # holds continuous variables, we will concat with nominal variables later\n",
    "\n",
    "    for col, new_col in zip(cont_cols, range(len(cont_cols))):                                                          # fill continuous matrix\n",
    "        continuous_vars[:, new_col] = data[:, col]\n",
    "\n",
    "    for col in nominal_cols:\n",
    "        min_val = min(data[:, col])\n",
    "        if min_val < 0:                                                                                                 # one hot from sklearn can only handles positive ints\n",
    "            data[:, col] += (-1 * min_val)                                                                              # if we have negative int, shift values up\n",
    "        feat_mat = encoder.fit_transform(data[:, col].reshape(data.shape[0], 1))                                        # create list of one hot encoded matrices\n",
    "        list_of_one_hot_mats.append(feat_mat)\n",
    "\n",
    "    nominal_features = np.concatenate((list_of_one_hot_mats), axis=1)                                                   # concatenate all categorical variables\n",
    "    new_data = np.concatenate((continuous_vars, nominal_features), axis=1)                                              # concatenate continuous vars w/ categorical\n",
    "    new_data = np.concatenate((new_data, labels), axis=1)                                                               # raw data without preprocessing\n",
    "    np.savetxt(save_path + name + '.csv', new_data, delimiter=',')                                                      # save file to disk\n",
    "\n",
    "\n",
    "def preprocess_data(data, technique, labels = True):\n",
    "    data_labels = None\n",
    "    if labels:\n",
    "        # data_labels = data[:, -1].reshape(data.shape[0], 1)\n",
    "        data_labels = data[:, -1]\n",
    "        data = data[:, :-1]\n",
    "    if technique == \"MinMax\":\n",
    "        transformed = MinMaxScaler().fit_transform(data)\n",
    "    elif technique == \"Standard\":\n",
    "        transformed = StandardScaler().fit_transform(data)\n",
    "    elif technique == \"Robust\":\n",
    "        transformed = RobustScaler().fit_transform(data)\n",
    "    elif technique == \"MaxAbs\":\n",
    "        transformed = MaxAbsScaler().fit_transform(data)\n",
    "    elif technique == \"Quantile\":\n",
    "        transformed = QuantileTransformer().fit_transform(data)\n",
    "    elif technique == \"Normalizer\":\n",
    "        transformed = Normalizer().fit_transform(data)\n",
    "    elif technique == \"Polynomial\":\n",
    "        transformed = PolynomialFeatures().fit_transform(data)\n",
    "    else:\n",
    "        print(\"Not a valid preprocessing method for this function\")\n",
    "        return None\n",
    "\n",
    "    if labels:\n",
    "        return transformed, data_labels\n",
    "    else:\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-73c7091562cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'red'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'green'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "    # ------- PCA --------\n",
    "    for preprocess in [MinMaxScaler(), StandardScaler(), RobustScaler(), MaxAbsScaler(), QuantileTransformer(), Normalizer(), PolynomialFeatures(), None]:\n",
    "        new_data = preprocess.fit_transform(data) if preprocess else data  # preprocess data and at the end, plot original data\n",
    "\n",
    "        # 2D Visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(new_data)\n",
    "        results = pca.transform(new_data)\n",
    "        colors = np.array(['red' if label == 1 else 'green'])\n",
    "        results, colors = results[random_indices,:], colors[random_indices]\n",
    "        plt.scatter(results[:, 0], results[:, 1], c=colors)\n",
    "        plt.title('PCA Visualization 2D ' + str(preprocess).split('(')[0])\n",
    "        plt.xlabel('1st Principal Component')\n",
    "        plt.ylabel('2nd Principal Component')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # 3D Visualization\n",
    "        pca_3d = PCA(n_components=3)\n",
    "        pca_3d.fit(new_data)\n",
    "        results_3d = pca_3d.transform(new_data)\n",
    "        colors_3d = np.array(['red' if label == 1 else 'green' for label in labels])\n",
    "        plot_3d(results_3d[random_indices, :], colors_3d[random_indices], 'PCA 3D Visualization ' + str(preprocess).split('(')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(train_portion, val_portion, data):\n",
    "    val_begin = round(0.8 * M) ; val_end = round(val_begin + M * val_portion)\n",
    "\n",
    "    train_set = data[: val_begin, :-1]\n",
    "    val_set = data[val_begin : val_end, :-1]\n",
    "    test_set = data[val_end :, :-1]\n",
    "\n",
    "    train_labels = data[: val_begin, -1]\n",
    "    val_labels = data[val_begin: val_end, -1]\n",
    "    test_labels = data[val_end:, -1]\n",
    "\n",
    "    data_dict = {'Training data' : train_set, 'Validation data': val_set, \"Test data\" : test_set, 'Training labels' : train_labels, 'Validation labels':val_labels, \"Test labels\": test_labels}\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distributions(train_set, train_labels, val_set, val_labels, test_set, test_labels):\n",
    "    # Visualize Proportions of positive defaults in each set:\n",
    "    grid = GridSpec(2, 2)\n",
    "    train_defaults = sum(train_labels == 1) ; val_defaults = sum(val_labels == 1) ; test_defaults = sum(test_labels == 1)\n",
    "    plt.subplot(grid[0, 0], aspect = 1, title= 'Training Set')\n",
    "    plt.pie([train_defaults, train_set.shape[0] - train_defaults], labels=['Defaults', 'Pass'], autopct='%1.1f%%')\n",
    "    plt.subplot(grid[1, 0], aspect = 1, title= 'Validation Set')\n",
    "    plt.pie([val_defaults, val_set.shape[0] - val_defaults], labels=['Defaults', 'Pass'], autopct='%1.1f%%')\n",
    "    plt.subplot(grid[0, 1], aspect = 1, title = \"Test Set\")\n",
    "    plt.pie([test_defaults, test_set.shape[0] - test_defaults], labels=['Defaults', 'Pass'], autopct='%1.1f%%')\n",
    "    plt.subplot(grid[1, 1], aspect = 1, title = \"Aggregate Data\")\n",
    "    plt.pie([sum(data[:, -1] == 1), m - sum(data[:, -1])],  labels=['Defaults', 'Pass'], autopct='%1.1f%%')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------- Normalizing & Visualizing Data ----------------------------------------------------\n",
    "\n",
    "# one_hot(data, FOLDER_PATH, 'OG_one_hot')                                                                                                    # one-hot enocde data and save. data is loaded above ^\n",
    "# og_one_hot = np.array(pd.read_csv(r'PATH HERE'))\n",
    "# print(\"og one hot shape = \", og_one_hot.shape)\n",
    "\n",
    "# mean_normalized_data = preprocess_data(data, 'MeanNormalized', save= True, save_name= \"\\CC_Data_ScaledCC_Data_MeanNormalized\")              # testing preprocessing functions\n",
    "# mean_normalized_data = preprocess_data(data, 'Scaled', save= True, save_name= \"\\CC_Data_ScaledCC_Data_Scaled\")\n",
    "\n",
    "# m = data.shape[0]\n",
    "# train_portion = 0.8 ; val_portion = 0.1                                                                                                       #  Set proportions of data segments\n",
    "#\n",
    "# visualize_data(data)\n",
    "# normalized_data = np.array(pd.read_csv(FOLDER_PATH + '\\Credit_Card_Scaled.csv'))\n",
    "# data_dict = split_data(train_portion, val_portion, normalized_data)\n",
    "#\n",
    "# data_dict = split_data(train_portion, val_portion, data)\n",
    "# visualize_data(normalized_data)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ MODELING FUNCTIONS ---------------------------------\n",
    "\n",
    "def build_nn(model_info):\n",
    "    \"\"\"\n",
    "    This function builds and compiles a NN given a hash table of the model's parameters.\n",
    "    :param model_info:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_info[\"Regularization\"] == \"l2\":                                # if we're using L2 regularization\n",
    "            lambda_ = model_info['Reg param']                                   # get lambda parameter\n",
    "            batch_norm, keep_prob = False, False                                # set other regularization tactics\n",
    "\n",
    "        elif model_info['Regularization'] == 'Batch norm':                      # batch normalization regularization\n",
    "            lambda_ = 0\n",
    "            batch_norm = model_info['Reg param']                                # get param\n",
    "            keep_prob = False\n",
    "            if batch_norm not in ['before', 'after']:                           # ensure we have a valid reg param\n",
    "                raise ValueError\n",
    "\n",
    "        elif model_info['Regularization'] == 'Dropout':                         # Dropout regularization\n",
    "            lambda_, batch_norm = 0, False\n",
    "            keep_prob = model_info['Reg param']\n",
    "    except:\n",
    "        lambda_, batch_norm, keep_prob = 0, False, False                        # if no regularization is being used\n",
    "\n",
    "    hidden, acts = model_info['Hidden layers'], model_info['Activations']\n",
    "    model = Sequential(name=model_info['Name'])\n",
    "    model.add(InputLayer((model_info['Input size'],)))                            # create input layer\n",
    "    first_hidden = True\n",
    "\n",
    "    for lay, act, i in zip(hidden, acts, range(len(hidden))):                                          # create all the hidden layers\n",
    "        if lambda_ > 0:                                                         # if we're doing L2 regularization\n",
    "            if not first_hidden:\n",
    "                model.add(Dense(lay, activation=act, W_regularizer=l2(lambda_), input_shape=(hidden[i - 1],)))    # add additional layers\n",
    "            else:\n",
    "                model.add(Dense(lay, activation=act, W_regularizer=l2(lambda_), input_shape=(model_info['Input size'],)))\n",
    "                first_hidden = False\n",
    "        else:                                                                   # if we're not regularizing\n",
    "            if not first_hidden:\n",
    "                model.add(Dense(lay, input_shape=(hidden[i-1], )))              # add un-regularized layers\n",
    "            else:\n",
    "                model.add(Dense(lay, input_shape=(model_info['Input size'],)))  # if its first layer, connect it to the input layer\n",
    "                first_hidden = False\n",
    "\n",
    "        if batch_norm == 'before':\n",
    "            model.add(BatchNormalization(input_shape=(lay,)))               # add batch normalization layer\n",
    "\n",
    "        model.add(Activation(act))                                          # activation layer is part of the hidden layer\n",
    "\n",
    "        if batch_norm == 'after':\n",
    "            model.add(BatchNormalization(input_shape=(lay,)))               # add batch normalization layer\n",
    "\n",
    "        if keep_prob:\n",
    "            model.add(Dropout(keep_prob, input_shape=(lay,)))               # dropout layer\n",
    "\n",
    "    # --------- Adding Output Layer -------------\n",
    "    model.add(Dense(1, input_shape=(hidden[-1], )))                             # add output layer\n",
    "    if batch_norm == 'before':                                                  # if we're using batch norm regularization\n",
    "        model.add(BatchNormalization(input_shape=(hidden[-1],)))\n",
    "    model.add(Activation('sigmoid'))                                            # apply output layer activation\n",
    "    if batch_norm == 'after':\n",
    "        model.add(BatchNormalization(input_shape=(hidden[-1],)))                # adding batch norm layer\n",
    "\n",
    "    if model_info['Optimization'] == 'adagrad':                                 # setting an optimization method\n",
    "        opt = optimizers.Adagrad(lr = model_info[\"Learning rate\"])\n",
    "    elif model_info['Optimization'] == 'rmsprop':\n",
    "        opt = optimizers.RMSprop(lr = model_info[\"Learning rate\"])\n",
    "    elif model_info['Optimization'] == 'adadelta':\n",
    "        opt = optimizers.Adadelta()\n",
    "    elif model_info['Optimization'] == 'adamax':\n",
    "        opt = optimizers.Adamax(lr = model_info[\"Learning rate\"])\n",
    "    else:\n",
    "        opt = optimizers.Nadam(lr = model_info[\"Learning rate\"])\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])  # compile model\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(param_dict, data, preproc_tech, model=None, num_epochs = 15):\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    results = []                                                    # list of the model's training accuracy, test accuracy, train AUC, and test AUC for each fold of the k-fold cross validation\n",
    "\n",
    "    for train_indices, test_indices in kf.split(data[:, :-1]):\n",
    "        train_data, train_labels = preprocess_data(data[train_indices, :], preproc_tech, True)                  # preprocess data in the CV loop to circumvent evaluation bias\n",
    "        test_data, test_labels = preprocess_data(data[test_indices, :], preproc_tech, True)\n",
    "\n",
    "        if not model:                                                                                           # if we don't provide a model, build one\n",
    "            model = build_nn(param_dict)\n",
    "        model.fit(train_data, train_labels, epochs=num_epochs, batch_size=param_dict[\"Batch size\"], verbose=0)  # train model\n",
    "        y_pred = model.predict(train_data).ravel()                                                              # predict on training data\n",
    "        fpr, tpr, thresholds = roc_curve(train_labels, y_pred)                                                  # compute fpr and tpr\n",
    "        auc_train = auc(fpr, tpr)                                                                               # compute AUC metric\n",
    "        _, train_acc = model.evaluate(train_data, train_labels, verbose=0)                                      # evaluate on training data\n",
    "\n",
    "        y_pred = model.predict(test_data).ravel()                                                               # same as above with test data\n",
    "        fpr, tpr, thresholds = roc_curve(test_labels, y_pred)                                                   # compute FPR & TPR\n",
    "        auc_test = auc(fpr, tpr)                                                                                # Get AUC for test set\n",
    "        _, test_acc = model.evaluate(test_data, test_labels, verbose=0)                                         # evaluate model\n",
    "        results.append((train_acc, test_acc, auc_train, auc_test))                                              # append results\n",
    "\n",
    "    return results\n",
    "\n",
    "def display_k_fold_results(model_hist, top_models = 1):\n",
    "    \"\"\"\n",
    "    Displays results of k-fold cross validation test.\n",
    "    :param model_hist:\n",
    "    :param top_models: the number of top models to display (e.g. if you're testing 40 models, this will only show the top_models best performing models\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    print('------------------ RESULTS FOR ' + model_hist[0]['Preprocessing'] + \" PREPROCESSING ------------------------------------\")\n",
    "    top_test_auc = sorted(model_hist, key=lambda k: k['Avg Test AUC'])\n",
    "    print('\\nTOP Avg AUC Test:\\n')\n",
    "    for a_model in top_test_auc[-top_models:]:\n",
    "        print('-------------------------\\nLearning Rate: ', a_model['Learning rate'], '\\nBatch size: ', a_model['Batch size'])\n",
    "        print('Avg Train AUC: ', a_model[\"Avg Train AUC\"], '\\nAvg Test AUC: ', a_model[\"Avg Test AUC\"], '\\nAvg Train Accuracy: ', a_model[\"Avg Train Accuracy\"], \"\\nAvg Test Accuracy\", a_model[\"Avg Test Accuracy\"])\n",
    "\n",
    "    top_train_auc = sorted(model_hist, key=lambda k: k['Avg Train AUC'])\n",
    "    print('\\nTOP Avg AUC Train:\\n\\n')\n",
    "    for a_model in top_train_auc[-top_models:]:\n",
    "        print('-------------------------\\nLearning Rate: ', a_model['Learning rate'], '\\nBatch size: ', a_model['Batch size'])\n",
    "        print('Avg Train AUC: ', a_model[\"Avg Train AUC\"], '\\nAvg Test AUC: ', a_model[\"Avg Test AUC\"], '\\nAvg Train Accuracy: ', a_model[\"Avg Train Accuracy\"], \"\\nAvg Test Accuracy\", a_model[\"Avg Test Accuracy\"])\n",
    "\n",
    "    top_test_acc = sorted(model_hist, key=lambda k: k[\"Avg Test Accuracy\"])\n",
    "    print('\\nTOP Avg Test Accuracy:\\n\\n')\n",
    "    for a_model in top_test_acc[-top_models:]:\n",
    "        print('-------------------------\\nLearning Rate: ', a_model['Learning rate'], '\\nBatch size: ', a_model['Batch size'])\n",
    "        print('Avg Train AUC: ', a_model[\"Avg Train AUC\"], '\\nAvg Test AUC: ', a_model[\"Avg Test AUC\"], '\\nAvg Train Accuracy: ', a_model[\"Avg Train Accuracy\"], \"\\nAvg Test Accuracy\", a_model[\"Avg Test Accuracy\"])\n",
    "\n",
    "    top_train_acc = sorted(model_hist, key=lambda k: k[\"Avg Train Accuracy\"])\n",
    "    print('\\nTOP Avg Train Accuracy:\\n\\n')\n",
    "    for a_model in top_train_acc[-top_models:]:\n",
    "        print('-------------------------\\nLearning Rate: ', a_model['Learning rate'], '\\nBatch size: ', a_model['Batch size'])\n",
    "        print('Avg Train AUC: ', a_model[\"Avg Train AUC\"], '\\nAvg Test AUC: ', a_model[\"Avg Test AUC\"], '\\nAvg Train Accuracy: ', a_model[\"Avg Train Accuracy\"], \"\\nAvg Test Accuracy\", a_model[\"Avg Test Accuracy\"])\n",
    "    print(\n",
    "        \"------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "def display_model_info(model_info):\n",
    "    print('\\n---------------------------------------------------------,')\n",
    "    print(\"Architecture:\\nLayers: \", [model_info['Input size']] + [model_info['Hidden layers']] + [1])                                                                   # show layers & number of units per layer\n",
    "    print(\"Activations: \", model_info[\"Activations\"] + [\"sigmoid\"])                                                                                                      # print hidden layer activations\n",
    "    print(\"Hyperparameters:\\nBatch size: \", model_info[\"Batch size\"], \"\\nLearning rate: \", model_info[\"Learning rate\"], \"\\nOptimization: \", model_info[\"Optimization\"])\n",
    "    if model_info[\"Preprocessing\"]:\n",
    "        print(\"Preprocessing: \", model_info[\"Preprocessing\"])\n",
    "    print(\"K-Fold CV RESULTS:\\nAvg Train AUC: \", model_info[\"Avg Train AUC\"], \"\\nAvg Test AUC: \", model_info[\"Avg Test AUC\"], \"\\nAvg Train Accuracy: \", model_info[\"Avg Train Accuracy\"])\n",
    "    print(\"Avg Test Accuracy: \", model_info[\"Avg Test Accuracy\"])\n",
    "\n",
    "def generate_random_model():\n",
    "    optimization_methods = ['adagrad', 'rmsprop', 'adadelta', 'adam', 'adamax', 'nadam']      # possible optimization methods\n",
    "    activation_functions = ['sigmoid', 'relu', 'tanh']          # possible activation functions\n",
    "    batch_sizes = [16, 32, 64, 128, 256, 512]                   # possible batch sizes\n",
    "    range_hidden_units = range(5, 250)                          # range of possible hidden units\n",
    "    model_info = {}                                             # create hash table\n",
    "    same_units = np.random.choice([0, 1], p=[1/5, 4/5])         # dictates whether all hidden layers will have the same number of units\n",
    "    same_act_fun = np.random.choice([0, 1], p=[1/10, 9/10])     # will each hidden layer have the same activation function?\n",
    "    really_deep = np.random.rand()\n",
    "    range_layers = range(1, 10) if really_deep < 0.8 else range(6, 20)          # 80% of time constrain number of hidden layers between 1 - 10, 20% of time permit really deep architectures\n",
    "    num_layers = np.random.choice(range_layers, p=[.1, .2, .2, .2, .05, .05, .05, .1, .05]) if really_deep < 0.8 else np.random.choice(range_layers)    # choose number of layers\n",
    "    model_info[\"Activations\"] = [np.random.choice(activation_functions, p = [0.25, 0.5, 0.25])] * num_layers if same_act_fun else [np.random.choice(activation_functions, p = [0.25, 0.5, 0.25]) for _ in range(num_layers)] # choose activation functions\n",
    "    model_info[\"Hidden layers\"] = [np.random.choice(range_hidden_units)] * num_layers if same_units else [np.random.choice(range_hidden_units) for _ in range(num_layers)]  # create hidden layers\n",
    "    model_info[\"Optimization\"] = np.random.choice(optimization_methods)         # choose an optimization method at random\n",
    "    model_info[\"Batch size\"] = np.random.choice(batch_sizes)                    # choose batch size\n",
    "    model_info[\"Learning rate\"] = 10 ** (-4 * np.random.rand())                 # choose a learning rate on a logarithmic scale\n",
    "    model_info[\"Training threshold\"] = 0.5                                      # set threshold for training\n",
    "    return model_info\n",
    "\n",
    "def quick_nn_test(model_info, data_dict, save_path):\n",
    "    model = build_nn(model_info)                                    # use model info to build and compile a nn\n",
    "    stop = EarlyStopping(patience=5, monitor='acc', verbose=1)      # maintain a max accuracy for a sliding window of 5 epochs. If we cannot breach max accuracy after 15 epochs, cut model off and move on.\n",
    "    tensorboard_path = save_path + model_info['Name'] + '\\\\'        # create path for tensorboard callback\n",
    "    tensorboard = TensorBoard(log_dir=tensorboard_path, histogram_freq=0, write_graph=True, write_images=True)              # create tensorboard callback\n",
    "    save_model = ModelCheckpoint(filepath= save_path + model_info['Name'] + '\\\\' + model_info['Name'] + '_saved_' + '.h5')  # save model after every epoch\n",
    "\n",
    "    model.fit(data_dict['Training data'], data_dict['Training labels'], epochs=3,                 # fit model\n",
    "              batch_size=model_info['Batch size'], callbacks=[save_model, stop, tensorboard])     # evaluate train accuracy\n",
    "\n",
    "    train_acc = model.evaluate(data_dict['Training data'], data_dict['Training labels'],\n",
    "                               batch_size=model_info['Batch size'], verbose=0)\n",
    "    test_acc = model.evaluate(data_dict['Test data'], data_dict['Test labels'],                     # evaluate test accuracy\n",
    "                              batch_size=model_info['Batch size'], verbose=0)\n",
    "\n",
    "                                                                                            # Get Train AUC\n",
    "    y_pred = model.predict(data_dict['Training data']).ravel()                          # predict on training data\n",
    "    fpr, tpr, thresholds = roc_curve(data_dict['Training labels'], y_pred)              # compute fpr and tpr\n",
    "    auc_train = auc(fpr, tpr)                                                           # compute AUC metric\n",
    "                                                                                        # Get Test AUC\n",
    "    y_pred = model.predict(data_dict['Test data']).ravel()                              # same as above with test data\n",
    "    fpr, tpr, thresholds = roc_curve(data_dict['Test labels'], y_pred)                  # compute AUC\n",
    "    auc_test = auc(fpr, tpr)\n",
    "\n",
    "    return train_acc, test_acc, auc_train, auc_test\n",
    "    # return 9, 9, 9, 9\n",
    "\n",
    "def test_nn_models(num_models, raw_data, preprocess_tech, save_path, eval_tech = 'k-fold', list_of_models= None):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate neural network performance on a dataset. We can feed it either a list of models to test explicitly, or if we leave this parameter as None,\n",
    "    it will randomly generate neural networks.\n",
    "    :param num_models:      number of models we want to test\n",
    "    :param raw_data:        raw data without any preprocessing done\n",
    "    :param preprocess_tech: the technique we want to employ in preprocessing the data\n",
    "    :param save_path:       where to save the results df\n",
    "    :param eval_tech:       how we want to evaluate the performance of the neural network.\n",
    "    :param list_of_models: if provided, it will test every neural network in the list. Models must be pre-compiled.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model_hist = [] ; model_results = pd.DataFrame(columns=['Hidden', 'Activations', 'Learning rate', 'Train Accuracy', 'Test Accuracy', 'Train AUC', 'Test AUC'])  # TODO: Add tensorboard and model saving capabilities to this function\n",
    "    result_index = 0\n",
    "    if eval_tech not in ['k-fold', 'boot']:                                                                 # must be a valid evaluation technique\n",
    "        raise NameError\n",
    "\n",
    "    for i in range(num_models):\n",
    "        if not list_of_models:                                                                              # if a list of models to test isn't provided\n",
    "            model_info = generate_random_model()                                                            # randomly generate models\n",
    "            model_info[\"Preprocessing\"] = preprocess_tech\n",
    "            model_info[\"Input size\"] = raw_data.shape[1] - 1                                                # set input data size\n",
    "        else:\n",
    "            assert num_models == len(list_of_models)                                                        # if we provide models, ensure the num_models param lines up\n",
    "            model_info = list_of_models[i]                                                                  # get the ith model's info\n",
    "\n",
    "        model_info[\"Model type\"] = 'NN'                                                                     # this is for k-fold cross validation, since k-fold is able to evaluate both neural nets and logistic regression models\n",
    "        model = build_nn(model_info)                                                                        # build & compile NN\n",
    "\n",
    "        if eval_tech == 'k-fold':\n",
    "            results = k_fold_cross_validation(model_info, raw_data, preprocess_tech, model=model)           # do k-fold cross validation on model\n",
    "        else:\n",
    "            print(\"currently not supporting bootstrap evaluation due to computational constraints\")\n",
    "\n",
    "        train_acc, test_acc, train_auc, test_auc = zip(*results)                                            # separate results into lists\n",
    "\n",
    "        model_info[\"Avg Test AUC\"] = round(np.mean(test_auc), 3)                                            # compute average metrics for each fold of the evaluation\n",
    "        model_info[\"Avg Train AUC\"] = round(np.mean(train_auc), 3)\n",
    "        model_info[\"Avg Train Accuracy\"] = round(np.mean(train_acc), 3)                                     # these are averages from k-fold cross validation\n",
    "        model_info[\"Avg Test Accuracy\"] = round(np.mean(test_acc), 3)\n",
    "        model_hist.append(model_info)                                                                       # add this model's info to the list\n",
    "\n",
    "        model_results.loc[result_index] = (model_info['Hidden layers'], model_info['Activations'],\\\n",
    "                                           model_info['Learning rate'], model_info['Avg Train Accuracy'],\\\n",
    "                                           model_info[\"Avg Test Accuracy\"], \\\n",
    "                                           model_info[\"Avg Train AUC\"], model_info[\"Avg Test AUC\"])         # write model results to df\n",
    "        result_index += 1\n",
    "\n",
    "    model_results.to_csv(save_path + \"K-fold NN Results.csv\")                                               # save results to disk\n",
    "    display_k_fold_results(model_hist, top_models=1)                                                        # display the top models' performance\n",
    "\n",
    "\n",
    "def create_five_nns(input_size, hidden_size, act=None):\n",
    "    \"\"\"\n",
    "    Creates 5 neural networks to be used as a baseline in determining the influence model depth & width has on performance.\n",
    "    :param input_size:\n",
    "    :param hidden_size:\n",
    "    :param act: activation function to use for each layer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    act = ['relu'] if not act else [act]                             # default activation = 'relu'\n",
    "    nns = []                                                         # list of model info hash tables\n",
    "    model_info = {}                                                  # hash tables storing model information\n",
    "    model_info['Hidden layers'] = [hidden_size]\n",
    "    model_info['Input size'] = input_size\n",
    "    model_info['Activations'] = act\n",
    "    model_info['Optimization'] = 'adadelta'\n",
    "    model_info[\"Learning rate\"] = .005\n",
    "    model_info[\"Batch size\"] = 32\n",
    "    model_info[\"Preprocessing\"] = 'Standard'\n",
    "    model_info2, model_info3, model_info4, model_info5 = model_info.copy(), model_info.copy(), model_info.copy(), model_info.copy()\n",
    "\n",
    "    model_info[\"Name\"] = 'Shallow NN'                                 # build shallow nn\n",
    "    nns.append(model_info)\n",
    "\n",
    "    model_info2['Hidden layers'] = [hidden_size] * 3                  # build medium nn\n",
    "    model_info2['Activations'] = act * 3\n",
    "    model_info2[\"Name\"] = 'Medium NN'\n",
    "    nns.append(model_info2)\n",
    "\n",
    "    model_info3['Hidden layers'] = [hidden_size] * 6                  # build deep nn\n",
    "    model_info3['Activations'] = act * 6\n",
    "    model_info3[\"Name\"] = 'Deep NN 1'\n",
    "    nns.append(model_info3)\n",
    "\n",
    "    model_info4['Hidden layers'] = [hidden_size] * 11                 # build really deep nn\n",
    "    model_info4['Activations'] = act * 11\n",
    "    model_info4[\"Name\"] = 'Deep NN 2'\n",
    "    nns.append(model_info4)\n",
    "\n",
    "    model_info5['Hidden layers'] = [hidden_size] * 20                   # build realllllly deep nn\n",
    "    model_info5['Activations'] = act * 20\n",
    "    model_info5[\"Name\"] = 'Deep NN 3'\n",
    "    nns.append(model_info5)\n",
    "    return nns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- ESTABLISH BASELINES w/ 5-FOLD CROSS VALIDATION -------------------\n",
    "# Test Logistic regression on original data preprocessed and og_one_hot preprocessed\n",
    "#og_one_hot = np.array(pd.read_csv(r'C:\\Users\\x2016onq\\PycharmProjects\\CreditDefault\\OG_one_hot.csv'))\n",
    "\n",
    "# print(\"TESTING.....\")\n",
    "\n",
    "# Evaluate several models via 5-Fold Cross validation\n",
    "#save_path = r'YOUR PATH HERE'\n",
    "\n",
    "# path = save_path + r'\\50 sigmoid\\\\'                                                             # test nns w/ 50 neurons on one-hot\n",
    "# list_of_nns = create_five_nns(input_size=og_one_hot.shape[1]-1, hidden_size=50, act='sigmoid')\n",
    "# test_nn_models(len(list_of_nns), og_one_hot, 'Standard', path, list_of_models=list_of_nns)\n",
    "#\n",
    "# path = save_path + r'\\50 tanh\\\\'                                                                # test nns w/ 50 neurons on one-hot\n",
    "# list_of_nns = create_five_nns(input_size=og_one_hot.shape[1]-1, hidden_size=50, act='tanh')\n",
    "# test_nn_models(len(list_of_nns), og_one_hot, 'Standard', path, list_of_models=list_of_nns)\n",
    "#\n",
    "# path = save_path + r'\\40\\\\'                                                                     # test nns w/ 40 neurons on one-hot\n",
    "# list_of_nns = create_five_nns(input_size=og_one_hot.shape[1]-1, hidden_size=40)\n",
    "# test_nn_models(len(list_of_nns), og_one_hot, 'Standard', path, list_of_models=list_of_nns)\n",
    "#\n",
    "# path = save_path + r'\\60\\\\'                                                                     # test nns w/ 40 neurons on one-hot\n",
    "# list_of_nns = create_five_nns(input_size=og_one_hot.shape[1]-1, hidden_size=60)\n",
    "# test_nn_models(len(list_of_nns), og_one_hot, 'Standard', path, list_of_models=list_of_nns)\n",
    "#\n",
    "# path = save_path + r'\\70\\\\'                                                                     # test nns w/ 40 neurons on one-hot\n",
    "# list_of_nns = create_five_nns(input_size=og_one_hot.shape[1]-1, hidden_size=70)\n",
    "# test_nn_models(len(list_of_nns), og_one_hot, 'Standard', path, list_of_models=list_of_nns)\n",
    "#\n",
    "# path = save_path + r'\\80\\\\'                                                                     # test nns w/ 40 neurons on one-hot\n",
    "# list_of_nns = create_five_nns(input_size=og_one_hot.shape[1]-1, hidden_size=80)\n",
    "# test_nn_models(len(list_of_nns), og_one_hot, 'Standard', path, list_of_models=list_of_nns)\n",
    "#\n",
    "# path = save_path + r'\\90\\\\'                                                                     # test nns w/ 40 neurons on one-hot\n",
    "# list_of_nns = create_five_nns(input_size=og_one_hot.shape[1]-1, hidden_size=90)\n",
    "# test_nn_models(len(list_of_nns), og_one_hot, 'Standard', path, list_of_models=list_of_nns)\n",
    "#\n",
    "# path = save_path + r'\\100\\\\'                                                                    # test nns w/ 40 neurons on one-hot\n",
    "# list_of_nns = create_five_nns(input_size=og_one_hot.shape[1]-1, hidden_size=100)\n",
    "# test_nn_models(len(list_of_nns), og_one_hot, 'Standard', path, list_of_models=list_of_nns)\n",
    "#\n",
    "# path = save_path + r'\\110\\\\'                                                                    # test nns w/ 40 neurons on one-hot\n",
    "# list_of_nns = create_five_nns(input_size=og_one_hot.shape[1]-1, hidden_size=110)\n",
    "# test_nn_models(len(list_of_nns), og_one_hot, 'Standard', path, list_of_models=list_of_nns)\n",
    "#\n",
    "# path = save_path + r'\\120\\\\'                                                                    # test nns w/ 40 neurons on one-hot\n",
    "# list_of_nns = create_five_nns(input_size=og_one_hot.shape[1]-1, hidden_size=120)\n",
    "# test_nn_models(len(list_of_nns), og_one_hot, 'Standard', path, list_of_models=list_of_nns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ QUICK TESTING ------------------------\n",
    "\n",
    "\"\"\"This section of code allows us to create and test many neural networks and save the results of a quick \n",
    "test into a CSV file. Once that CSV file has been created, we will continue to add results onto the existing \n",
    "file.\"\"\"\n",
    "\n",
    "rapid_testing_path = 'test.csv'  # TODO: UNCOMMENT THIS\n",
    "data_path = 'data.csv' # TODO: UNCOMMENT THIS\n",
    "\n",
    "\n",
    "try:                                                                        # try to load existing csv\n",
    "    rapid_mlp_results = pd.read_csv(rapid_testing_path + 'Results.csv')\n",
    "    index = rapid_mlp_results.shape[1]\n",
    "except:                                                                     # if no csv exists yet, create a DF\n",
    "    rapid_mlp_results = pd.DataFrame(columns=['Model', 'Train Accuracy', 'Test Accuracy', 'Train AUC', 'Test AUC',\n",
    "                                              'Preprocessing', 'Batch size', 'Learn Rate', 'Optimization', 'Activations',\n",
    "                                              'Hidden layers', 'Regularization'])\n",
    "    index = 0\n",
    "\n",
    "og_one_hot = np.array(pd.read_csv(data_path))                     # load one hot data\n",
    "\n",
    "model_info = {}                                                     # create model_info dicts for all the models we want to test\n",
    "model_info['Hidden layers'] = [100] * 6                             # specifies the number of hidden units per layer\n",
    "model_info['Input size'] = og_one_hot.shape[1] - 1                  # input data size\n",
    "model_info['Activations'] = ['relu'] * 6                            # activation function for each layer\n",
    "model_info['Optimization'] = 'adadelta'                             # optimization method\n",
    "model_info[\"Learning rate\"] = .005                                  # learning rate for optimization method\n",
    "model_info[\"Batch size\"] = 32\n",
    "model_info[\"Preprocessing\"] = 'Standard'                            # specifies the preprocessing method to be used\n",
    "\n",
    "model_0 = model_info.copy()                                         # create model 0\n",
    "model_0['Name'] = 'Model0'\n",
    "\n",
    "model_1 = model_info.copy()                                         # create model 1\n",
    "model_1['Hidden layers'] = [110] * 3\n",
    "model_1['Name'] = 'Model1'\n",
    "\n",
    "model_2 = model_info.copy()                                         # try best model so far with several regularization parameter values\n",
    "model_2['Hidden layers'] = [110] * 6\n",
    "model_2['Name'] = 'Model2'\n",
    "model_2['Regularization'] = 'l2'\n",
    "model_2['Reg param'] = 0.0005\n",
    "\n",
    "model_3 = model_info.copy()\n",
    "model_3['Hidden layers'] = [110] * 6\n",
    "model_3['Name'] = 'Model3'\n",
    "model_3['Regularization'] = 'l2'\n",
    "model_3['Reg param'] = 0.05\n",
    "\n",
    "model_4 = model_info.copy()                                                             # try best model so far with several regularization parameter values\n",
    "model_4['Hidden layers'] = [110] * 6\n",
    "model_4['Name'] = 'Model4'\n",
    "model_4['Regularization'] = 'l2'\n",
    "model_4['Reg param'] = 0.0005\n",
    "\n",
    "# .... create more models ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2014-03-10'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-8d9338dacc55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m                                                                                          \u001b[0;31m# for each model_info in list of models to test, test model and record results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mog_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Preprocessing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m                            \u001b[0;31m# preprocess raw data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m29999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# split data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquick_nn_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrapid_testing_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# quickly assess model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-36650aeb3cdb>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(data, technique, labels)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtechnique\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Standard\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtechnique\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Robust\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobustScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    647\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m    648\u001b[0m                         \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2014-03-10'"
     ]
    }
   ],
   "source": [
    "#-------------- REGULARIZATION OPTIONS -------------\n",
    "#   L2 Regularization:      Regularization: 'l2',           Reg param: lambda value\n",
    "#   Dropout:                Regularization: 'Dropout',      Reg param: keep_prob\n",
    "#   Batch normalization:    Regularization: 'Batch norm',   Reg param: 'before' or 'after'\n",
    "\n",
    "models = [model_0, model_1, model_2]                                  # make a list of model_info hash tables\n",
    "\n",
    "column_list = ['Model', 'Train Accuracy', 'Test Accuracy', 'Train AUC', 'Test AUC', 'Preprocessing',\n",
    "               'Batch size', 'Learn Rate', 'Optimization', 'Activations', 'Hidden layers',\n",
    "               'Regularization', 'Reg Param']\n",
    "\n",
    "for model in models:                                                                                          # for each model_info in list of models to test, test model and record results\n",
    "    train_data, labels = preprocess_data(og_one_hot, model['Preprocessing'], True)                            # preprocess raw data\n",
    "    data_dict = split_data(0.9, 0, np.concatenate((train_data, labels.reshape(29999, 1)), axis=1))             # split data\n",
    "    train_acc, test_acc, auc_train, auc_test = quick_nn_test(model, data_dict, save_path=rapid_testing_path)  # quickly assess model\n",
    "\n",
    "    try:\n",
    "        reg = model['Regularization']                                             # set regularization parameters if given\n",
    "        reg_param = model['Reg param']\n",
    "    except:\n",
    "        reg = \"None\"                                                              # else set NULL params\n",
    "        reg_param = 'NA'\n",
    "\n",
    "    val_lis = [model['Name'], train_acc[1], test_acc[1], auc_train, auc_test, model['Preprocessing'],\n",
    "                model[\"Batch size\"], model[\"Learning rate\"], model[\"Optimization\"], str(model[\"Activations\"]),\n",
    "                str(model[\"Hidden layers\"]), reg, reg_param]\n",
    "\n",
    "    df_dict = {}\n",
    "    for col, val in zip(column_list, val_lis):                                    # create df dict to append to csv file\n",
    "        df_dict[col] = val\n",
    "\n",
    "    df = pd.DataFrame(df_dict, index=[index])\n",
    "    rapid_mlp_results = rapid_mlp_results.append(df, ignore_index=False)\n",
    "    rapid_mlp_results.to_csv(rapid_testing_path + \"Results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
