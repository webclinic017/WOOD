{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import pandas_datareader\n",
    "import stock_data_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load newest Google stock data from Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>1227.140015</td>\n",
       "      <td>1196.170044</td>\n",
       "      <td>1197.349976</td>\n",
       "      <td>1223.969971</td>\n",
       "      <td>2227400.0</td>\n",
       "      <td>1223.969971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>1231.790039</td>\n",
       "      <td>1213.150024</td>\n",
       "      <td>1216.000000</td>\n",
       "      <td>1231.540039</td>\n",
       "      <td>1204000.0</td>\n",
       "      <td>1231.540039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-22</th>\n",
       "      <td>1230.000000</td>\n",
       "      <td>1202.824951</td>\n",
       "      <td>1226.319946</td>\n",
       "      <td>1205.500000</td>\n",
       "      <td>1714200.0</td>\n",
       "      <td>1205.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>1206.397949</td>\n",
       "      <td>1187.040039</td>\n",
       "      <td>1196.930054</td>\n",
       "      <td>1193.000000</td>\n",
       "      <td>1496800.0</td>\n",
       "      <td>1193.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>1202.829956</td>\n",
       "      <td>1176.719971</td>\n",
       "      <td>1198.530029</td>\n",
       "      <td>1184.619995</td>\n",
       "      <td>1898100.0</td>\n",
       "      <td>1184.619995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   High          Low         Open        Close     Volume  \\\n",
       "Date                                                                        \n",
       "2019-03-20  1227.140015  1196.170044  1197.349976  1223.969971  2227400.0   \n",
       "2019-03-21  1231.790039  1213.150024  1216.000000  1231.540039  1204000.0   \n",
       "2019-03-22  1230.000000  1202.824951  1226.319946  1205.500000  1714200.0   \n",
       "2019-03-25  1206.397949  1187.040039  1196.930054  1193.000000  1496800.0   \n",
       "2019-03-26  1202.829956  1176.719971  1198.530029  1184.619995  1898100.0   \n",
       "\n",
       "              Adj Close  \n",
       "Date                     \n",
       "2019-03-20  1223.969971  \n",
       "2019-03-21  1231.540039  \n",
       "2019-03-22  1205.500000  \n",
       "2019-03-25  1193.000000  \n",
       "2019-03-26  1184.619995  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "start = dt.datetime(1995,1,1)\n",
    "end   = dt.date.today()\n",
    "data = pandas_datareader.data.DataReader('GOOG','yahoo',start,end)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalise and Prepozess the data like a boss^12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ordinal/1e6</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Adj Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Adj Open</th>\n",
       "      <th>Adj High</th>\n",
       "      <th>Adj Low</th>\n",
       "      <th>Normalised Volume</th>\n",
       "      <th>Normalised Close</th>\n",
       "      <th>Normalised Open</th>\n",
       "      <th>Normalised High</th>\n",
       "      <th>Normalised Low</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-08-19</th>\n",
       "      <td>0.731812</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.543621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996611</td>\n",
       "      <td>1.037074</td>\n",
       "      <td>0.956348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-20</th>\n",
       "      <td>0.731813</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.277955</td>\n",
       "      <td>1.079430</td>\n",
       "      <td>1.006677</td>\n",
       "      <td>1.087104</td>\n",
       "      <td>1.001595</td>\n",
       "      <td>-0.722045</td>\n",
       "      <td>0.079430</td>\n",
       "      <td>0.006677</td>\n",
       "      <td>0.087104</td>\n",
       "      <td>0.001595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-23</th>\n",
       "      <td>0.731816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222226</td>\n",
       "      <td>1.090293</td>\n",
       "      <td>1.103747</td>\n",
       "      <td>1.130955</td>\n",
       "      <td>1.086805</td>\n",
       "      <td>-0.794127</td>\n",
       "      <td>0.010064</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.047733</td>\n",
       "      <td>0.006832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-24</th>\n",
       "      <td>0.731817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>1.045146</td>\n",
       "      <td>1.108631</td>\n",
       "      <td>1.112218</td>\n",
       "      <td>1.032190</td>\n",
       "      <td>-0.829770</td>\n",
       "      <td>-0.041408</td>\n",
       "      <td>0.016819</td>\n",
       "      <td>0.020110</td>\n",
       "      <td>-0.053291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-25</th>\n",
       "      <td>0.731818</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.111847</td>\n",
       "      <td>1.056408</td>\n",
       "      <td>1.046043</td>\n",
       "      <td>1.076340</td>\n",
       "      <td>1.035280</td>\n",
       "      <td>-0.892984</td>\n",
       "      <td>0.010775</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.029846</td>\n",
       "      <td>-0.009440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Ordinal/1e6  Weekday  Adj Volume  Adj Close  Adj Open  Adj High  \\\n",
       "Date                                                                          \n",
       "2004-08-19     0.731812      3.0    0.543621   1.000000  0.996611  1.037074   \n",
       "2004-08-20     0.731813      4.0    0.277955   1.079430  1.006677  1.087104   \n",
       "2004-08-23     0.731816      0.0    0.222226   1.090293  1.103747  1.130955   \n",
       "2004-08-24     0.731817      1.0    0.185600   1.045146  1.108631  1.112218   \n",
       "2004-08-25     0.731818      2.0    0.111847   1.056408  1.046043  1.076340   \n",
       "\n",
       "             Adj Low  Normalised Volume  Normalised Close  Normalised Open  \\\n",
       "Date                                                                         \n",
       "2004-08-19  0.956348           0.000000          0.000000         0.000000   \n",
       "2004-08-20  1.001595          -0.722045          0.079430         0.006677   \n",
       "2004-08-23  1.086805          -0.794127          0.010064         0.022528   \n",
       "2004-08-24  1.032190          -0.829770         -0.041408         0.016819   \n",
       "2004-08-25  1.035280          -0.892984          0.010775         0.000858   \n",
       "\n",
       "            Normalised High  Normalised Low  \n",
       "Date                                         \n",
       "2004-08-19         0.000000        0.000000  \n",
       "2004-08-20         0.087104        0.001595  \n",
       "2004-08-23         0.047733        0.006832  \n",
       "2004-08-24         0.020110       -0.053291  \n",
       "2004-08-25         0.029846       -0.009440  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalise data\n",
    "data_n = stock_data_preprocessing.normalise_stock_data(data)\n",
    "data_n.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1,2,3 Plot Line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-5-1df3a23faa43>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-1df3a23faa43>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    stock_data_preprocessing.stock_plot((data_n,)\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "stock_data_preprocessing.stock_plot((data_n,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickup the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "prediction_time = 1 #day\n",
    "testdatasize = 450\n",
    "unroll_length = 50\n",
    "testdatacut = testdatasize + unroll_length  + 1\n",
    "\n",
    "x_train = data_n[0:-prediction_time-testdatacut].as_matrix()\n",
    "y_train = data_n[prediction_time:-testdatacut  ]['Normalised Close'].as_matrix()\n",
    "\n",
    "# test data\n",
    "x_test = data_n[0-testdatacut:-prediction_time].as_matrix()\n",
    "y_test = data_n[prediction_time-testdatacut:  ]['Normalised Close'].as_matrix()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unroll it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (2604, 50, 12)\n",
      "y_train (2604,)\n",
      "x_test (450, 50, 12)\n",
      "y_test (450,)\n"
     ]
    }
   ],
   "source": [
    "def unroll(data,sequence_length=24):\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "    return np.asarray(result)\n",
    "\n",
    "\n",
    "x_train = unroll(x_train,unroll_length)\n",
    "x_test  = unroll(x_test,unroll_length)\n",
    "y_train = y_train[-x_train.shape[0]:]\n",
    "y_test  = y_test[-x_test.shape[0]:]\n",
    "\n",
    "\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go! (with Python 3.5, Keras 1.2.2 and Tensorflow 1.0, better on AWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "import lstm, time #helper libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compilation time : 0.019312381744384766\n"
     ]
    }
   ],
   "source": [
    "#Step 2 Build Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "    input_dim=x_train.shape[-1],\n",
    "    output_dim=50,\n",
    "    return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "    100,\n",
    "    return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(\n",
    "    output_dim=1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "start = time.time()\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "print('compilation time : {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2473 samples, validate on 131 samples\n",
      "Epoch 1/350\n",
      "2473/2473 [==============================] - 1s - loss: 0.1511 - val_loss: 1.5176\n",
      "Epoch 2/350\n",
      "2473/2473 [==============================] - 0s - loss: 1.5853 - val_loss: 0.0046\n",
      "Epoch 3/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0503 - val_loss: 0.0038\n",
      "Epoch 4/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0471 - val_loss: 4.8843e-04\n",
      "Epoch 5/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0443 - val_loss: 3.7705e-04\n",
      "Epoch 6/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0438 - val_loss: 7.4141e-04\n",
      "Epoch 7/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0422 - val_loss: 5.0372e-04\n",
      "Epoch 8/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0413 - val_loss: 7.1282e-04\n",
      "Epoch 9/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0410 - val_loss: 6.9578e-04\n",
      "Epoch 10/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0399 - val_loss: 7.0684e-04\n",
      "Epoch 11/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0392 - val_loss: 3.5026e-04\n",
      "Epoch 12/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0386 - val_loss: 3.4143e-04\n",
      "Epoch 13/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0362 - val_loss: 3.7493e-04\n",
      "Epoch 14/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0367 - val_loss: 4.1753e-04\n",
      "Epoch 15/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0337 - val_loss: 0.0010\n",
      "Epoch 16/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0334 - val_loss: 8.9436e-04\n",
      "Epoch 17/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0349 - val_loss: 0.0078\n",
      "Epoch 18/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0420 - val_loss: 0.0219\n",
      "Epoch 19/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0601 - val_loss: 0.0589\n",
      "Epoch 20/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0924 - val_loss: 0.0376\n",
      "Epoch 21/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0674 - val_loss: 0.0162\n",
      "Epoch 22/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0515 - val_loss: 0.0115\n",
      "Epoch 23/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0371 - val_loss: 0.0029\n",
      "Epoch 24/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0328 - val_loss: 0.0034\n",
      "Epoch 25/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0307 - val_loss: 0.0032\n",
      "Epoch 26/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0291 - val_loss: 0.0036\n",
      "Epoch 27/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0291 - val_loss: 0.0028\n",
      "Epoch 28/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0279 - val_loss: 0.0023\n",
      "Epoch 29/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0269 - val_loss: 0.0019\n",
      "Epoch 30/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0276 - val_loss: 0.0046\n",
      "Epoch 31/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0290 - val_loss: 0.0053\n",
      "Epoch 32/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0276 - val_loss: 0.0098\n",
      "Epoch 33/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0338 - val_loss: 0.0285\n",
      "Epoch 34/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0440 - val_loss: 0.0294\n",
      "Epoch 35/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0508 - val_loss: 0.0383\n",
      "Epoch 36/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0542 - val_loss: 0.0204\n",
      "Epoch 37/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0480 - val_loss: 0.0184\n",
      "Epoch 38/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0365 - val_loss: 0.0071\n",
      "Epoch 39/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0272 - val_loss: 0.0052\n",
      "Epoch 40/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0248 - val_loss: 0.0029\n",
      "Epoch 41/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0219 - val_loss: 0.0013\n",
      "Epoch 42/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0195 - val_loss: 9.8827e-04\n",
      "Epoch 43/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0193 - val_loss: 9.9133e-04\n",
      "Epoch 44/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0188 - val_loss: 0.0019\n",
      "Epoch 45/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0173 - val_loss: 0.0017\n",
      "Epoch 46/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0187 - val_loss: 0.0052\n",
      "Epoch 47/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0202 - val_loss: 0.0064\n",
      "Epoch 48/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0223 - val_loss: 0.0127\n",
      "Epoch 49/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0277 - val_loss: 0.0237\n",
      "Epoch 50/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0361 - val_loss: 0.0246\n",
      "Epoch 51/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0415 - val_loss: 0.0241\n",
      "Epoch 52/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0361 - val_loss: 0.0114\n",
      "Epoch 53/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0260 - val_loss: 0.0065\n",
      "Epoch 54/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0207 - val_loss: 0.0040\n",
      "Epoch 55/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0180 - val_loss: 0.0030\n",
      "Epoch 56/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0166 - val_loss: 0.0025\n",
      "Epoch 57/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0155 - val_loss: 0.0021\n",
      "Epoch 58/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0155 - val_loss: 0.0031\n",
      "Epoch 59/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0155 - val_loss: 0.0039\n",
      "Epoch 60/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0162 - val_loss: 0.0046\n",
      "Epoch 61/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0172 - val_loss: 0.0083\n",
      "Epoch 62/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0179 - val_loss: 0.0083\n",
      "Epoch 63/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0203 - val_loss: 0.0114\n",
      "Epoch 64/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0204 - val_loss: 0.0095\n",
      "Epoch 65/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0227 - val_loss: 0.0110\n",
      "Epoch 66/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0209 - val_loss: 0.0083\n",
      "Epoch 67/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0198 - val_loss: 0.0083\n",
      "Epoch 68/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0166 - val_loss: 0.0046\n",
      "Epoch 69/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0143 - val_loss: 0.0039\n",
      "Epoch 70/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0133 - val_loss: 0.0021\n",
      "Epoch 71/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0121 - val_loss: 0.0033\n",
      "Epoch 72/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0111 - val_loss: 0.0017\n",
      "Epoch 73/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0112 - val_loss: 0.0032\n",
      "Epoch 74/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0112 - val_loss: 0.0031\n",
      "Epoch 75/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0115 - val_loss: 0.0045\n",
      "Epoch 76/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0126 - val_loss: 0.0053\n",
      "Epoch 77/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0140 - val_loss: 0.0084\n",
      "Epoch 78/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0155 - val_loss: 0.0084\n",
      "Epoch 79/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0167 - val_loss: 0.0113\n",
      "Epoch 80/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0175 - val_loss: 0.0059\n",
      "Epoch 81/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0162 - val_loss: 0.0053\n",
      "Epoch 82/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0132 - val_loss: 0.0044\n",
      "Epoch 83/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0107 - val_loss: 0.0030\n",
      "Epoch 84/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0098 - val_loss: 0.0015\n",
      "Epoch 85/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0085 - val_loss: 0.0013\n",
      "Epoch 86/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0080 - val_loss: 0.0011\n",
      "Epoch 87/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0077 - val_loss: 0.0013\n",
      "Epoch 88/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0073 - val_loss: 0.0021\n",
      "Epoch 89/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0079 - val_loss: 0.0030\n",
      "Epoch 90/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0081 - val_loss: 0.0030\n",
      "Epoch 91/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0091 - val_loss: 0.0056\n",
      "Epoch 92/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0107 - val_loss: 0.0062\n",
      "Epoch 93/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0121 - val_loss: 0.0087\n",
      "Epoch 94/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0130 - val_loss: 0.0071\n",
      "Epoch 95/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0126 - val_loss: 0.0053\n",
      "Epoch 96/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0103 - val_loss: 0.0035\n",
      "Epoch 97/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0090 - val_loss: 0.0032\n",
      "Epoch 98/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0075 - val_loss: 0.0020\n",
      "Epoch 99/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0067 - val_loss: 0.0018\n",
      "Epoch 100/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0065 - val_loss: 0.0015\n",
      "Epoch 101/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0060 - val_loss: 0.0018\n",
      "Epoch 102/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0063 - val_loss: 0.0017\n",
      "Epoch 103/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0061 - val_loss: 0.0021\n",
      "Epoch 104/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0062 - val_loss: 0.0026\n",
      "Epoch 105/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0069 - val_loss: 0.0035\n",
      "Epoch 106/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0081 - val_loss: 0.0050\n",
      "Epoch 107/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0091 - val_loss: 0.0063\n",
      "Epoch 108/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0096 - val_loss: 0.0063\n",
      "Epoch 109/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0094 - val_loss: 0.0038\n",
      "Epoch 110/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0080 - val_loss: 0.0026\n",
      "Epoch 111/350\n",
      "2473/2473 [==============================] - 0s - loss: 0.0066 - val_loss: 0.0028\n",
      "Epoch 112/350\n"
     ]
    }
   ],
   "source": [
    "#Step 3 Train the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=3028,\n",
    "    nb_epoch=350,\n",
    "    validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Model & some (~ x10) fewer Loss !!! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step 4 - Plot the predictions!\n",
    "predictions = lstm.predict_sequences_multiple(model, x_test, 50, 50)\n",
    "lstm.plot_results_multiple(predictions, y_test, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
