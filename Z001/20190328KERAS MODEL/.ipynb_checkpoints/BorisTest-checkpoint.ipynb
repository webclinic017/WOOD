{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-97d5655c57cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn, rnn\n",
    "import mxnet as mx\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "context = mx.cpu(); model_ctx=mx.cpu()\n",
    "mx.random.seed(1719)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(x):\n",
    "    return datetime.datetime.strptime(x,'%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ex_df = pd.read_csv('data/panel_data_close.csv', header=0, parse_dates=[0], date_parser=parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ex_df[['Date', 'GS']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} number of days in the dataset.'.format(dataset_ex_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5), dpi=100)\n",
    "plt.plot(dataset_ex_df['Date'], dataset_ex_df['GS'], label='Goldman Sachs stock')\n",
    "plt.vlines(datetime.date(2016,4, 20), 0, 270, linestyles='--', colors='gray', label='Train/Test data cut-off')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('USD')\n",
    "plt.title('Figure 2: Goldman Sachs stock price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_days = int(dataset_ex_df.shape[0]*.7)\n",
    "print('Number of training days: {}. Number of test days: {}.'.format(num_training_days, \\\n",
    "                                                                    dataset_ex_df.shape[0]-num_training_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_technical_indicators(dataset):\n",
    "    # Create 7 and 21 days Moving Average\n",
    "    dataset['ma7'] = dataset['price'].rolling(window=7).mean()\n",
    "    dataset['ma21'] = dataset['price'].rolling(window=21).mean()\n",
    "    \n",
    "    # Create MACD\n",
    "    dataset['26ema'] = pd.ewma(dataset['price'], span=26)\n",
    "    dataset['12ema'] = pd.ewma(dataset['price'], span=12)\n",
    "    dataset['MACD'] = (dataset['12ema']-dataset['26ema'])\n",
    "\n",
    "    # Create Bollinger Bands\n",
    "    dataset['20sd'] = pd.stats.moments.rolling_std(dataset['price'],20)\n",
    "    dataset['upper_band'] = dataset['ma21'] + (dataset['20sd']*2)\n",
    "    dataset['lower_band'] = dataset['ma21'] - (dataset['20sd']*2)\n",
    "    \n",
    "    # Create Exponential moving average\n",
    "    dataset['ema'] = dataset['price'].ewm(com=0.5).mean()\n",
    "    \n",
    "    # Create Momentum\n",
    "    dataset['momentum'] = dataset['price']-1\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_TI_df = get_technical_indicators(dataset_ex_df[['GS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_technical_indicators(dataset, last_days):\n",
    "    plt.figure(figsize=(16, 10), dpi=100)\n",
    "    shape_0 = dataset.shape[0]\n",
    "    xmacd_ = shape_0-last_days\n",
    "    \n",
    "    dataset = dataset.iloc[-last_days:, :]\n",
    "    x_ = range(3, dataset.shape[0])\n",
    "    x_ =list(dataset.index)\n",
    "    \n",
    "    # Plot first subplot\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(dataset['ma7'],label='MA 7', color='g',linestyle='--')\n",
    "    plt.plot(dataset['price'],label='Closing Price', color='b')\n",
    "    plt.plot(dataset['ma21'],label='MA 21', color='r',linestyle='--')\n",
    "    plt.plot(dataset['upper_band'],label='Upper Band', color='c')\n",
    "    plt.plot(dataset['lower_band'],label='Lower Band', color='c')\n",
    "    plt.fill_between(x_, dataset['lower_band'], dataset['upper_band'], alpha=0.35)\n",
    "    plt.title('Technical indicators for Goldman Sachs - last {} days.'.format(last_days))\n",
    "    plt.ylabel('USD')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot second subplot\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title('MACD')\n",
    "    plt.plot(dataset['MACD'],label='MACD', linestyle='-.')\n",
    "    plt.hlines(15, xmacd_, shape_0, colors='g', linestyles='--')\n",
    "    plt.hlines(-15, xmacd_, shape_0, colors='g', linestyles='--')\n",
    "    plt.plot(dataset['log_momentum'],label='Momentum', color='b',linestyle='-')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_technical_indicators(dataset_TI_df, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just import bert\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_FT = dataset_ex_df[['Date', 'GS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_fft = np.fft.fft(np.asarray(data_FT['GS'].tolist()))\n",
    "fft_df = pd.DataFrame({'fft':close_fft})\n",
    "fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
    "fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7), dpi=100)\n",
    "fft_list = np.asarray(fft_df['fft'].tolist())\n",
    "for num_ in [3, 6, 9, 100]:\n",
    "    fft_list_m10= np.copy(fft_list); fft_list_m10[num_:-num_]=0\n",
    "    plt.plot(np.fft.ifft(fft_list_m10), label='Fourier transform with {} components'.format(num_))\n",
    "plt.plot(data_FT['GS'],  label='Real')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('USD')\n",
    "plt.title('Figure 3: Goldman Sachs (close) stock prices & Fourier transforms')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "items = deque(np.asarray(fft_df['absolute'].tolist()))\n",
    "items.rotate(int(np.floor(len(fft_df)/2)))\n",
    "plt.figure(figsize=(10, 7), dpi=80)\n",
    "plt.stem(items)\n",
    "plt.title('Figure 4: Components of Fourier transforms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pandas import DataFrame\n",
    "from pandas import datetime\n",
    "\n",
    "series = data_FT['GS']\n",
    "model = ARIMA(series, order=(5, 1, 0))\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(series)\n",
    "plt.figure(figsize=(10, 7), dpi=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = series.values\n",
    "size = int(len(X) * 0.66)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted (from ARIMA) and real prices\n",
    "\n",
    "plt.figure(figsize=(12, 6), dpi=100)\n",
    "plt.plot(test, label='Real')\n",
    "plt.plot(predictions, color='red', label='Predicted')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('USD')\n",
    "plt.title('Figure 5: ARIMA model on GS stock')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total dataset has {} samples, and {} features.'.format(dataset_total_df.shape[0], \\\n",
    "                                                              dataset_total_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_data(data_income):\n",
    "    data = data_income.copy()\n",
    "    y = data['price']\n",
    "    X = data.iloc[:, 1:]\n",
    "    \n",
    "    train_samples = int(X.shape[0] * 0.65)\n",
    " \n",
    "    X_train = X.iloc[:train_samples]\n",
    "    X_test = X.iloc[train_samples:]\n",
    "\n",
    "    y_train = y.iloc[:train_samples]\n",
    "    y_test = y.iloc[train_samples:]\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test data\n",
    "(X_train_FI, y_train_FI), (X_test_FI, y_test_FI) = get_feature_importance_data(dataset_TI_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(gamma=0.0,n_estimators=150,base_score=0.7,colsample_bytree=1,learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbModel = regressor.fit(X_train_FI,y_train_FI, \\\n",
    "                         eval_set = [(X_train_FI, y_train_FI), (X_test_FI, y_test_FI)], \\\n",
    "                         verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rounds = range(len(eval_result['validation_0']['rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = regressor.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=training_rounds,y=eval_result['validation_0']['rmse'],label='Training Error')\n",
    "plt.scatter(x=training_rounds,y=eval_result['validation_1']['rmse'],label='Validation Error')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training Vs Validation Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar([i for i in range(len(xgbModel.feature_importances_))], xgbModel.feature_importances_.tolist(), tick_label=X_test_FI.columns)\n",
    "plt.title('Figure 6: Feature importance of the technical indicators.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * math.pow(x, 3))))\n",
    "def relu(x):\n",
    "    return max(x, 0)\n",
    "def lrelu(x):\n",
    "    return max(0.01*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=.5, hspace=None)\n",
    "\n",
    "ranges_ = (-10, 3, .25)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([i for i in np.arange(*ranges_)], [relu(i) for i in np.arange(*ranges_)], label='ReLU', marker='.')\n",
    "plt.plot([i for i in np.arange(*ranges_)], [gelu(i) for i in np.arange(*ranges_)], label='GELU')\n",
    "plt.hlines(0, -10, 3, colors='gray', linestyles='--', label='0')\n",
    "plt.title('Figure 7: GELU as an activation function for autoencoders')\n",
    "plt.ylabel('f(x) for GELU and ReLU')\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([i for i in np.arange(*ranges_)], [lrelu(i) for i in np.arange(*ranges_)], label='Leaky ReLU')\n",
    "plt.hlines(0, -10, 3, colors='gray', linestyles='--', label='0')\n",
    "plt.ylabel('f(x) for Leaky ReLU')\n",
    "plt.xlabel('x')\n",
    "plt.title('Figure 8: LeakyReLU')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_batches = VAE_data.shape[0]/batch_size\n",
    "VAE_data = VAE_data.values\n",
    "\n",
    "train_iter = mx.io.NDArrayIter(data={'data': VAE_data[:num_training_days,:-1]}, \\\n",
    "                               label={'label': VAE_data[:num_training_days, -1]}, batch_size = batch_size)\n",
    "test_iter = mx.io.NDArrayIter(data={'data': VAE_data[num_training_days:,:-1]}, \\\n",
    "                              label={'label': VAE_data[num_training_days:,-1]}, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctx =  mx.cpu()\n",
    "class VAE(gluon.HybridBlock):\n",
    "    def __init__(self, n_hidden=400, n_latent=2, n_layers=1, n_output=784, \\\n",
    "                 batch_size=100, act_type='relu', **kwargs):\n",
    "        self.soft_zero = 1e-10\n",
    "        self.n_latent = n_latent\n",
    "        self.batch_size = batch_size\n",
    "        self.output = None\n",
    "        self.mu = None\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = nn.HybridSequential(prefix='encoder')\n",
    "            \n",
    "            for i in range(n_layers):\n",
    "                self.encoder.add(nn.Dense(n_hidden, activation=act_type))\n",
    "            self.encoder.add(nn.Dense(n_latent*2, activation=None))\n",
    "\n",
    "            self.decoder = nn.HybridSequential(prefix='decoder')\n",
    "            for i in range(n_layers):\n",
    "                self.decoder.add(nn.Dense(n_hidden, activation=act_type))\n",
    "            self.decoder.add(nn.Dense(n_output, activation='sigmoid'))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        h = self.encoder(x)\n",
    "        #print(h)\n",
    "        mu_lv = F.split(h, axis=1, num_outputs=2)\n",
    "        mu = mu_lv[0]\n",
    "        lv = mu_lv[1]\n",
    "        self.mu = mu\n",
    "\n",
    "        eps = F.random_normal(loc=0, scale=1, shape=(self.batch_size, self.n_latent), ctx=model_ctx)\n",
    "        z = mu + F.exp(0.5*lv)*eps\n",
    "        y = self.decoder(z)\n",
    "        self.output = y\n",
    "\n",
    "        KL = 0.5*F.sum(1+lv-mu*mu-F.exp(lv),axis=1)\n",
    "        logloss = F.sum(x*F.log(y+self.soft_zero)+ (1-x)*F.log(1-y+self.soft_zero), axis=1)\n",
    "        loss = -logloss-KL\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden=400 # neurons in each layer\n",
    "n_latent=2 \n",
    "n_layers=3 # num of dense layers in encoder and decoder respectively\n",
    "n_output=VAE_data.shape[1]-1 \n",
    "\n",
    "net = VAE(n_hidden=n_hidden, n_latent=n_latent, n_layers=n_layers, n_output=n_output, batch_size=batch_size, act_type='gelu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(), ctx=mx.cpu())\n",
    "net.hybridize()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': .01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 150\n",
    "print_period = n_epoch // 10\n",
    "start = time.time()\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "for epoch in range(n_epoch):\n",
    "    epoch_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "\n",
    "    train_iter.reset()\n",
    "    test_iter.reset()\n",
    "\n",
    "    n_batch_train = 0\n",
    "    for batch in train_iter:\n",
    "        n_batch_train +=1\n",
    "        data = batch.data[0].as_in_context(mx.cpu())\n",
    "\n",
    "        with autograd.record():\n",
    "            loss = net(data)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        epoch_loss += nd.mean(loss).asscalar()\n",
    "\n",
    "    n_batch_val = 0\n",
    "    for batch in test_iter:\n",
    "        n_batch_val +=1\n",
    "        data = batch.data[0].as_in_context(mx.cpu())\n",
    "        loss = net(data)\n",
    "        epoch_val_loss += nd.mean(loss).asscalar()\n",
    "\n",
    "    epoch_loss /= n_batch_train\n",
    "    epoch_val_loss /= n_batch_val\n",
    "\n",
    "    training_loss.append(epoch_loss)\n",
    "    validation_loss.append(epoch_val_loss)\n",
    "\n",
    "    \"\"\"if epoch % max(print_period, 1) == 0:\n",
    "        print('Epoch {}, Training loss {:.2f}, Validation loss {:.2f}'.\\\n",
    "              format(epoch, epoch_loss, epoch_val_loss))\"\"\"\n",
    "\n",
    "end = time.time()\n",
    "print('Training completed in {} seconds.'.format(int(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_total_df['Date'] = dataset_ex_df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_added_df = mx.nd.array(dataset_total_df.iloc[:, :-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of the newly created (from the autoencoder) features is {}.'.format(vae_added_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the PCA to create the new components to explain 80% of the variance\n",
    "pca = PCA(n_components=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = StandardScaler().fit_transform(vae_added_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents = pca.fit_transform(x_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_num_features = dataset_total_df.shape[1]\n",
    "sequence_length = 17\n",
    "\n",
    "class RNNModel(gluon.Block):\n",
    "    def __init__(self, num_embed, num_hidden, num_layers, bidirectional=False, \\\n",
    "                 sequence_length=sequence_length, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.num_hidden = num_hidden\n",
    "        with self.name_scope():\n",
    "            self.rnn = rnn.LSTM(num_hidden, num_layers, input_size=num_embed, \\\n",
    "                                bidirectional=bidirectional, layout='TNC')\n",
    "            \n",
    "            self.decoder = nn.Dense(1, in_units=num_hidden)\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        output, hidden = self.rnn(inputs, hidden)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)\n",
    "    \n",
    "lstm_model = RNNModel(num_embed=gan_num_features, num_hidden=500, num_layers=1)\n",
    "lstm_model.collect_params().initialize(mx.init.Xavier(), ctx=mx.cpu())\n",
    "trainer = gluon.Trainer(lstm_model.collect_params(), 'adam', {'learning_rate': .01})\n",
    "loss = gluon.loss.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriangularSchedule():\n",
    "    def __init__(self, min_lr, max_lr, cycle_length, inc_fraction=0.5):     \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.cycle_length = cycle_length\n",
    "        self.inc_fraction = inc_fraction\n",
    "        \n",
    "    def __call__(self, iteration):\n",
    "        if iteration <= self.cycle_length*self.inc_fraction:\n",
    "            unit_cycle = iteration * 1 / (self.cycle_length * self.inc_fraction)\n",
    "        elif iteration <= self.cycle_length:\n",
    "            unit_cycle = (self.cycle_length - iteration) * 1 / (self.cycle_length * (1 - self.inc_fraction))\n",
    "        else:\n",
    "            unit_cycle = 0\n",
    "        adjusted_cycle = (unit_cycle * (self.max_lr - self.min_lr)) + self.min_lr\n",
    "        return adjusted_cycle\n",
    "\n",
    "class CyclicalSchedule():\n",
    "    def __init__(self, schedule_class, cycle_length, cycle_length_decay=1, cycle_magnitude_decay=1, **kwargs):\n",
    "        self.schedule_class = schedule_class\n",
    "        self.length = cycle_length\n",
    "        self.length_decay = cycle_length_decay\n",
    "        self.magnitude_decay = cycle_magnitude_decay\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def __call__(self, iteration):\n",
    "        cycle_idx = 0\n",
    "        cycle_length = self.length\n",
    "        idx = self.length\n",
    "        while idx <= iteration:\n",
    "            cycle_length = math.ceil(cycle_length * self.length_decay)\n",
    "            cycle_idx += 1\n",
    "            idx += cycle_length\n",
    "        cycle_offset = iteration - idx + cycle_length\n",
    "        \n",
    "        schedule = self.schedule_class(cycle_length=cycle_length, **self.kwargs)\n",
    "        return schedule(cycle_offset) * self.magnitude_decay**cycle_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = CyclicalSchedule(TriangularSchedule, min_lr=0.5, max_lr=2, cycle_length=500)\n",
    "iterations=1500\n",
    "\n",
    "plt.plot([i+1 for i in range(iterations)],[schedule(i) for i in range(iterations)])\n",
    "plt.title('Learning rate for each epoch')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fc = 512\n",
    "\n",
    "# ... other parts of the GAN\n",
    "\n",
    "cnn_net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    \n",
    "    # Add the 1D Convolutional layers\n",
    "    cnn_net.add(gluon.nn.Conv1D(32, kernel_size=5, strides=2))\n",
    "    cnn_net.add(nn.LeakyReLU(0.01))\n",
    "    cnn_net.add(gluon.nn.Conv1D(64, kernel_size=5, strides=2))\n",
    "    cnn_net.add(nn.LeakyReLU(0.01))\n",
    "    cnn_net.add(nn.BatchNorm())\n",
    "    cnn_net.add(gluon.nn.Conv1D(128, kernel_size=5, strides=2))\n",
    "    cnn_net.add(nn.LeakyReLU(0.01))\n",
    "    cnn_net.add(nn.BatchNorm())\n",
    "    \n",
    "    # Add the two Fully Connected layers\n",
    "    cnn_net.add(nn.Dense(220, use_bias=False), nn.BatchNorm(), nn.LeakyReLU(0.01))\n",
    "    cnn_net.add(nn.Dense(220, use_bias=False), nn.Activation(activation='relu'))\n",
    "    cnn_net.add(nn.Dense(1))\n",
    "    \n",
    "# ... other parts of the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "utility = UtilityFunction(kind=\"ucb\", kappa=2.5, xi=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
