{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "___Author___='LumberJack Jyss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Algo MULTI LumberJack FROM KEREAS with memory\\nLumberJack Jyss (c)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader as web\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import keras\n",
    "import sklearn\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algo = ReLU #LSTM SimpleRNN GRU ReLU RNN NN\n",
    "nbneurons = 2000 #round(len(train_X)*2)\n",
    "ntimstep = 1 # incrémentation du timeframe\n",
    "nbfeature = 2 # nombre de colonnes\n",
    "activation = 'linear' #'tanh' # 'sigmoid' 'linear' 'Softmax activation' 'softmax'\n",
    "batchsize = 1 # incrémentation du tick\n",
    "nbepochs = 50\n",
    "loop = 500 # si on utilise le looping pour le reset_states()\n",
    "laloss = 'mean_squared_error' # 'mean_squared_error' 'binary_crossentropy'\n",
    "lemetrics =  ['accuracy'] # ['accuracy']\n",
    "optimize = 'adam' # 'adam'\n",
    "kernelinitializer = 'glorot_uniform'\n",
    "recurrentinitializer = 'orthogonal'\n",
    "biasinitializer = 'zeros'\n",
    "recurrentactivation = 'hard_sigmoid'\n",
    "\n",
    "print (\"Activation d'un modèle de type\",Algo,\" avec 4 layers identiques ayant respectivement \",nbneurons,', '\\\n",
    "       ,round(nbneurons/2),', ',round(nbneurons/4),' et ',round(nbneurons/8),\" neurones. L'activation est de type \"\\\n",
    "       ,activation,\" le loss, \",laloss,\" et les metrics, \",lemetrics,\". L'optimisation est \",optimize,\". Il y aura \",\\\n",
    "       nbepochs,'epochs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame([[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\\\n",
    "                        ,[0,3,6,9,12,15,18,21,24,27,30,33,36,39,42,45,48,51,54,57,60]\\\n",
    "                        ,[0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60,64,68,72,76,80]]).T\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "data=pd.DataFrame(dataset)\n",
    "data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot(figsize=(16,6),title='Original')\n",
    "data.plot(figsize=(16,6),title='Copie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloc1 = 15\n",
    "train_X = data.iloc[0:bloc1,0:2]\n",
    "test_X = data.iloc[bloc1:,0:2]\n",
    "train_y = data.iloc[0:bloc1,2]\n",
    "test_y = data.iloc[bloc1:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X\n",
    "#test_X\n",
    "#train_y\n",
    "#test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = train_X.values\n",
    "output_train = train_y.values\n",
    "input_test = test_X.values\n",
    "output_test = test_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = input_train.astype(float)\n",
    "output_train = output_train.astype(float)\n",
    "input_test = input_test.astype(float)\n",
    "output_test = output_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_train = input_train.reshape(-1,1)\n",
    "#output_train = output_train.reshape(-1,1)\n",
    "#input_test = input_test.reshape(-1,1)\n",
    "#output_test = output_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scalerXI = preprocessing.MinMaxScaler().fit(input_train)\n",
    "#scalerXO = preprocessing.MinMaxScaler().fit(output_train)\n",
    "#scalerYI = preprocessing.MinMaxScaler().fit(input_test)\n",
    "#scalerYO = preprocessing.MinMaxScaler().fit(output_test)\n",
    "input_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = input_train.reshape((input_train.shape[0],1,input_train.shape[1]))\n",
    "output_train = output_train.reshape(output_train.shape[0],1,1)#output_train.shape[1])\n",
    "input_test = input_test.reshape((input_test.shape[0],1,input_test.shape[1]))\n",
    "output_test = output_test.reshape(output_test.shape[0],1,1)#output_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_shape = input_train.shape\n",
    "output_train_shape = output_train.shape\n",
    "input_test_shape = input_test.shape\n",
    "output_test_shape = output_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_shape,output_train_shape,input_test_shape,output_test_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Algo(nbneurons,input_shape=(ntimstep,nbfeature)))#,stateful=True,\\\n",
    "               #kernelinitializer=kernelinitializer,recurrentinitializer = recurrentinitializer,\\\n",
    "        # biasinitializer='zeros',recurrent_activation==recurrentactivation,return_sequences=True))\n",
    "\n",
    "model.add(Algo(round(nbneurons/2)))#,return_sequences=True))\n",
    "model.add(Activation(activation))\n",
    "model.add(Algo(round(nbneurons/4)))#,return_sequences=True))\n",
    "model.add(Algo(round(nbneurons/8)))\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimize, loss=laloss,metrics=lemetrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(loop):\n",
    "history = model.fit(input_train, output_train, batch_size=batchsize, epochs=nbepochs)\n",
    "#    model.reset_states()\n",
    "#    print('Loop N°',i,'/',loop)\n",
    "print('Modèle entrainé.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(input_test, output_test,verbose=2,batch_size=batchsize)\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[[21],[63]] #=>84\n",
    "X = np.array(X).T\n",
    "X = X.reshape(X.shape[0],1,X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X,verbose=1)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(input_test,output_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"ModelMulti-NoNorm.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"ModelMulti-NoNorm-yaml.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"ModelMulti-NoNorm.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"ModelMulti-NoNorm-JSON.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load YAML and create model\n",
    "yaml_file = open('model.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
